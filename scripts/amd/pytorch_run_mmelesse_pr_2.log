Excluding distributed/nn/jit/test_instantiator on ROCm
Excluding distributed/rpc/test_faulty_agent on ROCm
Excluding distributed/rpc/test_tensorpipe_agent on ROCm
Excluding distributed/rpc/cuda/test_tensorpipe_agent on ROCm
Excluding distributed/_sharded_tensor/test_sharded_tensor on ROCm
Excluding test_determination on ROCm
Excluding test_multiprocessing on ROCm
Excluding test_multiprocessing_spawn on ROCm
Excluding test_type_hints on ROCm
Excluding test_openmp on ROCm
Selected tests: autograd/test_complex, benchmark_utils/test_benchmark_utils, distributed/_sharding_spec/test_sharding_spec, distributed/algorithms/test_join, distributed/elastic/events/lib_test, distributed/elastic/metrics/api_test, distributed/elastic/multiprocessing/api_test, distributed/elastic/timer/api_test, distributed/elastic/timer/local_timer_example, distributed/elastic/timer/local_timer_test, distributed/elastic/utils/distributed_test, distributed/elastic/utils/logging_test, distributed/elastic/utils/util_test, distributed/optim/test_zero_redundancy_optimizer, distributed/pipeline/sync/skip/test_api, distributed/pipeline/sync/skip/test_gpipe, distributed/pipeline/sync/skip/test_inspect_skip_layout, distributed/pipeline/sync/skip/test_leak, distributed/pipeline/sync/skip/test_portal, distributed/pipeline/sync/skip/test_stash_pop, distributed/pipeline/sync/skip/test_tracker, distributed/pipeline/sync/skip/test_verify_skippables, distributed/pipeline/sync/test_balance, distributed/pipeline/sync/test_bugs, distributed/pipeline/sync/test_checkpoint, distributed/pipeline/sync/test_copy, distributed/pipeline/sync/test_deferred_batch_norm, distributed/pipeline/sync/test_dependency, distributed/pipeline/sync/test_inplace, distributed/pipeline/sync/test_microbatch, distributed/pipeline/sync/test_phony, distributed/pipeline/sync/test_pipe, distributed/pipeline/sync/test_pipeline, distributed/pipeline/sync/test_stream, distributed/pipeline/sync/test_transparency, distributed/pipeline/sync/test_worker, distributed/test_c10d_common, distributed/test_c10d_gloo, distributed/test_c10d_nccl, distributed/test_c10d_spawn_gloo, distributed/test_c10d_spawn_nccl, distributed/test_data_parallel, distributed/test_distributed_spawn, distributed/test_jit_c10d, distributed/test_launcher, distributed/test_nccl, distributed/test_pg_wrapper, distributed/test_store, distributions/test_constraints, distributions/test_distributions, test_ao_sparsity, test_autocast, test_autograd, test_binary_ufuncs, test_buffer_protocol, test_bundled_inputs, test_complex, test_cpp_api_parity, test_cpp_extensions_aot_ninja, test_cpp_extensions_aot_no_ninja, test_cpp_extensions_jit, test_cuda, test_cuda_primary_ctx, test_dataloader, test_datapipe, test_dispatch, test_foreach, test_function_schema, test_functional_autograd_benchmark, test_functional_optim, test_futures, test_fx, test_fx_experimental, test_import_time, test_indexing, test_jit, test_jit_disabled, test_jit_fuser_te, test_license, test_linalg, test_logging, test_mkldnn, test_mobile_optimizer, test_model_dump, test_module_init, test_modules, test_namedtensor, test_namedtuple_return_api, test_native_functions, test_nn, test_numba_integration, test_numpy_interop, test_ops, test_optim, test_overrides, test_package, test_profiler, test_pruning_op, test_public_bindings, test_pytree, test_quantization, test_reductions, test_serialization, test_set_default_mobile_cpu_allocator, test_shape_ops, test_show_pickle, test_sort_and_select, test_sparse, test_sparse_csr, test_spectral_ops, test_tensor_creation_ops, test_tensorboard, test_tensorexpr, test_tensorexpr_pybind, test_testing, test_torch, test_type_info, test_type_promotion, test_unary_ufuncs, test_utils, test_view_ops, test_vmap, test_vulkan, test_xnnpack_integration
Running autograd/test_complex ... [2021-10-12 07:58:58.933348]
Executing ['/opt/conda/bin/python3.6', 'autograd/test_complex.py', '-v'] ... [2021-10-12 07:58:58.933383]
test_view_func_for_complex_views (__main__.TestAutogradComplex) ... ok
test_view_with_multi_output (__main__.TestAutogradComplex) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.222s

OK
Running benchmark_utils/test_benchmark_utils ... [2021-10-12 07:59:01.460849]
Executing ['/opt/conda/bin/python3.6', 'benchmark_utils/test_benchmark_utils.py', '-v'] ... [2021-10-12 07:59:01.460925]
test_adaptive_timer (__main__.TestBenchmarkUtils) ... ok
test_collect_callgrind (__main__.TestBenchmarkUtils) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_collect_cpp_callgrind (__main__.TestBenchmarkUtils) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_compare (__main__.TestBenchmarkUtils) ... ok
test_cpp_timer (__main__.TestBenchmarkUtils) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_fuzzer (__main__.TestBenchmarkUtils) ... ok
test_manipulate_callgrind_stats (__main__.TestBenchmarkUtils) ... ok
test_timer (__main__.TestBenchmarkUtils) ... ok
test_timer_tiny_fast_snippet (__main__.TestBenchmarkUtils) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'

----------------------------------------------------------------------
Ran 9 tests in 2.183s

OK (skipped=4)
Running distributed/_sharding_spec/test_sharding_spec ... [2021-10-12 07:59:06.648735]
Executing ['/opt/conda/bin/python3.6', 'distributed/_sharding_spec/test_sharding_spec.py', '-v'] ... [2021-10-12 07:59:06.648821]
test_chunked_sharding_spec (__main__.TestShardingSpec) ... ok
test_device_placement (__main__.TestShardingSpec) ... ok
test_enumerable_sharding_spec (__main__.TestShardingSpec) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.028s

OK
Running distributed/algorithms/test_join ... [2021-10-12 07:59:09.233065]
Executing ['/opt/conda/bin/python3.6', 'distributed/algorithms/test_join.py', '-v'] ... [2021-10-12 07:59:09.233150]
test_join_kwargs (__main__.TestJoin) ... ok
test_multiple_joinable_disable (__main__.TestJoin) ... ok
test_multiple_joinables (__main__.TestJoin) ... ok
test_multiple_joinables_throw (__main__.TestJoin) ... ok
test_single_joinable (__main__.TestJoin) ... ok
test_single_joinable_disable (__main__.TestJoin)
Tests ``enable=False`` for a single :class:`Joinable`. ... ok
test_single_joinable_main_hooks (__main__.TestJoin)
Tests the main hooks of a single :class:`Joinable`. ... ok
test_single_joinable_post_hooks (__main__.TestJoin)
Tests the post-hooks of a single :class:`Joinable`. ... ok
test_single_joinable_throw (__main__.TestJoin) ... ok

----------------------------------------------------------------------
Ran 9 tests in 32.766s

OK
Running distributed/elastic/events/lib_test ... [2021-10-12 07:59:44.476684]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/elastic/events/lib_test.py', '-v'] ... [2021-10-12 07:59:44.476768]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 8 items

distributed/elastic/events/lib_test.py::EventLibTest::test_event_created PASSED [ 12%]
distributed/elastic/events/lib_test.py::EventLibTest::test_event_deser PASSED [ 25%]
distributed/elastic/events/lib_test.py::EventLibTest::test_get_or_create_logger PASSED [ 37%]
distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_construct_and_record_rdzv_event PASSED [ 50%]
distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_construct_and_record_rdzv_event_does_not_run_if_invalid_dest PASSED [ 62%]
distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_created PASSED [ 75%]
distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_deserialize PASSED [ 87%]
distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_str PASSED [100%]

============================== 8 passed in 0.87s ===============================
Running distributed/elastic/metrics/api_test ... [2021-10-12 07:59:47.048416]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/metrics/api_test.py', '-v'] ... [2021-10-12 07:59:47.048504]
test_get_metric_name (__main__.MetricsApiTest) ... ok
test_inheritance (__main__.MetricsApiTest) ... ok
test_profile (__main__.MetricsApiTest) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.001s

OK
Running distributed/elastic/multiprocessing/api_test ... [2021-10-12 07:59:49.382078]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/multiprocessing/api_test.py', '-v'] ... [2021-10-12 07:59:49.382163]
test_get_failures (__main__.RunProcResultsTest) ... ok
test_is_failed (__main__.RunProcResultsTest) ... ok
test_args_env_len_mismatch (__main__.StartProcessesListTest) ... ok
test_binary (__main__.StartProcessesListTest) ... hello stderr from 0
hello stdout from 0
hello stderr from 1
hello stdout from 1
ok
test_binary_exit (__main__.StartProcessesListTest) ... bar stderr from 1
bar stdout from 1
failed (exitcode: 138) local_rank: 0 (pid: 556820) of binary: distributed/elastic/multiprocessing/bin/echo1.py
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesListTest_n2v7zynf/tmpllmsch47/0/stdout.log' mode='w' encoding='UTF-8'>
  testMethod()
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesListTest_n2v7zynf/tmpllmsch47/0/stderr.log' mode='w' encoding='UTF-8'>
  testMethod()
ok
test_binary_incorrect_entrypoint (__main__.StartProcessesListTest) ... ok
test_binary_raises (__main__.StartProcessesListTest) ... Traceback (most recent call last):
  File "distributed/elastic/multiprocessing/bin/echo2.py", line 22, in <module>
    raise RuntimeError(f"raised from {rank}")
RuntimeError: raised from 0
bar from 1
failed (exitcode: 1) local_rank: 0 (pid: 556823) of binary: distributed/elastic/multiprocessing/bin/echo2.py
ok
test_binary_redirect_and_tee (__main__.StartProcessesListTest) ... world stdout from 1
[trainer1]:world stderr from 1
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesListTest_hwk6tmo1/tmpthb3ox8n/0/stdout.log' mode='w' encoding='UTF-8'>
  testMethod()
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesListTest_hwk6tmo1/tmpthb3ox8n/0/stderr.log' mode='w' encoding='UTF-8'>
  testMethod()
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesListTest_hwk6tmo1/tmpthb3ox8n/1/stderr.log' mode='w' encoding='UTF-8'>
  testMethod()
ok
test_function (__main__.StartProcessesListTest) ... [trainer0]:hello stdout from 0
hello stdout from 0
hello stdout from 1
hello stderr from 1
hello stderr from 0
Closing process 556831 via signal SIGTERM
ok
test_function_large_ret_val (__main__.StartProcessesListTest) ... Closing process 557092 via signal SIGTERM
Closing process 557093 via signal SIGTERM
Closing process 557094 via signal SIGTERM
ok
test_function_raise (__main__.StartProcessesListTest) ... failed (exitcode: 1) local_rank: 0 (pid: 557616) of fn: echo2 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 382, in _wrap
    ret = record(fn)(*args_)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 364, in wrapper
    return f(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/elastic/multiprocessing/api_test.py", line 134, in echo2
    raise RuntimeError(msg)
RuntimeError: hello

ok
test_function_with_tensor (__main__.StartProcessesListTest) ... ok
test_invalid_log_dir (__main__.StartProcessesListTest) ... ok
test_multiprocess_context_close (__main__.StartProcessesListTest) ... Closing process 557878 via signal SIGTERM
ok
test_multiprocessing_context_poll_raises_exception (__main__.StartProcessesListTest) ... failed (exitcode: -1) local_rank: 0 (pid: 123) of fn: echo0 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 939, in __call__
    return _mock_self._mock_call(*args, **kwargs)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 999, in _mock_call
    raise effect
torch.multiprocessing.spawn.ProcessRaisedException: test msg
ok
test_pcontext_wait (__main__.StartProcessesListTest) ... ok
test_subprocess_context_close (__main__.StartProcessesListTest) ... Sending process 558010 closing signal SIGTERM
ok
test_to_map (__main__.StartProcessesListTest) ... ok
test_validate_full_rank (__main__.StartProcessesListTest) ... ok
test_void_function (__main__.StartProcessesListTest) ... hello
world
Closing process 558012 via signal SIGTERM
ok
test_args_env_len_mismatch (__main__.StartProcessesNotCITest) ... ok
test_binary_exit (__main__.StartProcessesNotCITest) ... bar stderr from 1
bar stdout from 1
failed (exitcode: 138) local_rank: 0 (pid: 558273) of binary: distributed/elastic/multiprocessing/bin/echo1.py
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesNotCITest_82_rxsn8/tmp0bnb9wmb/0/stdout.log' mode='w' encoding='UTF-8'>
  testMethod()
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesNotCITest_82_rxsn8/tmp0bnb9wmb/0/stderr.log' mode='w' encoding='UTF-8'>
  testMethod()
ok
test_binary_incorrect_entrypoint (__main__.StartProcessesNotCITest) ... ok
test_binary_raises (__main__.StartProcessesNotCITest) ... Traceback (most recent call last):
  File "distributed/elastic/multiprocessing/bin/echo2.py", line 22, in <module>
    raise RuntimeError(f"raised from {rank}")
RuntimeError: raised from 0
bar from 1
failed (exitcode: 1) local_rank: 0 (pid: 558276) of binary: distributed/elastic/multiprocessing/bin/echo2.py
ok
test_binary_signal (__main__.StartProcessesNotCITest) ... bar from 1
failed (exitcode: -11) local_rank: 0 (pid: 558278) of binary: distributed/elastic/multiprocessing/bin/echo3.py
ok
test_function (__main__.StartProcessesNotCITest) ... hello stdout from 0
hello stdout from 1
hello stderr from 1
hello stderr from 0
Closing process 558280 via signal SIGTERM
hello stderr from 0
hello stderr from 1
Closing process 558542 via signal SIGTERM
hello stdout from 1
hello stdout from 0
Closing process 558804 via signal SIGTERM
Closing process 559066 via signal SIGTERM
ok
test_function_exit (__main__.StartProcessesNotCITest) ... failed (exitcode: 138) local_rank: 0 (pid: 559328) of fn: echo1 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 144, in join
    exit_code=exitcode
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 138
ok
test_function_large_ret_val (__main__.StartProcessesNotCITest) ... Closing process 559590 via signal SIGTERM
Closing process 559592 via signal SIGTERM
Closing process 559593 via signal SIGTERM
ok
test_function_raise (__main__.StartProcessesNotCITest) ... failed (exitcode: 1) local_rank: 0 (pid: 560114) of fn: echo2 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 382, in _wrap
    ret = record(fn)(*args_)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 364, in wrapper
    return f(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/elastic/multiprocessing/api_test.py", line 134, in echo2
    raise RuntimeError(msg)
RuntimeError: hello

ok
test_function_redirect_and_tee (__main__.StartProcessesNotCITest) ... world stdout from 1
[trainer1]:world stderr from 1
Closing process 560377 via signal SIGTERM
ok
test_function_with_tensor (__main__.StartProcessesNotCITest) ... ok
test_invalid_log_dir (__main__.StartProcessesNotCITest) ... ok
test_multiprocess_context_close (__main__.StartProcessesNotCITest) ... [trainer0]:hello stdout from 0
Closing process 560640 via signal SIGTERM
ok
test_multiprocessing_context_poll_raises_exception (__main__.StartProcessesNotCITest) ... failed (exitcode: -1) local_rank: 0 (pid: 123) of fn: echo0 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 939, in __call__
    return _mock_self._mock_call(*args, **kwargs)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 999, in _mock_call
    raise effect
torch.multiprocessing.spawn.ProcessRaisedException: test msg
ok
test_no_zombie_process_binary (__main__.StartProcessesNotCITest) ... Sending process 560772 closing signal SIGTERM
Sending process 560773 closing signal SIGTERM
Sending process 560906 closing signal SIGTERM
Sending process 560907 closing signal SIGTERM
Sending process 561040 closing signal SIGTERM
Sending process 561041 closing signal SIGTERM
Sending process 561174 closing signal SIGTERM
Sending process 561175 closing signal SIGTERM
ok
test_no_zombie_process_function (__main__.StartProcessesNotCITest) ... Closing process 561309 via signal SIGTERM
Closing process 561310 via signal SIGTERM
Closing process 561444 via signal SIGTERM
Closing process 561445 via signal SIGTERM
Closing process 561579 via signal SIGTERM
Closing process 561580 via signal SIGTERM
Closing process 561714 via signal SIGTERM
Closing process 561715 via signal SIGTERM
ok
test_pcontext_wait (__main__.StartProcessesNotCITest) ... ok
test_subprocess_context_close (__main__.StartProcessesNotCITest) ... Sending process 561848 closing signal SIGTERM
ok
test_to_map (__main__.StartProcessesNotCITest) ... ok
test_validate_full_rank (__main__.StartProcessesNotCITest) ... ok
test_void_function (__main__.StartProcessesNotCITest) ... world
hello
Closing process 561850 via signal SIGTERM
ok
test_wrap_bad (__main__.StartProcessesNotCITest) ... hello stderr from 0
hello stdout from 0
hello stdout from 0
hello stderr from 0
ok
test_args_env_len_mismatch (__main__.StartProcessesTest) ... ok
test_binary_exit (__main__.StartProcessesTest) ... bar stderr from 1
bar stdout from 1
failed (exitcode: 138) local_rank: 0 (pid: 562111) of binary: distributed/elastic/multiprocessing/bin/echo1.py
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesTest_6i396u26/tmpoj48blwu/0/stdout.log' mode='w' encoding='UTF-8'>
  testMethod()
/opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/StartProcessesTest_6i396u26/tmpoj48blwu/0/stderr.log' mode='w' encoding='UTF-8'>
  testMethod()
ok
test_binary_incorrect_entrypoint (__main__.StartProcessesTest) ... ok
test_binary_raises (__main__.StartProcessesTest) ... Traceback (most recent call last):
  File "distributed/elastic/multiprocessing/bin/echo2.py", line 22, in <module>
    raise RuntimeError(f"raised from {rank}")
RuntimeError: raised from 0
bar from 1
failed (exitcode: 1) local_rank: 0 (pid: 562114) of binary: distributed/elastic/multiprocessing/bin/echo2.py
ok
test_function_large_ret_val (__main__.StartProcessesTest) ... Closing process 562117 via signal SIGTERM
Closing process 562118 via signal SIGTERM
Closing process 562119 via signal SIGTERM
ok
test_function_raise (__main__.StartProcessesTest) ... failed (exitcode: 1) local_rank: 0 (pid: 562640) of fn: echo2 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 382, in _wrap
    ret = record(fn)(*args_)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 364, in wrapper
    return f(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/elastic/multiprocessing/api_test.py", line 134, in echo2
    raise RuntimeError(msg)
RuntimeError: hello

ok
test_function_with_tensor (__main__.StartProcessesTest) ... ok
test_invalid_log_dir (__main__.StartProcessesTest) ... ok
test_multiprocess_context_close (__main__.StartProcessesTest) ... Closing process 562902 via signal SIGTERM
ok
test_multiprocessing_context_poll_raises_exception (__main__.StartProcessesTest) ... failed (exitcode: -1) local_rank: 0 (pid: 123) of fn: echo0 (start_method: spawn)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 466, in _poll
    self._pc.join(-1)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 939, in __call__
    return _mock_self._mock_call(*args, **kwargs)
  File "/opt/conda/lib/python3.6/unittest/mock.py", line 999, in _mock_call
    raise effect
torch.multiprocessing.spawn.ProcessRaisedException: test msg
ok
test_pcontext_wait (__main__.StartProcessesTest) ... ok
test_subprocess_context_close (__main__.StartProcessesTest) ... Sending process 563034 closing signal SIGTERM
ok
test_to_map (__main__.StartProcessesTest) ... ok
test_validate_full_rank (__main__.StartProcessesTest) ... ok
test_void_function (__main__.StartProcessesTest) ... world
hello
Closing process 563036 via signal SIGTERM
ok
test_from_str_bad_input (__main__.StdTest) ... ok
test_from_value (__main__.StdTest) ... ok
test_from_value_map (__main__.StdTest) ... ok

----------------------------------------------------------------------
Ran 60 tests in 102.243s

OK
Running distributed/elastic/timer/api_test ... [2021-10-12 08:01:33.942901]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/timer/api_test.py', '-v'] ... [2021-10-12 08:01:33.942982]
Running distributed/elastic/timer/local_timer_example ... [2021-10-12 08:01:35.978526]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/timer/local_timer_example.py', '-v'] ... [2021-10-12 08:01:35.978606]
test_example_start_method_spawn (__main__.LocalTimerExample) ... [INFO] 2021-10-12 08:01:36,972 api: Starting LocalTimerServer... max_interval=0.01, daemon=True
[INFO] 2021-10-12 08:01:36,972 api: Starting watchdog thread...
[INFO] 2021-10-12 08:01:38,083 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,097 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,112 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,120 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,145 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,154 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,154 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:38,195 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:39,165 api: Reaping worker_id=[563564]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:39,166 api: Successfully reaped worker=[563564]
[INFO] 2021-10-12 08:01:39,196 api: Reaping worker_id=[563560]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:39,197 api: Successfully reaped worker=[563560]
[INFO] 2021-10-12 08:01:39,217 api: Reaping worker_id=[563566]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:39,217 api: Successfully reaped worker=[563566]
[INFO] 2021-10-12 08:01:39,248 api: Reaping worker_id=[563562]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:39,248 api: Successfully reaped worker=[563562]
[INFO] 2021-10-12 08:01:40,053 api: Stopping LocalTimerServer
[INFO] 2021-10-12 08:01:40,053 api: Stopping watchdog thread...
ok
test_torch_mp_example (__main__.LocalTimerExample) ... [INFO] 2021-10-12 08:01:40,063 api: Starting LocalTimerServer... max_interval=0.01, daemon=True
[INFO] 2021-10-12 08:01:40,064 api: Starting watchdog thread...
[INFO] 2021-10-12 08:01:41,169 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,213 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,216 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,228 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,230 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,241 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,241 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:41,242 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,337 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,377 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,388 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,389 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,400 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,421 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,426 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:44,444 api: Timer client configured to: LocalTimerClient
[INFO] 2021-10-12 08:01:45,412 api: Reaping worker_id=[565679]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,412 api: Successfully reaped worker=[565679]
[INFO] 2021-10-12 08:01:45,473 api: Reaping worker_id=[565676]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,473 api: Successfully reaped worker=[565676]
[INFO] 2021-10-12 08:01:45,494 api: Reaping worker_id=[565678]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,494 api: Successfully reaped worker=[565678]
[INFO] 2021-10-12 08:01:45,494 api: Reaping worker_id=[565680]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,494 api: Successfully reaped worker=[565680]
[INFO] 2021-10-12 08:01:45,504 api: Reaping worker_id=[565673]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,504 local_timer: Process with pid=565673 does not exist. Skipping
[INFO] 2021-10-12 08:01:45,505 api: Successfully reaped worker=[565673]
[INFO] 2021-10-12 08:01:45,515 api: Reaping worker_id=[565677]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,515 api: Successfully reaped worker=[565677]
[INFO] 2021-10-12 08:01:45,525 api: Reaping worker_id=[565674]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,525 local_timer: Process with pid=565674 does not exist. Skipping
[INFO] 2021-10-12 08:01:45,526 api: Successfully reaped worker=[565674]
[INFO] 2021-10-12 08:01:45,536 api: Reaping worker_id=[565675]. Expired timers: ['/opt/conda/lib/python3.6/contextlib.py#81']
[INFO] 2021-10-12 08:01:45,536 local_timer: Process with pid=565675 does not exist. Skipping
[INFO] 2021-10-12 08:01:45,536 api: Successfully reaped worker=[565675]
[INFO] 2021-10-12 08:01:45,542 api: Stopping LocalTimerServer
[INFO] 2021-10-12 08:01:45,543 api: Stopping watchdog thread...
ok

----------------------------------------------------------------------
Ran 2 tests in 8.580s

OK
Running distributed/elastic/timer/local_timer_test ... [2021-10-12 08:01:46.768869]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/timer/local_timer_test.py', '-v'] ... [2021-10-12 08:01:46.768945]
test_acquire_release (__main__.LocalTimerServerTest) ... ok
test_expired_timers (__main__.LocalTimerServerTest) ... ok
test_valid_timers (__main__.LocalTimerServerTest) ... ok
test_watchdog_call_count (__main__.LocalTimerServerTest) ... ok
test_watchdog_empty_queue (__main__.LocalTimerServerTest) ... ok
test_client_interaction (__main__.LocalTimerTest) ... ok
test_exception_propagation (__main__.LocalTimerTest) ... ok
test_get_timer_recursive (__main__.LocalTimerTest) ... ok
test_happy_path (__main__.LocalTimerTest) ... ok
test_no_client (__main__.LocalTimerTest) ... ok
test_timer (__main__.LocalTimerTest) ... ok
test_get (__main__.MultiprocessingRequestQueueTest) ... ok
test_get_less_than_size (__main__.MultiprocessingRequestQueueTest) ... ok
test_get_size (__main__.MultiprocessingRequestQueueTest) ... ok

----------------------------------------------------------------------
Ran 14 tests in 3.381s

OK
Running distributed/elastic/utils/distributed_test ... [2021-10-12 08:01:52.741148]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/utils/distributed_test.py', '-v'] ... [2021-10-12 08:01:52.741234]
test_create_store_multi (__main__.DistributedUtilTest) ... ok
test_create_store_no_port_multi (__main__.DistributedUtilTest) ... ok
test_create_store_single_server (__main__.DistributedUtilTest) ... ok
test_create_store_timeout_on_server (__main__.DistributedUtilTest) ... ok
test_create_store_timeout_on_worker (__main__.DistributedUtilTest) ... ok
test_port_already_in_use_on_server (__main__.DistributedUtilTest) ... port: 36951 already in use, attempt: [1/3]
port: 36951 already in use, attempt: [2/3]
ok
test_port_already_in_use_on_worker (__main__.DistributedUtilTest) ... ok

----------------------------------------------------------------------
Ran 7 tests in 5.180s

OK
Running distributed/elastic/utils/logging_test ... [2021-10-12 08:02:00.281028]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/utils/logging_test.py', '-v'] ... [2021-10-12 08:02:00.281103]
test_derive_module_name (__main__.LoggingTest) ... ok
test_logger_name (__main__.LoggingTest) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.002s

OK
Running distributed/elastic/utils/util_test ... [2021-10-12 08:02:02.681359]
Executing ['/opt/conda/bin/python3.6', 'distributed/elastic/utils/util_test.py', '-v'] ... [2021-10-12 08:02:02.681444]
test_get_data (__main__.StoreUtilTest) ... ok
test_synchronize (__main__.StoreUtilTest) ... ok
test_get_logger (__main__.UtilTest) ... ok
test_get_logger_custom_name (__main__.UtilTest) ... ok
test_get_logger_different (__main__.UtilTest) ... ok
test_get_logger_none (__main__.UtilTest) ... ok

----------------------------------------------------------------------
Ran 6 tests in 0.051s

OK
Running distributed/optim/test_zero_redundancy_optimizer ... [2021-10-12 08:02:05.019153]
Executing ['/opt/conda/bin/python3.6', 'distributed/optim/test_zero_redundancy_optimizer.py', '-v'] ... [2021-10-12 08:02:05.019234]
test_add_param_group (__main__.TestZeroRedundancyOptimizerDistributed)
Check that ZeroRedundancyOptimizer properly handles adding a new param_group a posteriori, ... ok
test_collect_shards (__main__.TestZeroRedundancyOptimizerDistributed)
Check the state consolidation mechanism, and the state dict exposed by ZeroRedundancyOptimizer ... ok
test_ddp_with_zero_step_interleaved_parity_gpu (__main__.TestZeroRedundancyOptimizerDistributed) ... skipped 'Test skipped for ROCm'
test_ddp_with_zero_step_interleaved_uniform_parity_gpu (__main__.TestZeroRedundancyOptimizerDistributed) ... skipped 'Test skipped for ROCm'
test_ddp_with_zero_step_parity_gpu (__main__.TestZeroRedundancyOptimizerDistributed) ... skipped 'Test skipped for ROCm'
test_ddp_with_zero_step_uniform_parity_gpu (__main__.TestZeroRedundancyOptimizerDistributed) ... skipped 'Test skipped for ROCm'
test_local_optimizer_parity (__main__.TestZeroRedundancyOptimizerDistributed)
When combined with DDP, check that ZeroRedundancyOptimizer(optimizer) and the same monolithic optimizer ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
WARNING:root:ZeroRedundancyOptimizer detected that the trainable parameters changed; rebuilding the parameter buckets if enabled
ok
test_multiple_groups (__main__.TestZeroRedundancyOptimizerDistributed)
Check that the ZeroRedundancyOptimizer handles working with multiple process groups ... ok
test_sharding (__main__.TestZeroRedundancyOptimizerDistributed)
Check the sharding at construction time ... ok
test_step (__main__.TestZeroRedundancyOptimizerDistributed)
Check that the ZeroRedundancyOptimizer wrapper properly exposes the `.step()` interface ... skipped 'Test skipped for ROCm'
test_step_with_closure (__main__.TestZeroRedundancyOptimizerDistributed)
Check that the ZeroRedundancyOptimizer wrapper properly exposes the `.step(closure)` interface ... skipped 'Test skipped for ROCm'
test_zero_join_cpu (__main__.TestZeroRedundancyOptimizerDistributed)
Check that the ZeRO join hook allows training with uneven inputs on CPU. ... ok
test_zero_join_gpu (__main__.TestZeroRedundancyOptimizerDistributed)
Check that the ZeRO join hook allows training with uneven inputs on GPU. ... ok
test_zero_model_parallel_with_bucket_view (__main__.TestZeroRedundancyOptimizerDistributed) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 901, in test_zero_model_parallel_with_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=True)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step
 exiting process 0 with exit code: 10
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 901, in test_zero_model_parallel_with_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=True)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step
 exiting process 1 with exit code: 10
ERROR
test_zero_model_parallel_without_bucket_view (__main__.TestZeroRedundancyOptimizerDistributed) ... Process 0 terminated with exit code 10, terminating remaining processes.
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 912, in test_zero_model_parallel_without_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=False)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step
 exiting process 1 with exit code: 10
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 912, in test_zero_model_parallel_without_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=False)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step
 exiting process 0 with exit code: 10
ERROR
test_constructor (__main__.TestZeroRedundancyOptimizerSingleRank)
Check the robustness of the ZeroRedundancyOptimizer constructor by ... Process 1 terminated with exit code 10, terminating remaining processes.
ok
test_lr_scheduler (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that a normal torch lr_scheduler is usable with ZeroRedundancyOptimizer ... ok
test_same_dense_param_type (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that ZeroRedundancyOptimizer raises an exception if the input ... ok
test_state_dict (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that the ZeroRedundancyOptimizer exposes the expected state dict interface, ... ok
test_step_with_extra_inner_key (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that an optimizer adding extra keys to the param_groups ... ok
test_step_with_kwargs (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that the `step(**kwargs)` interface is properly exposed ... ok
test_step_without_closure (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that the step() method (without closure) is handlded as expected ... ok
test_zero_grad (__main__.TestZeroRedundancyOptimizerSingleRank)
Check that the zero_grad attribute is properly handled ... ok

======================================================================
ERROR: test_zero_model_parallel_with_bucket_view (__main__.TestZeroRedundancyOptimizerDistributed)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 418, in wrapper
    self._join_processes(fn)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 637, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 682, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 901, in test_zero_model_parallel_with_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=True)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step



======================================================================
ERROR: test_zero_model_parallel_without_bucket_view (__main__.TestZeroRedundancyOptimizerDistributed)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 418, in wrapper
    self._join_processes(fn)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 637, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 682, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 536, in run_test
    getattr(self, test_name)()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 420, in wrapper
    fn()
  File "/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py", line 111, in wrapper
    return func(*args, **kwargs)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 912, in test_zero_model_parallel_without_bucket_view
    self._test_zero_model_parallel(parameters_as_bucket_view=False)
  File "/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 890, in _test_zero_model_parallel
    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
AssertionError: Models differ after a step



----------------------------------------------------------------------
Ran 23 tests in 150.200s

FAILED (errors=2, skipped=6)
distributed/optim/test_zero_redundancy_optimizer failed!
Running distributed/pipeline/sync/skip/test_api ... [2021-10-12 08:04:37.980418]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_api.py', '-v'] ... [2021-10-12 08:04:37.980488]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 3 items

distributed/pipeline/sync/skip/test_api.py::test_namespace_difference PASSED [ 33%]
distributed/pipeline/sync/skip/test_api.py::test_namespace_copy PASSED   [ 66%]
distributed/pipeline/sync/skip/test_api.py::test_skippable_repr PASSED   [100%]

============================== 3 passed in 0.03s ===============================
Running distributed/pipeline/sync/skip/test_gpipe ... [2021-10-12 08:04:40.472049]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_gpipe.py', '-v'] ... [2021-10-12 08:04:40.472131]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 13 items

distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[never-3] PASSED  [  7%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[never-1:2] PASSED [ 15%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[never-2:1] PASSED [ 23%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[never-1:1:1] PASSED [ 30%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[always-3] PASSED [ 38%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[always-1:2] PASSED [ 46%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[always-2:1] PASSED [ 53%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[always-1:1:1] PASSED [ 61%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[except_last-3] PASSED [ 69%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[except_last-1:2] PASSED [ 76%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[except_last-2:1] PASSED [ 84%]
distributed/pipeline/sync/skip/test_gpipe.py::test_1to3[except_last-1:1:1] PASSED [ 92%]
distributed/pipeline/sync/skip/test_gpipe.py::test_none_skip PASSED      [100%]

============================= 13 passed in 17.03s ==============================
Running distributed/pipeline/sync/skip/test_inspect_skip_layout ... [2021-10-12 08:05:01.680769]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_inspect_skip_layout.py', '-v'] ... [2021-10-12 08:05:01.680857]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 6 items

distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_no_skippables PASSED [ 16%]
distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_inner_partition PASSED [ 33%]
distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_adjoining_partitions PASSED [ 50%]
distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_far_partitions PASSED [ 66%]
distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_pop_2_from_different_partitions PASSED [ 83%]
distributed/pipeline/sync/skip/test_inspect_skip_layout.py::test_namespace PASSED [100%]

============================== 6 passed in 0.02s ===============================
Running distributed/pipeline/sync/skip/test_leak ... [2021-10-12 08:05:04.106353]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_leak.py', '-v'] ... [2021-10-12 08:05:04.106436]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 8 items

distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[always-train] PASSED [ 12%]
distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[always-eval] PASSED [ 25%]
distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[except_last-train] PASSED [ 37%]
distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[except_last-eval] PASSED [ 50%]
distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[never-train] PASSED [ 62%]
distributed/pipeline/sync/skip/test_leak.py::test_delete_portal_tensor[never-eval] PASSED [ 75%]
distributed/pipeline/sync/skip/test_leak.py::test_no_portal_without_pipe[train] PASSED [ 87%]
distributed/pipeline/sync/skip/test_leak.py::test_no_portal_without_pipe[eval] PASSED [100%]

============================== 8 passed in 0.62s ===============================
Running distributed/pipeline/sync/skip/test_portal ... [2021-10-12 08:05:07.185002]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_portal.py', '-v'] ... [2021-10-12 08:05:07.185085]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 10 items

distributed/pipeline/sync/skip/test_portal.py::test_copy_returns_on_next_device PASSED [ 10%]
distributed/pipeline/sync/skip/test_portal.py::test_blue_orange PASSED   [ 20%]
distributed/pipeline/sync/skip/test_portal.py::test_blue_orange_not_requires_grad PASSED [ 30%]
distributed/pipeline/sync/skip/test_portal.py::test_use_grad PASSED      [ 40%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_0 PASSED [ 50%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_1 PASSED [ 60%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_2 PASSED [ 70%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_3 PASSED [ 80%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_4 PASSED [ 90%]
distributed/pipeline/sync/skip/test_portal.py::TestTensorLife::test_tensor_life_3_plus_1 PASSED [100%]

============================== 10 passed in 0.61s ==============================
Running distributed/pipeline/sync/skip/test_stash_pop ... [2021-10-12 08:05:10.396750]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_stash_pop.py', '-v'] ... [2021-10-12 08:05:10.396833]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 7 items

distributed/pipeline/sync/skip/test_stash_pop.py::test_stash PASSED      [ 14%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_pop PASSED        [ 28%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_declare_but_not_use PASSED [ 42%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_stash_not_declared PASSED [ 57%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_pop_not_declared PASSED [ 71%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_pop_not_stashed PASSED [ 85%]
distributed/pipeline/sync/skip/test_stash_pop.py::test_stash_none PASSED [100%]

============================== 7 passed in 0.03s ===============================
Running distributed/pipeline/sync/skip/test_tracker ... [2021-10-12 08:05:12.807519]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_tracker.py', '-v'] ... [2021-10-12 08:05:12.807590]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 6 items

distributed/pipeline/sync/skip/test_tracker.py::test_default_skip_tracker PASSED [ 16%]
distributed/pipeline/sync/skip/test_tracker.py::test_default_skip_tracker_by_data_parallel PASSED [ 33%]
distributed/pipeline/sync/skip/test_tracker.py::test_reuse_portal PASSED [ 50%]
distributed/pipeline/sync/skip/test_tracker.py::test_no_copy_no_portal PASSED [ 66%]
distributed/pipeline/sync/skip/test_tracker.py::test_tensor_life_without_checkpointing PASSED [ 83%]
distributed/pipeline/sync/skip/test_tracker.py::test_tensor_life_with_checkpointing PASSED [100%]

============================== 6 passed in 0.75s ===============================
Running distributed/pipeline/sync/skip/test_verify_skippables ... [2021-10-12 08:05:16.060803]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/skip/test_verify_skippables.py', '-v'] ... [2021-10-12 08:05:16.060897]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 9 items

distributed/pipeline/sync/skip/test_verify_skippables.py::test_matching PASSED [ 11%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_stash_not_pop PASSED [ 22%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_pop_unknown PASSED [ 33%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_stash_again PASSED [ 44%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_pop_again PASSED [ 55%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_stash_pop_together_different_names PASSED [ 66%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_stash_pop_together_same_name PASSED [ 77%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_double_stash_pop PASSED [ 88%]
distributed/pipeline/sync/skip/test_verify_skippables.py::test_double_stash_pop_but_isolated PASSED [100%]

============================== 9 passed in 0.03s ===============================
Running distributed/pipeline/sync/test_balance ... [2021-10-12 08:05:18.461935]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_balance.py', '-v'] ... [2021-10-12 08:05:18.462016]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 18 items

distributed/pipeline/sync/test_balance.py::test_blockpartition PASSED    [  5%]
distributed/pipeline/sync/test_balance.py::test_blockpartition_zeros PASSED [ 11%]
distributed/pipeline/sync/test_balance.py::test_blockpartition_non_positive_partitions PASSED [ 16%]
distributed/pipeline/sync/test_balance.py::test_blockpartition_short_sequence PASSED [ 22%]
distributed/pipeline/sync/test_balance.py::test_balance_by_time[cpu] SKIPPED [ 27%]
distributed/pipeline/sync/test_balance.py::test_balance_by_time[cuda] SKIPPED [ 33%]
distributed/pipeline/sync/test_balance.py::test_balance_by_time_loop_resets_input PASSED [ 38%]
distributed/pipeline/sync/test_balance.py::test_balance_by_size_latent PASSED [ 44%]
distributed/pipeline/sync/test_balance.py::test_balance_by_size_param PASSED [ 50%]
distributed/pipeline/sync/test_balance.py::test_balance_by_size_param_scale PASSED [ 55%]
distributed/pipeline/sync/test_balance.py::test_layerwise_sandbox[cpu] PASSED [ 61%]
distributed/pipeline/sync/test_balance.py::test_layerwise_sandbox[cuda] PASSED [ 66%]
distributed/pipeline/sync/test_balance.py::test_sandbox_during_profiling[cpu] PASSED [ 72%]
distributed/pipeline/sync/test_balance.py::test_sandbox_during_profiling[cuda] PASSED [ 77%]
distributed/pipeline/sync/test_balance.py::test_not_training PASSED      [ 83%]
distributed/pipeline/sync/test_balance.py::test_balance_by_time_tuple PASSED [ 88%]
distributed/pipeline/sync/test_balance.py::test_balance_by_size_tuple PASSED [ 94%]
distributed/pipeline/sync/test_balance.py::test_already_has_grad PASSED  [100%]

=========================== short test summary info ============================
SKIPPED [2] distributed/pipeline/sync/test_balance.py:45: Flaky due to time.sleep()
======================== 16 passed, 2 skipped in 15.25s ========================
Running distributed/pipeline/sync/test_bugs ... [2021-10-12 08:05:37.796672]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_bugs.py', '-v'] ... [2021-10-12 08:05:37.796760]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 4 items

distributed/pipeline/sync/test_bugs.py::test_python_autograd_function PASSED [ 25%]
distributed/pipeline/sync/test_bugs.py::test_exception_no_hang PASSED    [ 50%]
distributed/pipeline/sync/test_bugs.py::test_tuple_wait PASSED           [ 75%]
distributed/pipeline/sync/test_bugs.py::test_parallel_randoms PASSED     [100%]

============================== 4 passed in 2.46s ===============================
Running distributed/pipeline/sync/test_checkpoint ... [2021-10-12 08:05:42.488946]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_checkpoint.py', '-v'] ... [2021-10-12 08:05:42.489036]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 9 items

distributed/pipeline/sync/test_checkpoint.py::test_serial_checkpoints[cpu] PASSED [ 11%]
distributed/pipeline/sync/test_checkpoint.py::test_serial_checkpoints[cuda] PASSED [ 22%]
distributed/pipeline/sync/test_checkpoint.py::test_not_requires_grad PASSED [ 33%]
distributed/pipeline/sync/test_checkpoint.py::test_not_requires_grad_with_parameter PASSED [ 44%]
distributed/pipeline/sync/test_checkpoint.py::test_random_in_checkpoint[cpu] PASSED [ 55%]
distributed/pipeline/sync/test_checkpoint.py::test_random_in_checkpoint[cuda] PASSED [ 66%]
distributed/pipeline/sync/test_checkpoint.py::test_detect_checkpointing_recomputing PASSED [ 77%]
distributed/pipeline/sync/test_checkpoint.py::test_detect_checkpointing_recomputing_without_checkpoint PASSED [ 88%]
distributed/pipeline/sync/test_checkpoint.py::test_non_grad_output PASSED [100%]

============================== 9 passed in 0.90s ===============================
Running distributed/pipeline/sync/test_copy ... [2021-10-12 08:05:45.948792]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_copy.py', '-v'] ... [2021-10-12 08:05:45.948877]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 5 items

distributed/pipeline/sync/test_copy.py::test_copy_wait_cpu_cpu PASSED    [ 20%]
distributed/pipeline/sync/test_copy.py::test_copy_wait_cpu_cuda PASSED   [ 40%]
distributed/pipeline/sync/test_copy.py::test_copy_wait_cuda_cpu PASSED   [ 60%]
distributed/pipeline/sync/test_copy.py::test_copy_wait_cuda_cuda PASSED  [ 80%]
distributed/pipeline/sync/test_copy.py::test_wait_multiple_tensors PASSED [100%]

============================== 5 passed in 1.23s ===============================
Running distributed/pipeline/sync/test_deferred_batch_norm ... [2021-10-12 08:05:49.732716]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_deferred_batch_norm.py', '-v'] ... [2021-10-12 08:05:49.732799]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 11 items

distributed/pipeline/sync/test_deferred_batch_norm.py::test_transparency[True-1] PASSED [  9%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_transparency[True-4] PASSED [ 18%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_transparency[False-1] PASSED [ 27%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_transparency[False-4] PASSED [ 36%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_running_stats[0.1] PASSED [ 45%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_running_stats[None] PASSED [ 54%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_convert_deferred_batch_norm PASSED [ 63%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_eval PASSED  [ 72%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_optimize PASSED [ 81%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_conv_bn PASSED [ 90%]
distributed/pipeline/sync/test_deferred_batch_norm.py::test_input_requiring_grad PASSED [100%]

============================== 11 passed in 0.87s ==============================
Running distributed/pipeline/sync/test_dependency ... [2021-10-12 08:05:53.083650]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_dependency.py', '-v'] ... [2021-10-12 08:05:53.083744]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 6 items

distributed/pipeline/sync/test_dependency.py::test_fork_join PASSED      [ 16%]
distributed/pipeline/sync/test_dependency.py::test_fork_join_enable_grad PASSED [ 33%]
distributed/pipeline/sync/test_dependency.py::test_fork_join_no_grad PASSED [ 50%]
distributed/pipeline/sync/test_dependency.py::test_fork_leak PASSED      [ 66%]
distributed/pipeline/sync/test_dependency.py::test_join_when_fork_not_requires_grad PASSED [ 83%]
distributed/pipeline/sync/test_dependency.py::test_join_when_fork_requires_grad PASSED [100%]

============================== 6 passed in 0.60s ===============================
Running distributed/pipeline/sync/test_inplace ... [2021-10-12 08:05:56.168711]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_inplace.py', '-v'] ... [2021-10-12 08:05:56.168795]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 3 items

distributed/pipeline/sync/test_inplace.py::test_inplace_on_requires_grad PASSED [ 33%]
distributed/pipeline/sync/test_inplace.py::test_inplace_on_not_requires_grad XFAIL [ 66%]
distributed/pipeline/sync/test_inplace.py::test_inplace_incorrect_grad XFAIL [100%]

=========================== short test summary info ============================
XFAIL distributed/pipeline/sync/test_inplace.py::test_inplace_on_not_requires_grad
XFAIL distributed/pipeline/sync/test_inplace.py::test_inplace_incorrect_grad
========================= 1 passed, 2 xfailed in 0.41s =========================
Running distributed/pipeline/sync/test_microbatch ... [2021-10-12 08:05:59.103619]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_microbatch.py', '-v'] ... [2021-10-12 08:05:59.103701]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 10 items

distributed/pipeline/sync/test_microbatch.py::test_batch_atomic PASSED   [ 10%]
distributed/pipeline/sync/test_microbatch.py::test_batch_non_atomic PASSED [ 20%]
distributed/pipeline/sync/test_microbatch.py::test_batch_call PASSED     [ 30%]
distributed/pipeline/sync/test_microbatch.py::test_batch_setitem_by_index PASSED [ 40%]
distributed/pipeline/sync/test_microbatch.py::test_batch_setitem_by_slice PASSED [ 50%]
distributed/pipeline/sync/test_microbatch.py::test_check PASSED          [ 60%]
distributed/pipeline/sync/test_microbatch.py::test_gather_tensors PASSED [ 70%]
distributed/pipeline/sync/test_microbatch.py::test_gather_tuples PASSED  [ 80%]
distributed/pipeline/sync/test_microbatch.py::test_scatter_tensor PASSED [ 90%]
distributed/pipeline/sync/test_microbatch.py::test_scatter_multiple_tensors PASSED [100%]

============================== 10 passed in 0.03s ==============================
Running distributed/pipeline/sync/test_phony ... [2021-10-12 08:06:01.492791]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_phony.py', '-v'] ... [2021-10-12 08:06:01.492842]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 4 items

distributed/pipeline/sync/test_phony.py::test_phony_size PASSED          [ 25%]
distributed/pipeline/sync/test_phony.py::test_phony_requires_grad PASSED [ 50%]
distributed/pipeline/sync/test_phony.py::test_cached_phony PASSED        [ 75%]
distributed/pipeline/sync/test_phony.py::test_phony_in_autograd_function PASSED [100%]

============================== 4 passed in 0.03s ===============================
Running distributed/pipeline/sync/test_pipe ... [2021-10-12 08:06:03.914930]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_pipe.py', '-v'] ... [2021-10-12 08:06:03.915012]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 55 items

distributed/pipeline/sync/test_pipe.py::test_pipe_without_rpc PASSED     [  1%]
distributed/pipeline/sync/test_pipe.py::test_parameters PASSED           [  3%]
distributed/pipeline/sync/test_pipe.py::test_public_attrs PASSED         [  5%]
distributed/pipeline/sync/test_pipe.py::test_sequential_like PASSED      [  7%]
distributed/pipeline/sync/test_pipe.py::test_chunks_less_than_1 PASSED   [  9%]
distributed/pipeline/sync/test_pipe.py::test_batch_size_indivisible PASSED [ 10%]
distributed/pipeline/sync/test_pipe.py::test_batch_size_small PASSED     [ 12%]
distributed/pipeline/sync/test_pipe.py::test_checkpoint_mode PASSED      [ 14%]
distributed/pipeline/sync/test_pipe.py::test_checkpoint_mode_invalid PASSED [ 16%]
distributed/pipeline/sync/test_pipe.py::test_checkpoint_mode_when_chunks_1 PASSED [ 18%]
distributed/pipeline/sync/test_pipe.py::test_checkpoint_eval PASSED      [ 20%]
distributed/pipeline/sync/test_pipe.py::test_checkpoint_non_float_input PASSED [ 21%]
distributed/pipeline/sync/test_pipe.py::test_no_grad PASSED              [ 23%]
distributed/pipeline/sync/test_pipe.py::test_exception PASSED            [ 25%]
distributed/pipeline/sync/test_pipe.py::test_exception_early_stop_asap PASSED [ 27%]
distributed/pipeline/sync/test_pipe.py::test_nested_input PASSED         [ 29%]
distributed/pipeline/sync/test_pipe.py::test_input_pair PASSED           [ 30%]
distributed/pipeline/sync/test_pipe.py::test_multi_sequence_input PASSED [ 32%]
distributed/pipeline/sync/test_pipe.py::test_input_singleton PASSED      [ 34%]
distributed/pipeline/sync/test_pipe.py::test_input_varargs PASSED        [ 36%]
distributed/pipeline/sync/test_pipe.py::test_non_tensor PASSED           [ 38%]
distributed/pipeline/sync/test_pipe.py::test_non_tensor_sequence PASSED  [ 40%]
distributed/pipeline/sync/test_pipe.py::test_valid_non_tensor[never] PASSED [ 41%]
distributed/pipeline/sync/test_pipe.py::test_valid_non_tensor[always] PASSED [ 43%]
distributed/pipeline/sync/test_pipe.py::test_valid_non_tensor[except_last] PASSED [ 45%]
distributed/pipeline/sync/test_pipe.py::test_no_tensor_output[never] PASSED [ 47%]
distributed/pipeline/sync/test_pipe.py::test_no_tensor_output[always] PASSED [ 49%]
distributed/pipeline/sync/test_pipe.py::test_no_tensor_output[except_last] PASSED [ 50%]
distributed/pipeline/sync/test_pipe.py::test_uneven_batch_size[never] PASSED [ 52%]
distributed/pipeline/sync/test_pipe.py::test_uneven_batch_size[always] PASSED [ 54%]
distributed/pipeline/sync/test_pipe.py::test_uneven_batch_size[except_last] PASSED [ 56%]
distributed/pipeline/sync/test_pipe.py::test_no_chunk[never] PASSED      [ 58%]
distributed/pipeline/sync/test_pipe.py::test_no_chunk[always] PASSED     [ 60%]
distributed/pipeline/sync/test_pipe.py::test_no_chunk[except_last] PASSED [ 61%]
distributed/pipeline/sync/test_pipe.py::test_deferred_batch_norm[never] PASSED [ 63%]
distributed/pipeline/sync/test_pipe.py::test_deferred_batch_norm[always] PASSED [ 65%]
distributed/pipeline/sync/test_pipe.py::test_deferred_batch_norm[except_last] PASSED [ 67%]
distributed/pipeline/sync/test_pipe.py::test_deferred_batch_norm_params[never] PASSED [ 69%]
distributed/pipeline/sync/test_pipe.py::test_deferred_batch_norm_params[always] PASSED [ 70%]
distributed/pipeline/sync/test_pipe.py::test_devices PASSED              [ 72%]
distributed/pipeline/sync/test_pipe.py::test_partitions PASSED           [ 74%]
distributed/pipeline/sync/test_pipe.py::test_merged_partitions PASSED    [ 76%]
distributed/pipeline/sync/test_pipe.py::test_deny_moving PASSED          [ 78%]
distributed/pipeline/sync/test_pipe.py::test_empty_module PASSED         [ 80%]
distributed/pipeline/sync/test_pipe.py::test_named_children PASSED       [ 81%]
distributed/pipeline/sync/test_pipe.py::test_verify_module_non_sequential PASSED [ 83%]
distributed/pipeline/sync/test_pipe.py::test_verify_module_duplicate_children PASSED [ 85%]
distributed/pipeline/sync/test_pipe.py::test_verify_module_params_on_same_device PASSED [ 87%]
distributed/pipeline/sync/test_pipe.py::test_verify_nested_modules PASSED [ 89%]
distributed/pipeline/sync/test_pipe.py::test_verify_module_duplicate_parameters_on_same_device PASSED [ 90%]
distributed/pipeline/sync/test_pipe.py::test_forward_lockstep PASSED     [ 92%]
distributed/pipeline/sync/test_pipe.py::test_multiple_inputs[never] PASSED [ 94%]
distributed/pipeline/sync/test_pipe.py::test_multiple_inputs[always] PASSED [ 96%]
distributed/pipeline/sync/test_pipe.py::test_multiple_inputs[except_last] PASSED [ 98%]
distributed/pipeline/sync/test_pipe.py::test_inputs_wrong_device PASSED  [100%]

============================= 55 passed in 15.16s ==============================
Running distributed/pipeline/sync/test_pipeline ... [2021-10-12 08:06:23.868911]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_pipeline.py', '-v'] ... [2021-10-12 08:06:23.868999]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 1 item

distributed/pipeline/sync/test_pipeline.py::test_clock_cycles PASSED     [100%]

============================== 1 passed in 0.02s ===============================
Running distributed/pipeline/sync/test_stream ... [2021-10-12 08:06:26.408228]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_stream.py', '-v'] ... [2021-10-12 08:06:26.408284]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 19 items

distributed/pipeline/sync/test_stream.py::TestNewStream::test_new_stream_cpu PASSED [  5%]
distributed/pipeline/sync/test_stream.py::TestNewStream::test_new_stream_cuda PASSED [ 10%]
distributed/pipeline/sync/test_stream.py::TestCurrentStream::test_current_stream_cpu PASSED [ 15%]
distributed/pipeline/sync/test_stream.py::TestCurrentStream::test_current_stream_cuda PASSED [ 21%]
distributed/pipeline/sync/test_stream.py::TestDefaultStream::test_default_stream_cpu PASSED [ 26%]
distributed/pipeline/sync/test_stream.py::TestDefaultStream::test_default_stream_cuda PASSED [ 31%]
distributed/pipeline/sync/test_stream.py::TestUseDevice::test_use_device_cpu PASSED [ 36%]
distributed/pipeline/sync/test_stream.py::TestUseDevice::test_use_device_cuda PASSED [ 42%]
distributed/pipeline/sync/test_stream.py::TestUseStream::test_use_stream_cpu PASSED [ 47%]
distributed/pipeline/sync/test_stream.py::TestUseStream::test_use_stream_cuda PASSED [ 52%]
distributed/pipeline/sync/test_stream.py::TestGetDevice::test_get_device_cpu PASSED [ 57%]
distributed/pipeline/sync/test_stream.py::TestGetDevice::test_get_device_cuda PASSED [ 63%]
distributed/pipeline/sync/test_stream.py::TestWaitStream::test_wait_stream_cpu_cpu PASSED [ 68%]
distributed/pipeline/sync/test_stream.py::TestWaitStream::test_wait_stream_cpu_cuda PASSED [ 73%]
distributed/pipeline/sync/test_stream.py::TestWaitStream::test_wait_stream_cuda_cpu PASSED [ 78%]
distributed/pipeline/sync/test_stream.py::TestWaitStream::test_wait_stream_cuda_cuda PASSED [ 84%]
distributed/pipeline/sync/test_stream.py::TestRecordStream::test_record_stream_cpu PASSED [ 89%]
distributed/pipeline/sync/test_stream.py::TestRecordStream::test_record_stream_cuda PASSED [ 94%]
distributed/pipeline/sync/test_stream.py::TestRecordStream::test_record_stream_shifted_view PASSED [100%]

============================== 19 passed in 1.23s ==============================
Running distributed/pipeline/sync/test_transparency ... [2021-10-12 08:06:30.256797]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_transparency.py', '-v'] ... [2021-10-12 08:06:30.256881]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 1 item

distributed/pipeline/sync/test_transparency.py::test_simple_linears PASSED [100%]

============================== 1 passed in 0.30s ===============================
Running distributed/pipeline/sync/test_worker ... [2021-10-12 08:06:33.172840]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributed/pipeline/sync/test_worker.py', '-v'] ... [2021-10-12 08:06:33.172926]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
torch: 1.10.0a0+git5af04a0
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 6 items

distributed/pipeline/sync/test_worker.py::test_compute_multithreading PASSED [ 16%]
distributed/pipeline/sync/test_worker.py::test_compute_success PASSED    [ 33%]
distributed/pipeline/sync/test_worker.py::test_compute_exception PASSED  [ 50%]
distributed/pipeline/sync/test_worker.py::test_grad_mode[True] PASSED    [ 66%]
distributed/pipeline/sync/test_worker.py::test_grad_mode[False] PASSED   [ 83%]
distributed/pipeline/sync/test_worker.py::test_worker_per_device PASSED  [100%]

============================== 6 passed in 0.04s ===============================
Running distributed/test_c10d_common ... [2021-10-12 08:06:35.731036]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_c10d_common.py', '-v'] ... [2021-10-12 08:06:35.731117]
test_distributed_debug_mode (__main__.CommTest) ... ok
test_multi_limit_multi_dtype (__main__.ComputeBucketAssignmentTest) ... ok
test_multi_limit_single_dtype (__main__.ComputeBucketAssignmentTest) ... ok
test_single_limit_multi_dtype (__main__.ComputeBucketAssignmentTest) ... ok
test_single_limit_single_dtype (__main__.ComputeBucketAssignmentTest) ... ok
test_invalid_powerSGD_state (__main__.DistributedDataParallelTest) ... ok

----------------------------------------------------------------------
Ran 6 tests in 4.834s

OK
Running distributed/test_c10d_gloo ... [2021-10-12 08:06:42.850960]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_c10d_gloo.py', '-v'] ... [2021-10-12 08:06:42.851050]
test_broadcast_coalesced_gloo_cpu (__main__.CommTest) ... ok
test_broadcast_coalesced_gloo_cuda (__main__.CommTest) ... ok
test_gloo_barrier_device_ids (__main__.CommTest) ... ok
test_sequence_num_incremented_gloo_default (__main__.CommTest) ... ok
test_sequence_num_incremented_gloo_subgroup (__main__.CommTest) ... ok
test_sequence_num_set_default_pg_gloo (__main__.CommTest) ... ok
test_sequence_num_set_gloo_new_group (__main__.CommTest) ... ok
test_ddp_comm_hook_future_passing_cpu (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_future_passing_gpu_gloo (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_register_just_once (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_sparse_gradients (__main__.DistributedDataParallelTest) ... [W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
[W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/jit/python/pybind_utils.cpp:35.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/jit/python/pybind_utils.cpp:35.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
ok
test_ddp_invalid_comm_hook_init (__main__.DistributedDataParallelTest) ... ok
test_ddp_invalid_comm_hook_return_type (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_when_unused_parameters_empty (__main__.DistributedDataParallelTest) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_global_local_unused_params_grad (__main__.DistributedDataParallelTest) ... ok
test_global_local_unused_params_grad_with_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_global_local_unused_params_grad_with_static_graph (__main__.DistributedDataParallelTest) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_gloo_backend_1gpu_module_device_ids_integer_list (__main__.DistributedDataParallelTest) ... ok
test_gloo_backend_1gpu_module_device_ids_torch_device_list (__main__.DistributedDataParallelTest) ... ok
test_gloo_backend_2gpu_module (__main__.DistributedDataParallelTest) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ok
test_gloo_backend_4gpu_module (__main__.DistributedDataParallelTest) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ok
test_gloo_backend_cpu_module (__main__.DistributedDataParallelTest) ... ok
test_gloo_backend_cpu_module_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_ignored_output (__main__.DistributedDataParallelTest) ... ok
test_ignored_output_with_unused_parameters (__main__.DistributedDataParallelTest) ... ok
test_save_load_checkpoint (__main__.DistributedDataParallelTest) ... ok
test_sparse_gradients (__main__.DistributedDataParallelTest) ... ok
test_sparse_gradients_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_allgather_basics (__main__.ProcessGroupGlooTest) ... ok
test_allgather_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_allgather_checks (__main__.ProcessGroupGlooTest) ... ok
test_allgather_coalesced_async (__main__.ProcessGroupGlooTest) ... ok
test_allgather_coalesced_checks (__main__.ProcessGroupGlooTest) ... ok
test_allgather_stress (__main__.ProcessGroupGlooTest) ... ok
test_allgather_stress_cuda (__main__.ProcessGroupGlooTest) ... skipped 'Test skipped for ROCm'
test_allreduce_basics (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_basics_cuda_using_work_api (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_basics_using_work_api (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_checks (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_coalesced_async (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_coalesced_basics (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_coalesced_checks (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_coalesced_checks_cuda (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_coalesced_stress (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_stress (__main__.ProcessGroupGlooTest) ... ok
test_allreduce_stress_cuda (__main__.ProcessGroupGlooTest) ... skipped 'Test skipped for ROCm'
test_barrier_implies_wait (__main__.ProcessGroupGlooTest) ... ok
test_broadcast_basics (__main__.ProcessGroupGlooTest) ... ok
test_broadcast_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_broadcast_checks (__main__.ProcessGroupGlooTest) ... ok
test_broadcast_stress (__main__.ProcessGroupGlooTest) ... ok
test_broadcast_stress_cuda (__main__.ProcessGroupGlooTest) ... ok
test_empty_tensors (__main__.ProcessGroupGlooTest) ... ok
test_gather_basics (__main__.ProcessGroupGlooTest) ... ok
test_gather_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_gather_checks (__main__.ProcessGroupGlooTest) ... ok
test_gather_stress (__main__.ProcessGroupGlooTest) ... ok
test_gather_stress_cuda (__main__.ProcessGroupGlooTest) ... skipped 'Test skipped for ROCm'
test_multi_device_constructor (__main__.ProcessGroupGlooTest) ... ok
test_reduce_basics (__main__.ProcessGroupGlooTest) ... ok
test_reduce_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_reduce_checks (__main__.ProcessGroupGlooTest) ... ok
test_reduce_stress (__main__.ProcessGroupGlooTest) ... ok
test_reduce_stress_cuda (__main__.ProcessGroupGlooTest) ... skipped 'Test skipped for ROCm'
test_round_robin (__main__.ProcessGroupGlooTest) ... ok
test_round_robin_create_destroy (__main__.ProcessGroupGlooTest) ... ok
test_scatter_basics (__main__.ProcessGroupGlooTest) ... ok
test_scatter_basics_cuda (__main__.ProcessGroupGlooTest) ... ok
test_scatter_checks (__main__.ProcessGroupGlooTest) ... ok
test_scatter_stress (__main__.ProcessGroupGlooTest) ... ok
test_scatter_stress_cuda (__main__.ProcessGroupGlooTest) ... skipped 'Test is flaky, see https://github.com/pytorch/pytorch/issues/15963'
test_send_recv_all_to_all (__main__.ProcessGroupGlooTest) ... ok
test_sparse_allreduce_basics (__main__.ProcessGroupGlooTest) ... skipped 'intermittent failures on Windows, in CI'
test_sparse_allreduce_basics_cuda (__main__.ProcessGroupGlooTest) ... [W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
[W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
[W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
[W pybind_utils.h:691] Warning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted. (function operator())
ok
test_sparse_allreduce_checks (__main__.ProcessGroupGlooTest) ... ok
test_forward_backward (__main__.ReducerTest) ... ok
test_forward_backward_optimizer (__main__.ReducerTest) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_forward_backward_unused_parameters (__main__.ReducerTest) ... ok
test_multi_dtype_multi_bucket (__main__.ReducerTest) ... ok
test_multi_dtype_single_bucket (__main__.ReducerTest) ... ok
test_reducer_no_multi_replicas (__main__.ReducerTest) ... ok
test_single_dtype_single_bucket (__main__.ReducerTest) ... ok
test_logging_init (__main__.RendezvousEnvTest) ... ok
test_default_store_timeout_gloo (__main__.TimeoutTest) ... ok

----------------------------------------------------------------------
Ran 85 tests in 297.673s

OK (skipped=6)
Running distributed/test_c10d_nccl ... [2021-10-12 08:11:42.861236]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_c10d_nccl.py', '-v'] ... [2021-10-12 08:11:42.861326]
test_broadcast_coalesced_nccl (__main__.CommTest) ... ok
test_nccl_barrier (__main__.CommTest) ... ok
test_nccl_barrier_device_ids (__main__.CommTest) ... ok
test_nccl_barrier_device_ids_function_argument (__main__.CommTest) ... ok
test_nccl_barrier_timeout (__main__.CommTest) ... ok
test_nccl_barrier_timeout_new_group (__main__.CommTest) ... ok
test_nccl_barrier_timeout_new_group_non_member (__main__.CommTest) ... ok
test_pass_nccl_options_high_priority_stream (__main__.CommTest) ... ok
test_sequence_num_incremented_nccl_default (__main__.CommTest) ... ok
test_sequence_num_incremented_nccl_subgroup (__main__.CommTest) ... ok
test_sequence_num_set_default_pg_nccl (__main__.CommTest) ... ok
test_sequence_num_set_nccl_new_group (__main__.CommTest) ... ok
test_accumulate_gradients_module (__main__.DistributedDataParallelTest) ... ok
test_accumulate_gradients_module_with_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_arbitrary_forward_return_value (__main__.DistributedDataParallelTest) ... ok
test_arbitrary_forward_return_value_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_bf16_compress_wrapper_is_view (__main__.DistributedDataParallelTest) ... skipped 'BFloat16 is only supported by CUDA 11+'
test_bf16_compress_wrapper_nccl (__main__.DistributedDataParallelTest) ... skipped 'BFloat16 is only supported by CUDA 11+'
test_builtin_ddp_comm_hooks_nccl (__main__.DistributedDataParallelTest) ... ok
test_builtin_ddp_comm_hooks_nccl_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_ddp_checkpointing_once (__main__.DistributedDataParallelTest) ... ok
test_ddp_checkpointing_twice (__main__.DistributedDataParallelTest) ... ok
test_ddp_checkpointing_unused_params (__main__.DistributedDataParallelTest) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_checkpointing_weight_sharing (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_allreduce_hook_nccl (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_allreduce_hook_nccl_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_allreduce_hook_nccl_static_graph (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_allreduce_with_then_hook_nccl (__main__.DistributedDataParallelTest) ... ok
test_ddp_comm_hook_future_passing_gpu_nccl (__main__.DistributedDataParallelTest) ... ok
test_ddp_multi_device_module_config (__main__.DistributedDataParallelTest) ... ok
test_ddp_weight_sharing (__main__.DistributedDataParallelTest) ... ok
test_ddp_with_lazy_parameters (__main__.DistributedDataParallelTest) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_default_ddp_comm_hooks_nccl (__main__.DistributedDataParallelTest) ... ok
test_default_ddp_comm_hooks_nccl_is_view (__main__.DistributedDataParallelTest) ... ok
test_failure_recovery (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_debug_detail (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_debug_info (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_debug_off (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_grad_is_view_debug_detail (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_grad_is_view_debug_info (__main__.DistributedDataParallelTest) ... ok
test_find_unused_parameters_kwarg_grad_is_view_debug_off (__main__.DistributedDataParallelTest) ... ok
test_fp16 (__main__.DistributedDataParallelTest) ... ok
test_fp16_compress_wrapper_is_view (__main__.DistributedDataParallelTest) ... ok
test_fp16_compress_wrapper_nccl (__main__.DistributedDataParallelTest) ... ok
test_fp16_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_grad_layout_1devicemodule_1replicaperprocess (__main__.DistributedDataParallelTest) ... skipped 'Test skipped for ROCm'
test_grad_layout_2devicemodule (__main__.DistributedDataParallelTest) ... skipped 'Test skipped for ROCm'
test_hook_then_adam_nccl (__main__.DistributedDataParallelTest) ... ok
test_hook_then_adam_nccl_grad_as_bucket_view (__main__.DistributedDataParallelTest) ... ok
test_hook_then_adamw_nccl (__main__.DistributedDataParallelTest) ... ok
test_hook_then_sgd_nccl (__main__.DistributedDataParallelTest) ... ok
test_hook_then_sgd_nccl_grad_as_bucket_view (__main__.DistributedDataParallelTest) ... ok
test_multiple_outputs_multiple_backward (__main__.DistributedDataParallelTest) ... ok
test_multiple_outputs_multiple_backward_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_nccl_backend_1gpu_module_device_ids_integer_list (__main__.DistributedDataParallelTest) ... ok
test_nccl_backend_1gpu_module_device_ids_torch_device_list (__main__.DistributedDataParallelTest) ... ok
test_nccl_backend_2gpu_module (__main__.DistributedDataParallelTest) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ok
test_nccl_backend_4gpu_module (__main__.DistributedDataParallelTest) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ok
test_nccl_backend_multi_device_ids_not_allowed (__main__.DistributedDataParallelTest) ... ok
test_nccl_backend_multi_device_module_device_ids_None (__main__.DistributedDataParallelTest) ... [W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
[W logger.cpp:305] Warning: Cuda time stats are not collected for multi-device modules. (function operator())
ok
test_nccl_backend_single_device_module_device_ids_None (__main__.DistributedDataParallelTest) ... ok
test_nccl_backend_single_device_module_empty_device_ids (__main__.DistributedDataParallelTest) ... ok
test_nccl_propagate_error_reason (__main__.DistributedDataParallelTest) ... ok
test_no_grad (__main__.DistributedDataParallelTest) ... ok
test_param_layout_mismatch_error (__main__.DistributedDataParallelTest) ... ok
test_pass_default_pg (__main__.DistributedDataParallelTest) ... ok
test_powerSGD_ddp_comm_hook_nccl (__main__.DistributedDataParallelTest) ... ok
test_powerSGD_ddp_comm_hook_nccl_grad_is_view (__main__.DistributedDataParallelTest) ... ok
test_invalid_nccl_blocking_wait_env (__main__.NcclErrorHandlingTest) ... ok
test_nccl_blocking_wait_with_barrier (__main__.NcclErrorHandlingTest) ... ok
test_nccl_errors_blocking_abort (__main__.NcclErrorHandlingTest) ... skipped 'Frequently times out see https://github.com/pytorch/pytorch/issues/58920'
test_nccl_errors_blocking_clean_exit (__main__.NcclErrorHandlingTest) ... skipped 'Test skipped for ROCm'
test_nccl_errors_blocking_nonzero_exit (__main__.NcclErrorHandlingTest) ... ok
test_nccl_errors_blocking_sigkill (__main__.NcclErrorHandlingTest) ... ok
test_nccl_errors_blocking_sigterm (__main__.NcclErrorHandlingTest) ... ok
test_nccl_errors_nonblocking (__main__.NcclErrorHandlingTest) ... skipped 'Test skipped for ROCm'
test_nccl_timeout (__main__.NcclErrorHandlingTest) ... [W ProcessGroupNCCL.cpp:715] [Rank 1] Found key in store: NCCLABORTEDCOMM:20e5abad8509000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, from rank: 0. This means that rank has aborted its NCCL communicators previously and is not in a healthy state.. Aborting appropriate communicators
[W ProcessGroupNCCL.cpp:715] [Rank 2] Found key in store: NCCLABORTEDCOMM:20e5abad8509000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, from rank: 0. This means that rank has aborted its NCCL communicators previously and is not in a healthy state.. Aborting appropriate communicators
ok
test_init_no_gpus (__main__.ProcessGroupNCCLNoGPUTest) ... skipped 'GPUs are available, skipping test'
test_allgather_base_basics (__main__.ProcessGroupNCCLTest) ... ok
test_allgather_base_ops (__main__.ProcessGroupNCCLTest) ... ok
test_allgather_ops (__main__.ProcessGroupNCCLTest) ... ok
test_allreduce_ops (__main__.ProcessGroupNCCLTest) ... ok
test_barrier (__main__.ProcessGroupNCCLTest) ... ok
test_broadcast_ops (__main__.ProcessGroupNCCLTest) ... ok
test_empty_tensors (__main__.ProcessGroupNCCLTest) ... ok
test_reduce_ops (__main__.ProcessGroupNCCLTest) ... ok
test_reduce_scatter_base_basics (__main__.ProcessGroupNCCLTest) ... ok
test_reduce_scatter_base_ops (__main__.ProcessGroupNCCLTest) ... ok
test_reduce_scatter_ops (__main__.ProcessGroupNCCLTest) ... ok
test_common_errors (__main__.RendezvousEnvTest) ... ok
test_default_store_timeout_nccl (__main__.TimeoutTest) ... ok

----------------------------------------------------------------------
Ran 91 tests in 598.276s

OK (skipped=8)
Running distributed/test_c10d_spawn_gloo ... [2021-10-12 08:21:44.470661]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_c10d_spawn_gloo.py', '-v'] ... [2021-10-12 08:21:44.470744]
test_cpu (__main__.DistributedDataParallelSingleProcessTest) ... ok
test_cuda (__main__.DistributedDataParallelSingleProcessTest) ... ok
test_rnn (__main__.DistributedDataParallelSingleProcessTest) ... ok
test_shared_allgather_chunk_gloo (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_allgather_gloo (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_allreduce_gloo (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_broadcast_gloo (__main__.ProcessGroupShareTensorTest) ... ok
test_all_gather (__main__.TestDistributedNNFunctions) ... ok
test_all_to_all (__main__.TestDistributedNNFunctions) ... ok
test_allreduce (__main__.TestDistributedNNFunctions) ... ok
test_broadcast (__main__.TestDistributedNNFunctions) ... ok
test_gather (__main__.TestDistributedNNFunctions) ... ok
test_reduce (__main__.TestDistributedNNFunctions) ... ok
test_scatter (__main__.TestDistributedNNFunctions) ... ok

----------------------------------------------------------------------
Ran 14 tests in 59.949s

OK
Running distributed/test_c10d_spawn_nccl ... [2021-10-12 08:22:49.072765]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_c10d_spawn_nccl.py', '-v'] ... [2021-10-12 08:22:49.072843]
test_shared_allgather_nccl (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_allreduce_nccl (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_broadcast_nccl (__main__.ProcessGroupShareTensorTest) ... ok
test_shared_reduce_nccl (__main__.ProcessGroupShareTensorTest) ... ok

----------------------------------------------------------------------
Ran 4 tests in 18.632s

OK
Running distributed/test_data_parallel ... [2021-10-12 08:23:10.956627]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_data_parallel.py', '-v'] ... [2021-10-12 08:23:10.956704]
test_autocast (__main__.TestDataParallel) ... ok
test_data_parallel (__main__.TestDataParallel) ... ok
test_data_parallel_buffers_requiring_grad (__main__.TestDataParallel) ... ok
test_data_parallel_complex (__main__.TestDataParallel) ... ok
test_data_parallel_device_args (__main__.TestDataParallel) ... ok
test_data_parallel_function_deletion (__main__.TestDataParallel) ... ok
test_data_parallel_lazy_linear (__main__.TestDataParallel) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_data_parallel_model_device (__main__.TestDataParallel)
Test device[0] check at forward time. ... ok
test_data_parallel_model_no_refcycles (__main__.TestDataParallel) ... ok
test_data_parallel_module (__main__.TestDataParallel) ... ok
test_data_parallel_module_kwargs_only (__main__.TestDataParallel) ... ok
test_data_parallel_module_kwargs_only_empty_dict (__main__.TestDataParallel) ... ok
test_data_parallel_module_kwargs_only_empty_list (__main__.TestDataParallel) ... ok
test_data_parallel_module_kwargs_only_empty_tuple (__main__.TestDataParallel) ... ok
test_data_parallel_module_zero_inputs (__main__.TestDataParallel) ... ok
test_data_parallel_multiple_input (__main__.TestDataParallel) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/comm.py:232: UserWarning: Using -1 to represent CPU tensor is deprecated. Please use a device object or string instead, e.g., "cpu".
  'Using -1 to represent CPU tensor is deprecated. Please use a '
ok
test_data_parallel_nested_input (__main__.TestDataParallel) ... ok
test_data_parallel_nested_output (__main__.TestDataParallel) ... ok
test_data_parallel_no_grad (__main__.TestDataParallel) ... ok
test_data_parallel_rnn (__main__.TestDataParallel) ... ok
test_data_parallel_small_back (__main__.TestDataParallel) ... ok
test_data_parallel_sparse (__main__.TestDataParallel) ... ok
test_gather_cpu (__main__.TestDataParallel) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
ok
test_gather_different_len_dicts (__main__.TestDataParallel) ... ok
test_gather_gpu (__main__.TestDataParallel) ... ok
test_parallel_apply (__main__.TestDataParallel) ... ok
test_parallel_apply_autocast (__main__.TestDataParallel) ... ok
test_parallel_apply_passes_exception (__main__.TestDataParallel) ... ok
test_parameter_list_dict_replica (__main__.TestDataParallel) ... ok
test_replicate (__main__.TestDataParallel) ... ok
test_replicate_buffers (__main__.TestDataParallel) ... ok
test_save_replica_module (__main__.TestDataParallel) ... ok
test_scatter_cpu (__main__.TestDataParallel) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/comm.py:232: UserWarning: Using -1 to represent CPU tensor is deprecated. Please use a device object or string instead, e.g., "cpu".
  'Using -1 to represent CPU tensor is deprecated. Please use a '
ok
test_scatter_gpu (__main__.TestDataParallel) ... ok
test_strided_grad_layout (__main__.TestDataParallel) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_zero_grad (__main__.TestDataParallel) ... ok

----------------------------------------------------------------------
Ran 36 tests in 27.332s

OK
/opt/ompi/bin/mpiexec
Running distributed/test_distributed_spawn ... [2021-10-12 08:23:44.472327]
Running distributed tests for the test backend with env init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 08:23:44.486506]

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK
Running distributed tests for the test backend with file init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 08:23:46.975875]

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK
Running distributed tests for the nccl backend with env init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 08:23:49.511560]
test_Backend_enum_class (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_DistributedDataParallelCPU_grad_is_view (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_DistributedDataParallel_SyncBatchNorm (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_2D_Input (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_SyncBatchNorm_Channels_Last (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_No_Affine (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_non_default_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_DistributedDataParallel_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_with_amp_and_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedSampler_padding (__main__.TestDistBackendWithSpawn) ... ok
test_SyncBatchNorm_process_group (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_with_then_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_coalesced_complex (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_group (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_simple (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_with_empty (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all gather skipped for NCCL'
test_all_gather_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all gather skipped for NCCL'
test_all_gather_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_multigpu_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_max_complex_unsupported (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_coalesced_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_complex_unsupported_ops (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all_reduce multigpu skipped for NCCL'
test_all_reduce_multigpu_complex (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all_reduce multigpu skipped for NCCL'
test_all_reduce_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_result_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_async (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_single_equal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_allgather_object (__main__.TestDistBackendWithSpawn) ... ok
test_average_parameters (__main__.TestDistBackendWithSpawn) ... ok
test_backend_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_backend_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_barrier_timeout_global (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_barrier_timeout_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_batch_isend_irecv_gloo (__main__.TestDistBackendWithSpawn) ... skipped 'GLOO Batch Send Recv CPU'
test_batch_isend_irecv_gloo_tags (__main__.TestDistBackendWithSpawn) ... skipped 'GLOO Batch Send Recv CPU'
test_batch_isend_irecv_mixed_backend_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_no_rank_zero_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_op_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_op_list_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_self_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_tensor_err (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL broadcast multigpu skipped'
test_broadcast_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_build_param_to_name_mapping (__main__.TestDistBackendWithSpawn) ... <class 'torch.nn.parameter.Parameter'>
ok
test_ddp_build_param_to_name_mapping_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_comm_hook_logging (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_control_flow_different_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_control_flow_same_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_create_graph (__main__.TestDistBackendWithSpawn) ... skipped 'Gloo-only test'
test_ddp_device (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_get_bucket_sizes (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_grad_div_uneven_inputs (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_hook_parity_allreduce (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_allreduce_process_group (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_post_localSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_powerSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adam (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adamw (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_sgd (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_ignore_params_arg (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_inference (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_join_model_equivalence (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_cpu (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_ddp_logging_data_gpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_multiple_nested_unused_params_err_ignore_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_multiple_nested_unused_params_error (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_namedtuple (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_new_tensor_in_fwd (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_new_tensor_in_fwd_static_graph (__main__.TestDistBackendWithSpawn) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_profiling_autograd_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_profiling_torch_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_python_error_logged (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_returns_tensor_with_no_grad (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_shared_grad_acc_unused_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_static_graph_nested_types (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_bn_training_vs_eval (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_params_and_buffers (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_exception (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_join_disable (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_inputs (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function allreduce_hook at 0x7f2e2038d0d0>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function powerSGD_hook at 0x7f2e2038d840>
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function allreduce_hook at 0x7fcead5d40d0>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function powerSGD_hook at 0x7fcead5d4840>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function allreduce_hook at 0x7fb28bb780d0>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>
registered hook <function powerSGD_hook at 0x7fb28bb78840>

Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
ok
test_ddp_uneven_inputs_stop_iteration_sync_bn (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_unused_params_rebuild_buckets_exception (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_group (__main__.TestDistBackendWithSpawn) ... ok
test_detect_ddp_is_actually_static (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_different_graph_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_dump_DDP_relevant_env_vars (__main__.TestDistBackendWithSpawn) ... ok
test_gather (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_checks (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_object (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_get_backend (__main__.TestDistBackendWithSpawn) ... ok
test_get_future (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_group (__main__.TestDistBackendWithSpawn) ... ok
test_invalid_static_graph (__main__.TestDistBackendWithSpawn) ... ok
test_irecv (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support irecv'
test_isend (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_isend_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_isend_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_monitored_barrier_allreduce_hang (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_allreduce_hang_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_failure_order (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo_rank_0_timeout (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo_subgroup (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_nccl_backend_bool_allgather (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_allreduce (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_broadcast (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_reduce (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_gather_object_err (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_high_priority_stream (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_input_rank_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_negative_input_rank (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_group_size_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_overlap_not_allowed (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_world_size_not_divisible_by_group_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_output_unused_in_loss_dict_module (__main__.TestDistBackendWithSpawn) ... ok
test_output_unused_in_loss_tuple_module (__main__.TestDistBackendWithSpawn) ... ok
test_periodic_model_averager (__main__.TestDistBackendWithSpawn) ... ok
test_post_localSGD_optimizer_parity (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_cuda_twice (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_twice (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_scatter (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_checks (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_scatter_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_object_list (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_send_recv (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl send/recv tested by test_send_recv_nccl'
test_send_recv_any_source (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_any_source_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_any_source_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_sparse_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Only Gloo backend support sparse all reduce'
test_sparse_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Gloo backend support sparse all reduce'
test_static_graph_api_cpu (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_undefined_grad_parity_unused_parameters (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok

----------------------------------------------------------------------
Ran 241 tests in 1012.590s

OK (skipped=135)
Running distributed tests for the nccl backend with file init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 08:40:44.582980]
test_Backend_enum_class (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_DistributedDataParallelCPU_grad_is_view (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_DistributedDataParallel_SyncBatchNorm (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_2D_Input (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_SyncBatchNorm_Channels_Last (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_No_Affine (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_non_default_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_DistributedDataParallel_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_with_amp_and_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedSampler_padding (__main__.TestDistBackendWithSpawn) ... ok
test_SyncBatchNorm_process_group (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_with_then_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_coalesced_complex (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_group (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_simple (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_coalesced_with_empty (__main__.TestDistBackendWithSpawn) ... skipped 'all_gather_coalesced does not support NCCL'
test_all_gather_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all gather skipped for NCCL'
test_all_gather_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all gather skipped for NCCL'
test_all_gather_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_gather_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_multigpu_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_group_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_max (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_max_complex_unsupported (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_coalesced_min (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_product (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_coalesced_sum (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_all_reduce_complex_unsupported_ops (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all_reduce multigpu skipped for NCCL'
test_all_reduce_multigpu_complex (__main__.TestDistBackendWithSpawn) ... skipped 'CUDA all_reduce multigpu skipped for NCCL'
test_all_reduce_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_result_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_async (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_all_to_all_single_equal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_equal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all_single_unequal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_allgather_object (__main__.TestDistBackendWithSpawn) ... ok
test_average_parameters (__main__.TestDistBackendWithSpawn) ... ok
test_backend_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_backend_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL does not support CPU barrier'
test_barrier_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_barrier_timeout_global (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_barrier_timeout_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only gloo backend supports timeouts'
test_batch_isend_irecv_gloo (__main__.TestDistBackendWithSpawn) ... skipped 'GLOO Batch Send Recv CPU'
test_batch_isend_irecv_gloo_tags (__main__.TestDistBackendWithSpawn) ... skipped 'GLOO Batch Send Recv CPU'
test_batch_isend_irecv_mixed_backend_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_no_rank_zero_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_op_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_op_list_err (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_self_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_tensor_err (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_broadcast_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL broadcast multigpu skipped'
test_broadcast_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_build_param_to_name_mapping (__main__.TestDistBackendWithSpawn) ... <class 'torch.nn.parameter.Parameter'>
ok
test_ddp_build_param_to_name_mapping_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_comm_hook_logging (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_control_flow_different_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_control_flow_same_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_create_graph (__main__.TestDistBackendWithSpawn) ... skipped 'Gloo-only test'
test_ddp_device (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_get_bucket_sizes (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_grad_div_uneven_inputs (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_hook_parity_allreduce (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_allreduce_process_group (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_post_localSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_powerSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adam (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adamw (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_sgd (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_ignore_params_arg (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_inference (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_join_model_equivalence (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_cpu (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_ddp_logging_data_gpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_multiple_nested_unused_params_err_ignore_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_multiple_nested_unused_params_error (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_namedtuple (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_new_tensor_in_fwd (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_new_tensor_in_fwd_static_graph (__main__.TestDistBackendWithSpawn) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_profiling_autograd_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_profiling_torch_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_python_error_logged (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_returns_tensor_with_no_grad (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_shared_grad_acc_unused_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_static_graph_nested_types (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_bn_training_vs_eval (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_params_and_buffers (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_exception (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_join_disable (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_inputs (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function allreduce_hook at 0x7f5ac0ce80d0>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function powerSGD_hook at 0x7f5ac0ce8840>
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function allreduce_hook at 0x7f91488670d0>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>
registered hook <function powerSGD_hook at 0x7f9148867840>

Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function allreduce_hook at 0x7f26013e50d0>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
registered hook <function powerSGD_hook at 0x7f26013e5840>
ok
test_ddp_uneven_inputs_stop_iteration_sync_bn (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_unused_params_rebuild_buckets_exception (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_group (__main__.TestDistBackendWithSpawn) ... ok
test_detect_ddp_is_actually_static (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_different_graph_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_dump_DDP_relevant_env_vars (__main__.TestDistBackendWithSpawn) ... ok
test_gather (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_checks (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_gather_object (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_get_backend (__main__.TestDistBackendWithSpawn) ... ok
test_get_future (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_group (__main__.TestDistBackendWithSpawn) ... ok
test_invalid_static_graph (__main__.TestDistBackendWithSpawn) ... ok
test_irecv (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support irecv'
test_isend (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_isend_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_isend_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support isend'
test_monitored_barrier_allreduce_hang (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_allreduce_hang_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_failure_order (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo_rank_0_timeout (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_gloo_subgroup (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_monitored_barrier_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_nccl_backend_bool_allgather (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_allreduce (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_broadcast (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_backend_bool_reduce (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_gather_object_err (__main__.TestDistBackendWithSpawn) ... ok
test_nccl_high_priority_stream (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_input_rank_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_negative_input_rank (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_group_size_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_overlap_not_allowed (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_world_size_not_divisible_by_group_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_output_unused_in_loss_dict_module (__main__.TestDistBackendWithSpawn) ... ok
test_output_unused_in_loss_tuple_module (__main__.TestDistBackendWithSpawn) ... ok
test_periodic_model_averager (__main__.TestDistBackendWithSpawn) ... ok
test_post_localSGD_optimizer_parity (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_max (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_min (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_product (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_cuda_twice (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_twice (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_scatter (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_checks (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support CPU tensors'
test_scatter_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_group (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support scatter'
test_scatter_object_list (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'gloo'}"
test_send_recv (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl send/recv tested by test_send_recv_nccl'
test_send_recv_any_source (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_any_source_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_any_source_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'Nccl does not support send/recv from any source'
test_send_recv_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_nccl (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_send_recv_with_tag_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL send/recv tested by test_send_recv_nccl'
test_sparse_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... skipped 'Only Gloo backend support sparse all reduce'
test_sparse_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Gloo backend support sparse all reduce'
test_static_graph_api_cpu (__main__.TestDistBackendWithSpawn) ... skipped 'nccl does not support DDP on CPU models'
test_undefined_grad_parity_unused_parameters (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok

----------------------------------------------------------------------
Ran 241 tests in 1023.098s

OK (skipped=135)
Running distributed tests for the gloo backend with env init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 08:57:50.146671]
test_Backend_enum_class (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_2D_Input (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_SyncBatchNorm_Channels_Last (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_No_Affine (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_non_default_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_DistributedDataParallel_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_with_amp_and_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedSampler_padding (__main__.TestDistBackendWithSpawn) ... ok
test_SyncBatchNorm_process_group (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_with_then_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_simple (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_with_empty (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all gather'
test_all_gather_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all gather'
test_all_gather_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports allgather multigpu'
test_all_gather_multigpu_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports allgather multigpu'
test_all_reduce_coalesced_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_max_complex_unsupported (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_complex_unsupported_ops (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_multigpu_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_result_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_allgather_object (__main__.TestDistBackendWithSpawn) ... ok
test_average_parameters (__main__.TestDistBackendWithSpawn) ... ok
test_backend_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_backend_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_global (__main__.TestDistBackendWithSpawn) ... skipped 'Requires file:// initialization method. Both tcp:// and env:// rely on the TCP store for which reinitialization has proven racy.'
test_barrier_timeout_group (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_gloo (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_gloo_tags (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_mixed_backend_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_no_rank_zero_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_op_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_op_list_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_self_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_tensor_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_broadcast (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_group (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_build_param_to_name_mapping (__main__.TestDistBackendWithSpawn) ... <class 'torch.nn.parameter.Parameter'>
ok
test_ddp_build_param_to_name_mapping_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_comm_hook_logging (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_control_flow_different_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_control_flow_same_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_create_graph (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
ok
test_ddp_device (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_get_bucket_sizes (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_grad_div_uneven_inputs (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_hook_parity_allreduce (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_allreduce_process_group (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_post_localSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_powerSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adam (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adamw (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_sgd (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_ignore_params_arg (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_inference (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_join_model_equivalence (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_cpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_gpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_multiple_nested_unused_params_err_ignore_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_multiple_nested_unused_params_error (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_namedtuple (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_new_tensor_in_fwd (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_new_tensor_in_fwd_static_graph (__main__.TestDistBackendWithSpawn) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_profiling_autograd_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_profiling_torch_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_python_error_logged (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_returns_tensor_with_no_grad (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_shared_grad_acc_unused_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_static_graph_nested_types (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_bn_training_vs_eval (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_params_and_buffers (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_exception (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_join_disable (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_inputs (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function allreduce_hook at 0x7fc24a0df0d0>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function powerSGD_hook at 0x7fc24a0df840>
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function allreduce_hook at 0x7f9db222c0d0>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function powerSGD_hook at 0x7f9db222c840>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function allreduce_hook at 0x7fabd82050d0>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>
registered hook <function powerSGD_hook at 0x7fabd8205840>

Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
ok
test_ddp_uneven_inputs_stop_iteration_sync_bn (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_unused_params_rebuild_buckets_exception (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_group (__main__.TestDistBackendWithSpawn) ... ok
test_detect_ddp_is_actually_static (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_different_graph_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_dump_DDP_relevant_env_vars (__main__.TestDistBackendWithSpawn) ... ok
test_gather (__main__.TestDistBackendWithSpawn) ... ok
test_gather_checks (__main__.TestDistBackendWithSpawn) ... ok
test_gather_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_gather_group (__main__.TestDistBackendWithSpawn) ... ok
test_gather_object (__main__.TestDistBackendWithSpawn) ... ok
test_get_backend (__main__.TestDistBackendWithSpawn) ... ok
test_get_future (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_group (__main__.TestDistBackendWithSpawn) ... ok
test_invalid_static_graph (__main__.TestDistBackendWithSpawn) ... ok
test_irecv (__main__.TestDistBackendWithSpawn) ... ok
test_isend (__main__.TestDistBackendWithSpawn) ... ok
test_isend_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_isend_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_monitored_barrier_allreduce_hang (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_allreduce_hang_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_failure_order (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 1 successfully reached monitoredBarrier, but received errors while waiting to be unblocked by rank 0. Please check rank 0 logs for faulty rank.
[E ProcessGroupGloo.cpp:136] Rank 2 failed to pass monitoredBarrier in 2000 ms
ok
test_monitored_barrier_gloo (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 2000 ms
[E ProcessGroupGloo.cpp:136] Rank 2 successfully reached monitoredBarrier, but received errors while waiting to be unblocked by rank 0. Please check rank 0 logs for faulty rank.
ok
test_monitored_barrier_gloo_rank_0_timeout (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 0 timed out in monitoredBarrier after 0 ms.
No ranks successfully processed in monitoredBarrier.
[E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 0 ms
ok
test_monitored_barrier_gloo_subgroup (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 100 ms
ok
test_monitored_barrier_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:2781] Rank 1 failed to pass monitoredBarrier in 100 ms
[E ProcessGroupGloo.cpp:2781] Rank 2 failed to pass monitoredBarrier in 100 ms
[E ProcessGroupGloo.cpp:136] Ranks 1, 2 failed to pass monitoredBarrier in 100 ms
ok
test_nccl_backend_bool_allgather (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_allreduce (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_broadcast (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_reduce (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_gather_object_err (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_high_priority_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL backend supports high priority stream'
test_new_subgroups (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_input_rank_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_negative_input_rank (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_group_size_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_overlap_not_allowed (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_world_size_not_divisible_by_group_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_output_unused_in_loss_dict_module (__main__.TestDistBackendWithSpawn) ... ok
test_output_unused_in_loss_tuple_module (__main__.TestDistBackendWithSpawn) ... ok
test_periodic_model_averager (__main__.TestDistBackendWithSpawn) ... ok
test_post_localSGD_optimizer_parity (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports reduce multigpu'
test_reduce_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA reduce'
test_reduce_sum_cuda_twice (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA reduce'
test_reduce_sum_twice (__main__.TestDistBackendWithSpawn) ... ok
test_scatter (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_checks (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_complex (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_group (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_nccl_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_nccl_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_sparse_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_sparse_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_static_graph_api_cpu (__main__.TestDistBackendWithSpawn) ... ok
test_undefined_grad_parity_unused_parameters (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok

----------------------------------------------------------------------
Ran 241 tests in 1333.250s

OK (skipped=65)
Running distributed tests for the gloo backend with file init_method
Executing ['/opt/conda/bin/python3.6', 'distributed/test_distributed_spawn.py', '-v'] ... [2021-10-12 09:20:05.992540]
test_Backend_enum_class (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallelCPU_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_2D_Input (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_SyncBatchNorm_Channels_Last (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_No_Affine (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process (__main__.TestDistBackendWithSpawn) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_DistributedDataParallel_non_default_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_DistributedDataParallel_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedDataParallel_with_amp_and_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_DistributedSampler_padding (__main__.TestDistBackendWithSpawn) ... ok
test_SyncBatchNorm_process_group (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_allreduce_with_then_hook (__main__.TestDistBackendWithSpawn) ... ok
test_accumulate_gradients_no_sync_grad_is_view (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_simple (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_coalesced_with_empty (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all gather'
test_all_gather_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all gather'
test_all_gather_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_group (__main__.TestDistBackendWithSpawn) ... ok
test_all_gather_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports allgather multigpu'
test_all_gather_multigpu_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports allgather multigpu'
test_all_reduce_coalesced_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_max_complex_unsupported (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_coalesced_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_complex_unsupported_ops (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_max (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_min (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_multigpu_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_product (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_result_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_async (__main__.TestDistBackendWithSpawn) ... ok
test_all_reduce_sum_cuda_complex (__main__.TestDistBackendWithSpawn) ... ok
test_all_to_all (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL supports CUDA all_to_all'
test_all_to_all_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports all_to_all'
test_all_to_all_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_equal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_equal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_cuda_complex (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_full_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_full_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_all_to_all_single_unequal_split_group (__main__.TestDistBackendWithSpawn) ... skipped 'Only MPI supports CPU all_to_all_single'
test_all_to_all_single_unequal_split_group_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA all_to_all_single'
test_allgather_object (__main__.TestDistBackendWithSpawn) ... ok
test_average_parameters (__main__.TestDistBackendWithSpawn) ... ok
test_backend_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_backend_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_full_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_group_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_global (__main__.TestDistBackendWithSpawn) ... ok
test_barrier_timeout_group (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_gloo (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_gloo_tags (__main__.TestDistBackendWithSpawn) ... ok
test_batch_isend_irecv_mixed_backend_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_no_rank_zero_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_op_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_op_list_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_self_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_batch_isend_irecv_tensor_err (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Batch Send Recv Only'
test_broadcast (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_group (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_multigpu (__main__.TestDistBackendWithSpawn) ... ok
test_broadcast_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_build_param_to_name_mapping (__main__.TestDistBackendWithSpawn) ... <class 'torch.nn.parameter.Parameter'>
ok
test_ddp_build_param_to_name_mapping_requires_grad (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_comm_hook_logging (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_control_flow_different_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_control_flow_same_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_create_graph (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
[W reducer.cpp:389] Using DistributedDataParallel with create_graph=True  is not well-supported. The higher-order gradient will  not be synchronized across ranks, and backpropagation  through all_reduce operations will not occur. If you require  DDP to work with higher-order gradients for your use case,  please ping https://github.com/pytorch/pytorch/issues/63929
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
ok
test_ddp_device (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_get_bucket_sizes (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_grad_div_uneven_inputs (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_hook_parity_allreduce (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_allreduce_process_group (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_post_localSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_parity_powerSGD (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adam (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_adamw (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_hook_with_optimizer_parity_sgd (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_ignore_params_arg (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_inference (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_join_model_equivalence (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_cpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_logging_data_gpu (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_ddp_multiple_nested_unused_params_err_ignore_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_multiple_nested_unused_params_error (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_namedtuple (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_new_tensor_in_fwd (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_new_tensor_in_fwd_static_graph (__main__.TestDistBackendWithSpawn) ... /opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_profiling_autograd_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_profiling_torch_profiler (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_ddp_python_error_logged (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_returns_tensor_with_no_grad (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:1516: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  "You passed find_unused_parameters=true to DistributedDataParallel, "
ok
test_ddp_shared_grad_acc_unused_params (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_static_graph_nested_types (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_bn_training_vs_eval (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_sync_params_and_buffers (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_exception (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_input_join_disable (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_uneven_inputs (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function allreduce_hook at 0x7f42722410d0>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
registered hook <function powerSGD_hook at 0x7f4272241840>
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        2 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        3 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        4 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        5 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        6 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        7 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        8 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        9 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: batch_norm_net sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: large_conv_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: unjoined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: joined_rank_with_unused_params_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: small_model_allreduce_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function allreduce_hook at 0x7fdeedef60d0>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function powerSGD_hook at 0x7fdeedef6840>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function allreduce_hook at 0x7fe32e8650d0>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>
registered hook <function powerSGD_hook at 0x7fe32e865840>

Running test: small_model_power_sgd_hook sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 2, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 3, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 10, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 7, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 8, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 15, 2: 15}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 2}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 1, 2: 3}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 0, 1: 5, 2: 10}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 7}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 6, 2: 8}
Running test: resnet_model sync interval
                        1 with iteration mapping
                        {0: 5, 1: 10, 2: 15}
ok
test_ddp_uneven_inputs_stop_iteration_sync_bn (__main__.TestDistBackendWithSpawn) ... ok
test_ddp_unused_params_rebuild_buckets_exception (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_destroy_group (__main__.TestDistBackendWithSpawn) ... ok
test_detect_ddp_is_actually_static (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_different_graph_across_ranks (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok
test_dump_DDP_relevant_env_vars (__main__.TestDistBackendWithSpawn) ... ok
test_gather (__main__.TestDistBackendWithSpawn) ... ok
test_gather_checks (__main__.TestDistBackendWithSpawn) ... ok
test_gather_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_gather_group (__main__.TestDistBackendWithSpawn) ... ok
test_gather_object (__main__.TestDistBackendWithSpawn) ... ok
test_get_backend (__main__.TestDistBackendWithSpawn) ... ok
test_get_future (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_get_rank_size_group (__main__.TestDistBackendWithSpawn) ... ok
test_invalid_static_graph (__main__.TestDistBackendWithSpawn) ... ok
test_irecv (__main__.TestDistBackendWithSpawn) ... ok
test_isend (__main__.TestDistBackendWithSpawn) ... ok
test_isend_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_isend_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_monitored_barrier_allreduce_hang (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_allreduce_hang_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... skipped 'Test skipped for ROCm'
test_monitored_barrier_failure_order (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 1 successfully reached monitoredBarrier, but received errors while waiting to be unblocked by rank 0. Please check rank 0 logs for faulty rank.
[E ProcessGroupGloo.cpp:136] Rank 2 failed to pass monitoredBarrier in 2000 ms
ok
test_monitored_barrier_gloo (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 2 successfully reached monitoredBarrier, but received errors while waiting to be unblocked by rank 0. Please check rank 0 logs for faulty rank.
[E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 2000 ms
ok
test_monitored_barrier_gloo_rank_0_timeout (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 0 timed out in monitoredBarrier after 0 ms.
No ranks successfully processed in monitoredBarrier.
[E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 0 ms
ok
test_monitored_barrier_gloo_subgroup (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:136] Rank 1 failed to pass monitoredBarrier in 100 ms
ok
test_monitored_barrier_wait_all_ranks (__main__.TestDistBackendWithSpawn) ... [E ProcessGroupGloo.cpp:2781] Rank 1 failed to pass monitoredBarrier in 100 ms
[E ProcessGroupGloo.cpp:2781] Rank 2 failed to pass monitoredBarrier in 100 ms
[E ProcessGroupGloo.cpp:136] Ranks 1, 2 failed to pass monitoredBarrier in 100 ms
ok
test_nccl_backend_bool_allgather (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_allreduce (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_broadcast (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_backend_bool_reduce (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_gather_object_err (__main__.TestDistBackendWithSpawn) ... skipped "Test requires backend to be one of {'nccl'}"
test_nccl_high_priority_stream (__main__.TestDistBackendWithSpawn) ... skipped 'Only NCCL backend supports high priority stream'
test_new_subgroups (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_input_rank_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_by_enumeration_negative_input_rank (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_group_size_exceeds_world_size (__main__.TestDistBackendWithSpawn) ... ok
test_new_subgroups_overlap_not_allowed (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_new_subgroups_world_size_not_divisible_by_group_size (__main__.TestDistBackendWithSpawn) ... skipped 'Test requires world size of 4'
test_output_unused_in_loss_dict_module (__main__.TestDistBackendWithSpawn) ... ok
test_output_unused_in_loss_tuple_module (__main__.TestDistBackendWithSpawn) ... ok
test_periodic_model_averager (__main__.TestDistBackendWithSpawn) ... ok
test_post_localSGD_optimizer_parity (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_full_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_group_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_max (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_min (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_multigpu (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl backend supports reduce multigpu'
test_reduce_product (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA reduce'
test_reduce_sum_cuda_twice (__main__.TestDistBackendWithSpawn) ... skipped 'Only Nccl supports CUDA reduce'
test_reduce_sum_twice (__main__.TestDistBackendWithSpawn) ... ok
test_scatter (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_checks (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_complex (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_full_group (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_group (__main__.TestDistBackendWithSpawn) ... ok
test_scatter_object_list (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_any_source_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_nccl (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_nccl_autograd_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_nccl_torch_profiler (__main__.TestDistBackendWithSpawn) ... skipped 'NCCL Send Recv Only'
test_send_recv_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag_autograd_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_send_recv_with_tag_torch_profiler (__main__.TestDistBackendWithSpawn) ... ok
test_sparse_all_reduce_sum (__main__.TestDistBackendWithSpawn) ... ok
test_sparse_all_reduce_sum_cuda (__main__.TestDistBackendWithSpawn) ... ok
test_static_graph_api_cpu (__main__.TestDistBackendWithSpawn) ... ok
test_undefined_grad_parity_unused_parameters (__main__.TestDistBackendWithSpawn) ... [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
ok

----------------------------------------------------------------------
Ran 241 tests in 1302.156s

OK (skipped=64)
Running distributed/test_jit_c10d ... [2021-10-12 09:41:50.623923]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_jit_c10d.py', '-v'] ... [2021-10-12 09:41:50.624000]
test_frontend_singleton (__main__.C10dFrontendJitTest) ... ok
test_process_group_as_module_member (__main__.C10dProcessGroupSerialization) ... ok
test_init_process_group_nccl_as_base_process_group_torchbind (__main__.ProcessGroupNCCLJitTest) ... ok
test_init_process_group_nccl_torchbind (__main__.ProcessGroupNCCLJitTest) ... ok
test_process_group_nccl_as_base_process_group_torchbind_alltoall (__main__.ProcessGroupNCCLJitTest) ... ok
test_process_group_nccl_serialization (__main__.ProcessGroupNCCLJitTest) ... ok
test_process_group_nccl_torchbind_alltoall (__main__.ProcessGroupNCCLJitTest) ... ok
test_create_file_store (__main__.StoreTest) ... ok
test_create_prefix_store (__main__.StoreTest) ... ok

----------------------------------------------------------------------
Ran 9 tests in 0.781s

OK
Running distributed/test_launcher ... [2021-10-12 09:41:54.725019]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_launcher.py', '-v'] ... [2021-10-12 09:41:54.725098]
test_launch_user_script (__main__.TestDistributedLaunch) ... /opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Success, smoke test
Success, smoke test
Success, smoke test
Success, smoke test
ok

----------------------------------------------------------------------
Ran 1 test in 1.132s

OK
Running distributed/test_nccl ... [2021-10-12 09:41:58.371268]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_nccl.py', '-v'] ... [2021-10-12 09:41:58.371351]
test_all_gather_cuda_bfloat16 (__main__.TestNCCLCUDA) ... ok
test_all_gather_cuda_float32 (__main__.TestNCCLCUDA) ... ok
test_all_reduce_cuda_bfloat16 (__main__.TestNCCLCUDA) ... ok
test_all_reduce_cuda_float32 (__main__.TestNCCLCUDA) ... ok
test_broadcast_cuda_bfloat16 (__main__.TestNCCLCUDA) ... ok
test_broadcast_cuda_float32 (__main__.TestNCCLCUDA) ... ok
test_collective_errors_cuda (__main__.TestNCCLCUDA) ... ok
test_reduce_cuda_bfloat16 (__main__.TestNCCLCUDA) ... ok
test_reduce_cuda_float32 (__main__.TestNCCLCUDA) ... ok
test_reduce_scatter_cuda_bfloat16 (__main__.TestNCCLCUDA) ... ok
test_reduce_scatter_cuda_float32 (__main__.TestNCCLCUDA) ... ok
test_unique_id_cuda (__main__.TestNCCLCUDA) ... ok

----------------------------------------------------------------------
Ran 12 tests in 7.491s

OK
Running distributed/test_pg_wrapper ... [2021-10-12 09:42:09.383155]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_pg_wrapper.py', '-v'] ... [2021-10-12 09:42:09.383252]

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK
Running distributed/test_store ... [2021-10-12 09:42:11.676506]
Executing ['/opt/conda/bin/python3.6', 'distributed/test_store.py', '-v'] ... [2021-10-12 09:42:11.676590]
test_compare_set (__main__.FileStoreTest) ... ok
test_set_get (__main__.FileStoreTest) ... ok
test_compare_set (__main__.PrefixFileStoreTest) ... ok
test_set_get (__main__.PrefixFileStoreTest) ... ok
test_compare_set (__main__.PrefixTCPStoreTest) ... ok
test_set_get (__main__.PrefixTCPStoreTest) ... ok
test_set_get (__main__.PythonStoreTest) ... ok
test_nominal (__main__.RendezvousEnvTest) ... ok
test_common_errors (__main__.RendezvousFileTest) ... ok
test_nominal (__main__.RendezvousFileTest) ... ok
test_unknown_handler (__main__.RendezvousTest) ... ok
test_address_already_in_use (__main__.TCPStoreTest) ... ok
test_compare_set (__main__.TCPStoreTest) ... ok
test_init_pg_and_rpc_with_same_socket (__main__.TCPStoreTest) ... ok
test_multi_worker_with_fixed_world_size (__main__.TCPStoreTest) ... ok
test_multi_worker_with_nonfixed_world_size (__main__.TCPStoreTest) ... ok
test_multitenancy (__main__.TCPStoreTest) ... ok
test_numkeys_delkeys (__main__.TCPStoreTest) ... ok
test_set_get (__main__.TCPStoreTest) ... ok

----------------------------------------------------------------------
Ran 19 tests in 2.771s

OK
Running distributions/test_constraints ... [2021-10-12 09:42:16.888915]
Executing ['/opt/conda/bin/python3.6', '-m', 'pytest', 'distributions/test_constraints.py', '-v'] ... [2021-10-12 09:42:16.888998]
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /opt/conda/bin/python3.6
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/var/lib/jenkins/pytorch/test/.hypothesis/examples')
rootdir: /var/lib/jenkins/pytorch, configfile: pytest.ini
plugins: hypothesis-4.53.2
collecting ... collected 96 items

distributions/test_constraints.py::test_biject_to[False-constraint_fn0-args0] PASSED [  1%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn1-args1] PASSED [  2%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn2-args2] PASSED [  3%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThan-args3] PASSED [  4%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThan-args4] PASSED [  5%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThan-args5] PASSED [  6%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThan-args6] PASSED [  7%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThanEq-args7] PASSED [  8%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThanEq-args8] PASSED [  9%]
distributions/test_constraints.py::test_biject_to[False-_GreaterThanEq-args9] PASSED [ 10%]
distributions/test_constraints.py::test_biject_to[False-_LessThan-args10] PASSED [ 11%]
distributions/test_constraints.py::test_biject_to[False-_LessThan-args11] PASSED [ 12%]
distributions/test_constraints.py::test_biject_to[False-_LessThan-args12] PASSED [ 13%]
distributions/test_constraints.py::test_biject_to[False-_LessThan-args13] PASSED [ 14%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn14-args14] PASSED [ 15%]
distributions/test_constraints.py::test_biject_to[False-_Interval-args15] PASSED [ 16%]
distributions/test_constraints.py::test_biject_to[False-_Interval-args16] PASSED [ 17%]
distributions/test_constraints.py::test_biject_to[False-_Interval-args17] PASSED [ 18%]
distributions/test_constraints.py::test_biject_to[False-_HalfOpenInterval-args18] PASSED [ 19%]
distributions/test_constraints.py::test_biject_to[False-_HalfOpenInterval-args19] PASSED [ 20%]
distributions/test_constraints.py::test_biject_to[False-_HalfOpenInterval-args20] PASSED [ 21%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn21-args21] PASSED [ 22%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn22-args22] PASSED [ 23%]
distributions/test_constraints.py::test_biject_to[False-constraint_fn23-args23] SKIPPED [ 25%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn0-args0] PASSED [ 26%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn1-args1] PASSED [ 27%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn2-args2] PASSED [ 28%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThan-args3] PASSED [ 29%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThan-args4] PASSED [ 30%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThan-args5] PASSED [ 31%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThan-args6] PASSED [ 32%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThanEq-args7] PASSED [ 33%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThanEq-args8] PASSED [ 34%]
distributions/test_constraints.py::test_biject_to[True-_GreaterThanEq-args9] PASSED [ 35%]
distributions/test_constraints.py::test_biject_to[True-_LessThan-args10] PASSED [ 36%]
distributions/test_constraints.py::test_biject_to[True-_LessThan-args11] PASSED [ 37%]
distributions/test_constraints.py::test_biject_to[True-_LessThan-args12] PASSED [ 38%]
distributions/test_constraints.py::test_biject_to[True-_LessThan-args13] PASSED [ 39%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn14-args14] PASSED [ 40%]
distributions/test_constraints.py::test_biject_to[True-_Interval-args15] PASSED [ 41%]
distributions/test_constraints.py::test_biject_to[True-_Interval-args16] PASSED [ 42%]
distributions/test_constraints.py::test_biject_to[True-_Interval-args17] PASSED [ 43%]
distributions/test_constraints.py::test_biject_to[True-_HalfOpenInterval-args18] PASSED [ 44%]
distributions/test_constraints.py::test_biject_to[True-_HalfOpenInterval-args19] PASSED [ 45%]
distributions/test_constraints.py::test_biject_to[True-_HalfOpenInterval-args20] PASSED [ 46%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn21-args21] PASSED [ 47%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn22-args22] PASSED [ 48%]
distributions/test_constraints.py::test_biject_to[True-constraint_fn23-args23] SKIPPED [ 50%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn0-args0] PASSED [ 51%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn1-args1] PASSED [ 52%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn2-args2] PASSED [ 53%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThan-args3] PASSED [ 54%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThan-args4] PASSED [ 55%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThan-args5] PASSED [ 56%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThan-args6] PASSED [ 57%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThanEq-args7] PASSED [ 58%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThanEq-args8] PASSED [ 59%]
distributions/test_constraints.py::test_transform_to[False-_GreaterThanEq-args9] PASSED [ 60%]
distributions/test_constraints.py::test_transform_to[False-_LessThan-args10] PASSED [ 61%]
distributions/test_constraints.py::test_transform_to[False-_LessThan-args11] PASSED [ 62%]
distributions/test_constraints.py::test_transform_to[False-_LessThan-args12] PASSED [ 63%]
distributions/test_constraints.py::test_transform_to[False-_LessThan-args13] PASSED [ 64%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn14-args14] PASSED [ 65%]
distributions/test_constraints.py::test_transform_to[False-_Interval-args15] PASSED [ 66%]
distributions/test_constraints.py::test_transform_to[False-_Interval-args16] PASSED [ 67%]
distributions/test_constraints.py::test_transform_to[False-_Interval-args17] PASSED [ 68%]
distributions/test_constraints.py::test_transform_to[False-_HalfOpenInterval-args18] PASSED [ 69%]
distributions/test_constraints.py::test_transform_to[False-_HalfOpenInterval-args19] PASSED [ 70%]
distributions/test_constraints.py::test_transform_to[False-_HalfOpenInterval-args20] PASSED [ 71%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn21-args21] PASSED [ 72%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn22-args22] PASSED [ 73%]
distributions/test_constraints.py::test_transform_to[False-constraint_fn23-args23] PASSED [ 75%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn0-args0] PASSED [ 76%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn1-args1] PASSED [ 77%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn2-args2] PASSED [ 78%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThan-args3] PASSED [ 79%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThan-args4] PASSED [ 80%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThan-args5] PASSED [ 81%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThan-args6] PASSED [ 82%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThanEq-args7] PASSED [ 83%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThanEq-args8] PASSED [ 84%]
distributions/test_constraints.py::test_transform_to[True-_GreaterThanEq-args9] PASSED [ 85%]
distributions/test_constraints.py::test_transform_to[True-_LessThan-args10] PASSED [ 86%]
distributions/test_constraints.py::test_transform_to[True-_LessThan-args11] PASSED [ 87%]
distributions/test_constraints.py::test_transform_to[True-_LessThan-args12] PASSED [ 88%]
distributions/test_constraints.py::test_transform_to[True-_LessThan-args13] PASSED [ 89%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn14-args14] PASSED [ 90%]
distributions/test_constraints.py::test_transform_to[True-_Interval-args15] PASSED [ 91%]
distributions/test_constraints.py::test_transform_to[True-_Interval-args16] PASSED [ 92%]
distributions/test_constraints.py::test_transform_to[True-_Interval-args17] PASSED [ 93%]
distributions/test_constraints.py::test_transform_to[True-_HalfOpenInterval-args18] PASSED [ 94%]
distributions/test_constraints.py::test_transform_to[True-_HalfOpenInterval-args19] PASSED [ 95%]
distributions/test_constraints.py::test_transform_to[True-_HalfOpenInterval-args20] PASSED [ 96%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn21-args21] PASSED [ 97%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn22-args22] PASSED [ 98%]
distributions/test_constraints.py::test_transform_to[True-constraint_fn23-args23] PASSED [100%]

=========================== short test summary info ============================
SKIPPED [2] distributions/test_constraints.py:52: `biject_to` not implemented.
======================== 94 passed, 2 skipped in 2.49s =========================
Running distributions/test_distributions ... [2021-10-12 09:42:21.300781]
Executing ['/opt/conda/bin/python3.6', 'distributions/test_distributions.py', '-v'] ... [2021-10-12 09:42:21.300860]
test_cdf (__main__.TestAgainstScipy) ... ok
test_icdf (__main__.TestAgainstScipy) ... ok
test_mean (__main__.TestAgainstScipy) ... ok
test_variance_stddev (__main__.TestAgainstScipy) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:1603: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/utils/tensor_numpy.cpp:187.)
  return torch.from_numpy(a)
ok
test_params_constraints (__main__.TestConstraints) ... ok
test_support_constraints (__main__.TestConstraints) ... ok
test_bernoulli_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_bernoulli_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_beta_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_beta_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_binomial_shape (__main__.TestDistributionShapes) ... ok
test_binomial_shape_vectorized_n (__main__.TestDistributionShapes) ... ok
test_categorical_shape (__main__.TestDistributionShapes) ... ok
test_cauchy_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_cauchy_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_chi2_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_chi2_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_continuous_bernoulli_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_continuous_bernoulli_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_dirichlet_shape (__main__.TestDistributionShapes) ... ok
test_entropy_shape (__main__.TestDistributionShapes) ... ok
test_exponential_shape_scalar_param (__main__.TestDistributionShapes) ... ok
test_exponential_shape_tensor_param (__main__.TestDistributionShapes) ... ok
test_gamma_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_gamma_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_geometric_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_geometric_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_gumbel_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_halfcauchy_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_halfcauchy_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_kumaraswamy_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_laplace_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_laplace_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_mixture_same_family_shape (__main__.TestDistributionShapes) ... ok
test_multinomial_shape (__main__.TestDistributionShapes) ... ok
test_normal_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_normal_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_one_hot_categorical_shape (__main__.TestDistributionShapes) ... ok
test_pareto_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_studentT_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_studentT_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_uniform_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_uniform_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_vonmises_shape_scalar_params (__main__.TestDistributionShapes) ... ok
test_vonmises_shape_tensor_params (__main__.TestDistributionShapes) ... ok
test_weibull_scale_scalar_params (__main__.TestDistributionShapes) ... ok
test_argmax_relaxed_categorical (__main__.TestDistributions) ... ok
test_bernoulli (__main__.TestDistributions) ... ok
test_bernoulli_3d (__main__.TestDistributions) ... ok
test_bernoulli_enumerate_support (__main__.TestDistributions) ... ok
test_beta_log_prob (__main__.TestDistributions) ... ok
test_beta_sample (__main__.TestDistributions) ... ok
test_beta_shape (__main__.TestDistributions) ... ok
test_beta_underflow (__main__.TestDistributions) ... ok
test_beta_underflow_gpu (__main__.TestDistributions) ... ok
test_binomial (__main__.TestDistributions) ... ok
test_binomial_enumerate_support (__main__.TestDistributions) ... ok
test_binomial_extreme_vals (__main__.TestDistributions) ... ok
test_binomial_log_prob (__main__.TestDistributions) ... ok
test_binomial_log_prob_vectorized_count (__main__.TestDistributions) ... ok
test_binomial_sample (__main__.TestDistributions) ... ok
test_binomial_stable (__main__.TestDistributions) ... ok
test_binomial_vectorized_count (__main__.TestDistributions) ... ok
test_categorical_1d (__main__.TestDistributions) ... ok
test_categorical_2d (__main__.TestDistributions) ... ok
test_categorical_enumerate_support (__main__.TestDistributions) ... ok
test_cauchy (__main__.TestDistributions) ... ok
test_cdf_icdf_inverse (__main__.TestDistributions) ... ok
test_cdf_log_prob (__main__.TestDistributions) ... ok
test_chi2_sample (__main__.TestDistributions) ... ok
test_chi2_shape (__main__.TestDistributions) ... ok
test_continuous_bernoulli (__main__.TestDistributions) ... ok
test_continuous_bernoulli_3d (__main__.TestDistributions) ... ok
test_dirichlet_log_prob (__main__.TestDistributions) ... ok
test_dirichlet_sample (__main__.TestDistributions) ... ok
test_dirichlet_shape (__main__.TestDistributions) ... ok
test_distribution_expand (__main__.TestDistributions) ... ok
test_distribution_subclass_expand (__main__.TestDistributions) ... ok
test_enumerate_support_type (__main__.TestDistributions) ... ok
test_exponential (__main__.TestDistributions) ... ok
test_exponential_sample (__main__.TestDistributions) ... ok
test_fishersnedecor (__main__.TestDistributions) ... ok
test_fishersnedecor_sample (__main__.TestDistributions) ... ok
test_gamma_gpu_sample (__main__.TestDistributions) ... ok
test_gamma_gpu_shape (__main__.TestDistributions) ... ok
test_gamma_sample (__main__.TestDistributions) ... ok
test_gamma_shape (__main__.TestDistributions) ... ok
test_geometric (__main__.TestDistributions) ... ok
test_geometric_log_prob_and_entropy (__main__.TestDistributions) ... ok
test_geometric_sample (__main__.TestDistributions) ... ok
test_gumbel (__main__.TestDistributions) ... ok
test_gumbel_sample (__main__.TestDistributions) ... ok
test_halfcauchy (__main__.TestDistributions) ... ok
test_halfnormal (__main__.TestDistributions) ... ok
test_halfnormal_logprob (__main__.TestDistributions) ... ok
test_halfnormal_sample (__main__.TestDistributions) ... ok
test_has_examples (__main__.TestDistributions) ... ok
test_independent_expand (__main__.TestDistributions) ... ok
test_independent_shape (__main__.TestDistributions) ... ok
test_invalid_parameter_broadcasting (__main__.TestDistributions) ... ok
test_kumaraswamy_mean_variance (__main__.TestDistributions) ... ok
test_kumaraswamy_shape (__main__.TestDistributions) ... ok
test_laplace (__main__.TestDistributions) ... ok
test_laplace_sample (__main__.TestDistributions) ... ok
test_lazy_property_grad (__main__.TestDistributions) ... ok
test_lkj_cholesky_log_prob (__main__.TestDistributions) ... ok
test_logisticnormal (__main__.TestDistributions) ... ok
test_logisticnormal_logprob (__main__.TestDistributions) ... ok
test_logisticnormal_sample (__main__.TestDistributions) ... ok
test_lognormal (__main__.TestDistributions) ... ok
test_lognormal_logprob (__main__.TestDistributions) ... ok
test_lognormal_sample (__main__.TestDistributions) ... ok
test_lowrank_multivariate_normal_log_prob (__main__.TestDistributions) ... ok
test_lowrank_multivariate_normal_moments (__main__.TestDistributions) ... ok
test_lowrank_multivariate_normal_properties (__main__.TestDistributions) ... ok
test_lowrank_multivariate_normal_sample (__main__.TestDistributions) ... ok
test_lowrank_multivariate_normal_shape (__main__.TestDistributions) ... ok
test_mixture_same_family_log_prob (__main__.TestDistributions) ... ok
test_mixture_same_family_sample (__main__.TestDistributions) ... ok
test_mixture_same_family_shape (__main__.TestDistributions) ... ok
test_multinomial_1d (__main__.TestDistributions) ... ok
test_multinomial_1d_log_prob (__main__.TestDistributions) ... ok
test_multinomial_2d (__main__.TestDistributions) ... ok
test_multivariate_normal_log_prob (__main__.TestDistributions) ... ok
test_multivariate_normal_moments (__main__.TestDistributions) ... ok
test_multivariate_normal_properties (__main__.TestDistributions) ... ok
test_multivariate_normal_sample (__main__.TestDistributions) ... ok
test_multivariate_normal_shape (__main__.TestDistributions) ... ok
test_multivariate_normal_stable_with_precision_matrix (__main__.TestDistributions) ... ok
test_negative_binomial (__main__.TestDistributions) ... ok
test_negative_binomial_log_prob (__main__.TestDistributions) ... ok
test_negative_binomial_log_prob_vectorized_count (__main__.TestDistributions) ... ok
test_normal (__main__.TestDistributions) ... ok
test_normal_sample (__main__.TestDistributions) ... ok
test_one_hot_categorical_1d (__main__.TestDistributions) ... ok
test_one_hot_categorical_2d (__main__.TestDistributions) ... ok
test_one_hot_categorical_enumerate_support (__main__.TestDistributions) ... ok
test_pareto (__main__.TestDistributions) ... ok
test_pareto_sample (__main__.TestDistributions) ... ok
test_poisson_gpu_sample (__main__.TestDistributions) ... ok
test_poisson_log_prob (__main__.TestDistributions) ... ok
test_poisson_sample (__main__.TestDistributions) ... ok
test_poisson_shape (__main__.TestDistributions) ... ok
test_relaxed_bernoulli (__main__.TestDistributions) ... ok
test_relaxed_one_hot_categorical_1d (__main__.TestDistributions) ... ok
test_relaxed_one_hot_categorical_2d (__main__.TestDistributions) ... ok
test_repr (__main__.TestDistributions) ... ok
test_rounded_relaxed_bernoulli (__main__.TestDistributions) ... ok
test_rsample_requires_grad (__main__.TestDistributions) ... ok
test_sample_detached (__main__.TestDistributions) ... ok
test_studentT (__main__.TestDistributions) ... ok
test_studentT_log_prob (__main__.TestDistributions) ... ok
test_studentT_sample (__main__.TestDistributions) ... ok
test_support_attributes (__main__.TestDistributions) ... ok
test_uniform (__main__.TestDistributions) ... ok
test_valid_parameter_broadcasting (__main__.TestDistributions) ... ok
test_vonmises_logprob (__main__.TestDistributions) ... ok
test_vonmises_sample (__main__.TestDistributions) ... ok
test_zero_excluded_binomial (__main__.TestDistributions) ... FAIL
test_cat_event_dim (__main__.TestFunctors) ... ok
test_cat_transform (__main__.TestFunctors) ... ok
test_cat_transform_non_uniform (__main__.TestFunctors) ... ok
test_stack_transform (__main__.TestFunctors) ... ok
test_cdf (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/distributions/distribution.py:54: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not valid.all():
/opt/conda/lib/python3.6/site-packages/torch/distributions/distribution.py:62: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not constraint.check(getattr(self, param)).all():
/opt/conda/lib/python3.6/site-packages/torch/distributions/geometric.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not valid.all():
/opt/conda/lib/python3.6/site-packages/torch/distributions/distribution.py:275: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if i != 1 and j != 1 and i != j:
/opt/conda/lib/python3.6/site-packages/torch/distributions/distribution.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not valid.all():
/opt/conda/lib/python3.6/site-packages/torch/distributions/utils.py:38: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  for v in values]
/opt/conda/lib/python3.6/site-packages/torch/distributions/uniform.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self._validate_args and not torch.lt(self.low, self.high).all():
/opt/conda/lib/python3.6/site-packages/torch/distributions/transformed_distribution.py:65: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if base_shape != expanded_base_shape:
/opt/conda/lib/python3.6/site-packages/torch/distributions/distribution.py:268: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if value.size()[event_dim_start:] != self._event_shape:
/opt/conda/lib/python3.6/site-packages/torch/distributions/lowrank_multivariate_normal.py:89: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if cov_factor.shape[-2:-1] != event_shape:
/opt/conda/lib/python3.6/site-packages/torch/distributions/lowrank_multivariate_normal.py:92: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if cov_diag.shape[-1:] != event_shape:
/opt/conda/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py:73: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  L = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),
ok
test_entropy (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2979: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (target.size() == input.size()):
ok
test_enumerate_support (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/distributions/one_hot_categorical.py:99: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  values = torch.eye(n, dtype=self._param.dtype, device=self._param.device)
ok
test_log_prob (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/distributions/gamma.py:66: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  value = torch.as_tensor(value, dtype=self.rate.dtype, device=self.rate.device)
/opt/conda/lib/python3.6/site-packages/torch/distributions/half_cauchy.py:56: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  device=self.base_dist.scale.device)
/opt/conda/lib/python3.6/site-packages/torch/distributions/relaxed_categorical.py:80: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  log_scale = (torch.full_like(self.temperature, float(K)).lgamma() -
ok
test_mean (__main__.TestJit) ... ok
test_rsample (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/random.py:111: UserWarning: CUDA reports that you have 8 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of GPUs.  If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using.  For example, if you are using CPU only, set CUDA_VISIBLE_DEVICES= or devices=[]; if you are using GPU 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.
  ).format(num_devices=num_devices, caller=_caller, devices_kw=_devices_kw))
/opt/conda/lib/python3.6/site-packages/torch/distributions/one_hot_categorical.py:86: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return torch.nn.functional.one_hot(indices, num_events).to(probs)
ok
test_sample (__main__.TestJit) ... ok
test_variance (__main__.TestJit) ... ok
test_entropy_exponential_family (__main__.TestKL) ... ok
test_entropy_monte_carlo (__main__.TestKL) ... ok
test_kl_edgecases (__main__.TestKL) ... ok
test_kl_exponential_family (__main__.TestKL) ... ok
test_kl_infinite (__main__.TestKL) ... ok
test_kl_lowrank_multivariate_normal (__main__.TestKL) ... ok
test_kl_lowrank_multivariate_normal_batched (__main__.TestKL) ... ok
test_kl_monte_carlo (__main__.TestKL) ... ok
test_kl_multivariate_normal (__main__.TestKL) ... ok
test_kl_multivariate_normal_batched (__main__.TestKL) ... ok
test_kl_multivariate_normal_batched_broadcasted (__main__.TestKL) ... ok
test_kl_shape (__main__.TestKL) ... ok
test_kl_transformed (__main__.TestKL) ... ok
test_lazy_logits_initialization (__main__.TestLazyLogitsInitialization) ... ok
test_lazy_probs_initialization (__main__.TestLazyLogitsInitialization) ... ok
test_bernoulli_gradient (__main__.TestNumericalStability) ... ok
test_bernoulli_with_logits_overflow (__main__.TestNumericalStability) ... ok
test_bernoulli_with_logits_underflow (__main__.TestNumericalStability) ... ok
test_categorical_log_prob (__main__.TestNumericalStability) ... ok
test_categorical_log_prob_with_logits (__main__.TestNumericalStability) ... ok
test_continuous_bernoulli_gradient (__main__.TestNumericalStability) ... ok
test_continuous_bernoulli_with_logits_overflow (__main__.TestNumericalStability) ... ok
test_continuous_bernoulli_with_logits_underflow (__main__.TestNumericalStability) ... ok
test_multinomial_log_prob (__main__.TestNumericalStability) ... ok
test_multinomial_log_prob_with_logits (__main__.TestNumericalStability) ... ok
test_beta_wrt_alpha (__main__.TestRsample) ... ok
test_beta_wrt_beta (__main__.TestRsample) ... ok
test_chi2 (__main__.TestRsample) ... ok
test_dirichlet_multivariate (__main__.TestRsample) ... ok
test_dirichlet_on_diagonal (__main__.TestRsample) ... ok
test_dirichlet_tangent_field (__main__.TestRsample) ... ok
test_gamma (__main__.TestRsample) ... ok
test_invalid (__main__.TestValidation) ... ok
test_invalid_log_probs_arg (__main__.TestValidation) ... ok
test_valid (__main__.TestValidation) ... ok
test_warning_unimplemented_constraints (__main__.TestValidation) ... ok

======================================================================
FAIL: test_zero_excluded_binomial (__main__.TestDistributions)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "distributions/test_distributions.py", line 1171, in test_zero_excluded_binomial
    self.assertTrue((vals >= 0).all())
AssertionError: tensor(False, device='cuda:0') is not true

----------------------------------------------------------------------
Ran 207 tests in 32.579s

FAILED (failures=1)
distributions/test_distributions failed!
Running test_ao_sparsity ... [2021-10-12 09:42:58.229131]
Executing ['/opt/conda/bin/python3.6', 'test_ao_sparsity.py', '-v'] ... [2021-10-12 09:42:58.229194]
test_constructor (ao.sparsity.test_sparsifier.TestBaseSparsifier) ... ok
test_mask_squash (ao.sparsity.test_sparsifier.TestBaseSparsifier) ... ok
test_state_dict (ao.sparsity.test_sparsifier.TestBaseSparsifier) ... ok
test_step (ao.sparsity.test_sparsifier.TestBaseSparsifier) ... ok
test_jit_trace (ao.sparsity.test_parametrization.TestFakeSparsity) ... /opt/conda/lib/python3.6/site-packages/torch/ao/sparsity/sparsifier/utils.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert self.mask.shape == x.shape
ok
test_masking_logic (ao.sparsity.test_parametrization.TestFakeSparsity) ... ok
test_state_dict_preserved (ao.sparsity.test_parametrization.TestFakeSparsity) ... ok
test_weights_parametrized (ao.sparsity.test_parametrization.TestFakeSparsity) ... ok
test_sparse_qlinear (ao.sparsity.test_kernels.TestQuantizedSparseKernels) ... [W TensorImpl.h:1378] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())
2021-10-12 09:42:59,556 - root - INFO - static sparse qlinear is only available in fbgemm
2021-10-12 09:42:59,628 - root - INFO - static sparse qlinear is only available in fbgemm
2021-10-12 09:42:59,635 - root - INFO - dynamic sparse qlinear is only available in qnnpack
2021-10-12 09:42:59,653 - root - INFO - dynamic sparse qlinear is only available in qnnpack
ok
test_sparse_qlinear (ao.sparsity.test_kernels.TestQuantizedSparseLayers) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/utils.py:158: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  "Returning default values."
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
/opt/conda/lib/python3.6/site-packages/torch/quantization/utils.py:150: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  "Returning default values."
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
ok
test_sparse_qlinear_serdes (ao.sparsity.test_kernels.TestQuantizedSparseLayers) ... ok
test_constructor (ao.sparsity.test_scheduler.TestScheduler) ... ok
test_lambda_scheduler (ao.sparsity.test_scheduler.TestScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/ao/sparsity/scheduler/base_scheduler.py:125: UserWarning: Detected call of `scheduler.step()` before `sparsifier.step()`. You have to make sure you run the sparsifier.step() BEFORE any calls to the scheduer.step().
  "calls to the scheduer.step().", UserWarning)
ok
test_order_of_steps (ao.sparsity.test_scheduler.TestScheduler)
Checks if the warning is thrown if the scheduler step is called ... ok
test_step (ao.sparsity.test_scheduler.TestScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py:9: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  rows = idx // shape[1]
ok
test_constructor (ao.sparsity.test_sparsifier.TestWeightNormSparsifier) ... ok
test_mask_squash (ao.sparsity.test_sparsifier.TestWeightNormSparsifier) ... ok
test_prepare (ao.sparsity.test_sparsifier.TestWeightNormSparsifier) ... ok
test_step (ao.sparsity.test_sparsifier.TestWeightNormSparsifier) ... ok

----------------------------------------------------------------------
Ran 19 tests in 0.598s

OK
Running test_autocast ... [2021-10-12 09:43:01.312308]
Executing ['/opt/conda/bin/python3.6', 'test_autocast.py', '-v'] ... [2021-10-12 09:43:01.312386]
test_autocast_methods_expect_builtin_promote (__main__.TestAutocastCPU) ... ok
test_autocast_nn_bf16 (__main__.TestAutocastCPU) ... ok
test_autocast_nn_fp32 (__main__.TestAutocastCPU) ... ok
test_autocast_torch_bf16 (__main__.TestAutocastCPU) ... ok
test_autocast_torch_expect_builtin_promote (__main__.TestAutocastCPU) ... ok
test_autocast_torch_fp32 (__main__.TestAutocastCPU) ... ok
test_autocast_torch_need_autocast_promote (__main__.TestAutocastCPU) ... ok

----------------------------------------------------------------------
Ran 7 tests in 0.336s

OK
Running test_autograd ... [2021-10-12 09:43:04.035930]
Executing ['/opt/conda/bin/python3.6', 'test_autograd.py', '-v'] ... [2021-10-12 09:43:04.036008]
test_accumulate_grad (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/autograd/engine.cpp:976.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
ok
test_accumulate_grad_tensor_reference (__main__.TestAutograd) ... ok
test_anomaly_assign_parent_cleanup (__main__.TestAutograd) ... test_autograd.py:3626: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with detect_anomaly():
ok
test_anomaly_detect_nan (__main__.TestAutograd) ... ok
test_anomaly_grad_warnings (__main__.TestAutograd) ... ok
test_as_strided (__main__.TestAutograd) ... ok
test_attribute_deletion (__main__.TestAutograd) ... ok
test_autograd_inplace_views_creation_meta (__main__.TestAutograd) ... ok
test_autograd_inplace_views_cross_dtype (__main__.TestAutograd) ... ok
test_autograd_multiple_views_python (__main__.TestAutograd) ... ok
test_autograd_python_custom_function_inplace (__main__.TestAutograd) ... ok
test_autograd_simple_views_python (__main__.TestAutograd) ... ok
test_autograd_views_codegen (__main__.TestAutograd) ... ok
test_backward (__main__.TestAutograd) ... ok
test_backward_badcalls (__main__.TestAutograd) ... ok
test_backward_copy (__main__.TestAutograd) ... ok
test_backward_create_graph_warns (__main__.TestAutograd) ... ok
test_backward_no_grad (__main__.TestAutograd) ... ok
test_backward_twice_retained_graph_with_saved_values (__main__.TestAutograd) ... ok
test_backward_twice_retained_graph_without_saved_values (__main__.TestAutograd) ... ok
test_backward_twice_with_saved_values (__main__.TestAutograd) ... ok
test_backward_twice_without_saved_values (__main__.TestAutograd) ... ok
test_backward_with_inputs (__main__.TestAutograd) ... ok
test_backward_with_nonleaf_inputs (__main__.TestAutograd) ... ok
test_block_diag (__main__.TestAutograd) ... ok
test_broadcast_tensors (__main__.TestAutograd) ... ok
test_callback_adds_callback (__main__.TestAutograd) ... ok
test_cant_create_saved_tensors (__main__.TestAutograd) ... ok
test_cat_empty_legacy (__main__.TestAutograd) ... ok
test_checkpoint_valid_reset_on_error (__main__.TestAutograd) ... ok
test_checkpointing (__main__.TestAutograd) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_custom_autograd_no_early_free (__main__.TestAutograd) ... ok
test_custom_autograd_repeated_grad_grad (__main__.TestAutograd) ... ok
test_custom_function_cycle (__main__.TestAutograd) ... ok
test_custom_function_error (__main__.TestAutograd) ... ok
test_custom_function_exception (__main__.TestAutograd) ... ok
test_custom_function_forward_mode_inplace_checks (__main__.TestAutograd) ... ok
test_custom_function_forward_mode_view_checks (__main__.TestAutograd) ... ok
test_custom_function_forward_mode_wrong_formula (__main__.TestAutograd) ... ok
test_custom_function_local_inplace (__main__.TestAutograd) ... ok
test_custom_function_no_tensors (__main__.TestAutograd) ... ok
test_custom_function_non_tensor_inputs_outputs (__main__.TestAutograd) ... ok
test_custom_function_return_view_in_nograd (__main__.TestAutograd) ... ok
test_custom_function_saved_tensors (__main__.TestAutograd) ... ok
test_deep_reentrant (__main__.TestAutograd) ... ok
test_default_saved_variable_hooks_double_backward (__main__.TestAutograd) ... ok
test_dep_nograd (__main__.TestAutograd) ... ok
test_dependent_backward (__main__.TestAutograd) ... ok
test_detach (__main__.TestAutograd) ... ok
test_detach_base (__main__.TestAutograd)
detaching base does not detach view ... ok
test_diagonal_derivative_requires_grad (__main__.TestAutograd) ... ok
test_diagonal_expanded_v (__main__.TestAutograd) ... test_autograd.py:2026: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  v_expanded = torch.tensor(value).expand(10)
ok
test_dir (__main__.TestAutograd) ... ok
test_dont_materialize_grads (__main__.TestAutograd) ... ok
test_duplicate_backward_root (__main__.TestAutograd) ... ok
test_eig_complex_eigenvalues (__main__.TestAutograd) ... test_autograd.py:3719: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
L, _ = torch.eig(A)
should be replaced with
L_complex = torch.linalg.eigvals(A)
and
L, V = torch.eig(A, eigenvectors=True)
should be replaced with
L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2894.)
  w, v = torch.eig(A, eigenvectors=True)
ok
test_eig_no_eigenvectors (__main__.TestAutograd) ... ok
test_fill (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_free_deep_graph (__main__.TestAutograd) ... ok
test_free_deep_graph_complicated (__main__.TestAutograd) ... ok
test_free_deep_graph_pyfunction (__main__.TestAutograd) ... ok
test_function (__main__.TestAutograd) ... ok
test_function_returns_input (__main__.TestAutograd) ... ok
test_function_returns_undefined_tensor (__main__.TestAutograd) ... ok
test_gc_in_destructor (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/function.py:258: DeprecationWarning: <class '__main__.TestAutograd.test_gc_in_destructor.<locals>.CollectOnDelete'> should not be instantiated. Methods on autograd functionsare all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.
  "error in a future version of PyTorch.", DeprecationWarning)
ok
test_grad (__main__.TestAutograd) ... ok
test_grad_badcalls (__main__.TestAutograd) ... ok
test_grad_empty_inputs (__main__.TestAutograd) ... ok
test_grad_fn_attr_bindings (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
ok
test_grad_fn_badcalls (__main__.TestAutograd) ... ok
test_grad_mode_restored_reentrant (__main__.TestAutograd) ... ok
test_grad_nonleaf (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:1012: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /var/lib/jenkins/pytorch/build/aten/src/ATen/core/TensorBody.h:420.)
  return self._grad
ok
test_grad_nonleaf_many_outputs (__main__.TestAutograd) ... ok
test_grad_nonleaf_register_hook (__main__.TestAutograd) ... ok
test_grad_unreachable (__main__.TestAutograd) ... ok
test_grad_unreachable_discovery (__main__.TestAutograd) ... ok
test_gradcheck_backward_mul_by_grad_output (__main__.TestAutograd) ... ok
test_gradcheck_check_batched_grad (__main__.TestAutograd) ... ok
test_gradcheck_check_no_differentiable_outputs (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_gradcheck_complex_non_complex_outputs (__main__.TestAutograd) ... ok
test_gradcheck_custom_error (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_gradcheck_dense_and_sparse_inputs (__main__.TestAutograd) ... ok
test_gradcheck_forward_ad (__main__.TestAutograd) ... ok
test_gradcheck_get_analytical_jacobian (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:575: UserWarning: get_analytical_jacobian was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new
  warnings.warn("get_analytical_jacobian was part of PyTorch's private API and not "
ok
test_gradcheck_get_numerical_jacobian (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:168: UserWarning: get_numerical_jacobian was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new
  warnings.warn("get_numerical_jacobian was part of PyTorch's private API and not "
ok
test_gradcheck_jacobian_mismatch (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_gradcheck_multiple_mkldnn_inputs (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_gradcheck_nondeterministic (__main__.TestAutograd) ... ok
test_gradcheck_output_shape_or_dtype_depend_on_values (__main__.TestAutograd) ... ok
test_gradcheck_single_input (__main__.TestAutograd) ... ok
test_gradcheck_sparse_input (__main__.TestAutograd) ... ok
test_gradcheck_test_outputs (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_gradcheck_undefined_grad (__main__.TestAutograd) ... ok
test_gradcheck_validates_input_mkldnn (__main__.TestAutograd) ... ok
test_gradcheck_validates_inputs (__main__.TestAutograd) ... ok
test_graph_save_on_cpu (__main__.TestAutograd) ... ok
test_graph_save_on_cpu_cuda (__main__.TestAutograd) ... ok
test_hessian_vector (__main__.TestAutograd) ... ok
test_hook_none (__main__.TestAutograd) ... ok
test_hook_with_no_name (__main__.TestAutograd) ... ok
test_hooks (__main__.TestAutograd) ... ok
test_hooks_cpp (__main__.TestAutograd) ... ok
test_index_backward_does_not_save_tensor (__main__.TestAutograd) ... ok
test_indexing (__main__.TestAutograd) ... ok
test_indexing_duplicates (__main__.TestAutograd) ... ok
test_inplace (__main__.TestAutograd) ... ok
test_inplace_not_requires_grad (__main__.TestAutograd) ... ok
test_inplace_on_view_backward (__main__.TestAutograd) ... ok
test_inplace_on_view_leaf_errors (__main__.TestAutograd) ... ok
test_inplace_on_view_saved_output (__main__.TestAutograd) ... ok
test_inplace_on_view_weak_grad_fn (__main__.TestAutograd) ... ok
test_input_buffer_accum (__main__.TestAutograd) ... ok
test_integer_outputs (__main__.TestAutograd) ... ok
test_invalid_gradients (__main__.TestAutograd) ... ok
test_isolated_node (__main__.TestAutograd) ... ok
test_leaf_assignment (__main__.TestAutograd) ... ok
test_legacy_function_deprecation_exception (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/autograd/function.py:258: DeprecationWarning: <class '__main__.TestAutograd.test_legacy_function_deprecation_exception.<locals>.MyFunction'> should not be instantiated. Methods on autograd functionsare all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.
  "error in a future version of PyTorch.", DeprecationWarning)
ok
test_lerp_tensor_weights (__main__.TestAutograd) ... ok
test_lobpcg (__main__.TestAutograd) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_mark_non_differentiable (__main__.TestAutograd) ... ok
test_mark_non_differentiable_mixed (__main__.TestAutograd) ... ok
test_mark_non_differentiable_none (__main__.TestAutograd) ... ok
test_materialize_grads (__main__.TestAutograd) ... ok
test_maximum_and_minimum_subgradient (__main__.TestAutograd) ... ok
test_mul_out (__main__.TestAutograd) ... ok
test_mul_out_result_requires_grad (__main__.TestAutograd) ... ok
test_multi_backward (__main__.TestAutograd) ... ok
test_multi_backward_no_grad (__main__.TestAutograd) ... ok
test_named_tensor_for_complex_views (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:814: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /var/lib/jenkins/pytorch/c10/core/TensorImpl.h:1378.)
  return super(Tensor, self).refine_names(names)
ok
test_nan_to_num (__main__.TestAutograd) ... ok
test_nansum_dtype (__main__.TestAutograd) ... ok
test_nansum_with_nans (__main__.TestAutograd) ... ok
test_naughty_anomaly_access (__main__.TestAutograd) ... expected failure
test_naughty_autograd_function_attribute_access (__main__.TestAutograd) ... ok
test_naughty_autograd_function_stashing_ctx (__main__.TestAutograd) ... ok
test_nested_anomaly_detect_nan (__main__.TestAutograd) ... ok
test_nested_anomaly_printstack_cleanup (__main__.TestAutograd) ... ok
test_next_functions (__main__.TestAutograd) ... ok
test_no_grad (__main__.TestAutograd) ... ok
test_no_grad_assignment (__main__.TestAutograd) ... ok
test_no_grad_copy (__main__.TestAutograd) ... ok
test_no_grad_copy_sparse (__main__.TestAutograd) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2159: UserWarning: Argument order of nn.functional.embedding_bag was changed. Usage `embedding_bag(weight, input, ...)` is deprecated, and should now be `embedding_bag(input, weight, ...)`.
  "Argument order of nn.functional.embedding_bag was changed. "
ok
test_no_grad_input (__main__.TestAutograd) ... ok
test_no_grad_modifies_version (__main__.TestAutograd) ... ok
test_no_grad_python_function (__main__.TestAutograd)
Python Functions should respect grad mode. ... ok
test_no_requires_grad_inplace (__main__.TestAutograd) ... ok
test_no_unnecessary_save (__main__.TestAutograd) ... ok
test_no_unnecessary_unwrapping (__main__.TestAutograd) ... ok
test_norm_inf_subgradient (__main__.TestAutograd) ... test_autograd.py:2878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(input, requires_grad=True)
ok
test_norm_subgradient (__main__.TestAutograd) ... ok
test_not_implemented_fwad (__main__.TestAutograd) ... ok
test_not_implemented_grad (__main__.TestAutograd) ... ok
test_numpy_requires_grad (__main__.TestAutograd) ... ok
test_once_differentiable (__main__.TestAutograd) ... ok
test_pack_hook_with_inplace_modification_should_fail (__main__.TestAutograd) ... ok
test_pickle (__main__.TestAutograd) ... ok
test_pow_scalar_base (__main__.TestAutograd) ... ok
test_pow_zero_tensor_gradient (__main__.TestAutograd) ... ok
test_power_function (__main__.TestAutograd) ... ok
test_profiler (__main__.TestAutograd) ... test_autograd.py:2929: DeprecationWarning: Please use assertEqual instead.
  self.assertEquals(len(found_indices), len(names))
ok
test_profiler_aggregation_fake (__main__.TestAutograd) ... ok
test_profiler_aggregation_lstm (__main__.TestAutograd) ... 
===================================================================================================================================================================
TEST
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                      Input Shapes  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                     aten::mm         3.75%       1.642ms         3.75%       1.643ms       1.643ms             1                              [[15, 10], [10, 80]]  
                   aten::lstm         2.46%       1.077ms        11.82%       5.180ms       5.180ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
              aten::transpose         1.51%     660.000us         1.51%     660.000us     660.000us             1                                [[80, 20], [], []]  
                  aten::randn         1.14%     498.000us         1.15%     504.000us     504.000us             1                              [[], [], [], [], []]  
                 aten::expand         0.65%     286.000us         0.65%     287.000us     287.000us             1                                    [[80], [], []]  
                   aten::lstm         0.51%     224.000us         4.84%       2.123ms       2.123ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.51%     223.000us         4.62%       2.024ms       2.024ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.50%     220.000us         4.49%       1.970ms       1.970ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.50%     219.000us         4.69%       2.055ms       2.055ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.49%     215.000us         5.97%       2.616ms       2.616ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
Self CPU time total: 43.842ms

-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                      Input Shapes  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                  aten::addmm        25.13%      11.016ms        28.38%      12.443ms      62.215us           200                 [[80], [3, 20], [20, 80], [], []]  
                   aten::lstm        11.24%       4.926ms        97.85%      42.901ms       2.145ms            20      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                     aten::mm         8.27%       3.624ms         8.30%       3.638ms     181.900us            20                              [[15, 10], [10, 80]]  
                  aten::slice         4.72%       2.071ms         6.19%       2.716ms       3.395us           800                         [[3, 80], [], [], [], []]  
                     aten::mm         4.24%       1.858ms         4.28%       1.877ms      93.850us            20                              [[15, 20], [20, 80]]  
                    aten::mul         4.16%       1.825ms         4.16%       1.825ms       3.042us           600                                [[3, 20], [3, 20]]  
               aten::sigmoid_         3.98%       1.745ms         3.98%       1.745ms       2.908us           600                                         [[3, 20]]  
           aten::unsafe_split         3.43%       1.505ms        12.83%       5.623ms      28.115us           200                                 [[3, 80], [], []]  
                 aten::narrow         3.43%       1.504ms         9.30%       4.076ms       5.095us           800                             [[3, 80], [], [], []]  
              aten::transpose         2.94%       1.289ms         3.22%       1.413ms       6.423us           220                                [[80, 20], [], []]  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
Self CPU time total: 43.842ms

===================================================================================================================================================================
TEST
===================================================================================================================================================================
This report only display top-level ops statistics
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                      Input Shapes  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                   aten::lstm         2.46%       1.077ms        11.82%       5.180ms       5.180ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                  aten::randn         1.14%     498.000us         1.15%     504.000us     504.000us             1                              [[], [], [], [], []]  
                   aten::lstm         0.51%     224.000us         4.84%       2.123ms       2.123ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.51%     223.000us         4.62%       2.024ms       2.024ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.50%     220.000us         4.49%       1.970ms       1.970ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.50%     219.000us         4.69%       2.055ms       2.055ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.49%     215.000us         5.97%       2.616ms       2.616ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.49%     215.000us         4.62%       2.026ms       2.026ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.48%     209.000us         4.74%       2.076ms       2.076ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                   aten::lstm         0.47%     208.000us         5.07%       2.222ms       2.222ms             1      [[5, 3, 10], [], [], [], [], [], [], [], []]  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
Self CPU time total: 43.842ms

===================================================================================================================================================================
This report only display top-level ops statistics
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                      Input Shapes  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
                   aten::lstm        11.24%       4.926ms        97.85%      42.901ms       2.145ms            20      [[5, 3, 10], [], [], [], [], [], [], [], []]  
                  aten::randn         1.66%     729.000us         2.15%     941.000us      15.683us            60                              [[], [], [], [], []]  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------  
Self CPU time total: 43.842ms
ok
test_profiler_aggregation_table (__main__.TestAutograd) ... ok
test_profiler_function_event_avg (__main__.TestAutograd) ... ok
test_profiler_propagation (__main__.TestAutograd) ... ok
test_profiler_seq_nr (__main__.TestAutograd) ... ok
test_profiler_shapes (__main__.TestAutograd) ... ok
test_profiler_unboxed_only (__main__.TestAutograd) ... ok
test_record_function (__main__.TestAutograd) ... ok
test_record_function_callbacks (__main__.TestAutograd) ... ok
test_record_function_multithreaded (__main__.TestAutograd) ... ok
test_reduce_dtype (__main__.TestAutograd) ... ok
test_reentrant_child_error (__main__.TestAutograd) ... ok
test_reentrant_priority (__main__.TestAutograd) ... ok
test_reentrant_with_callbacks_both_depths (__main__.TestAutograd) ... ok
test_reentrant_with_callbacks_depth_0 (__main__.TestAutograd) ... ok
test_reentrant_with_callbacks_depth_1 (__main__.TestAutograd) ... ok
test_reentrant_with_leaf_variable_hook (__main__.TestAutograd) ... ok
test_reentrant_with_non_leaf_variable_hook (__main__.TestAutograd) ... ok
test_requires_grad (__main__.TestAutograd) ... ok
test_requires_grad_ (__main__.TestAutograd) ... ok
test_requires_grad_inplace (__main__.TestAutograd) ... ok
test_resize (__main__.TestAutograd) ... ok
test_retain_grad (__main__.TestAutograd) ... ok
test_retain_grad_cycle (__main__.TestAutograd) ... ok
test_return_duplicate (__main__.TestAutograd) ... ok
test_return_duplicate_inplace (__main__.TestAutograd) ... ok
test_return_leaf (__main__.TestAutograd) ... ok
test_return_leaf_inplace (__main__.TestAutograd) ... ok
test_save_none_for_backward (__main__.TestAutograd) ... ok
test_save_output_nr (__main__.TestAutograd) ... ok
test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks (__main__.TestAutograd) ... ok
test_saved_variable_packing_unpacking_did_not_save_original_with_hooks (__main__.TestAutograd) ... ok
test_saved_variable_packing_unpacking_saved_original_with_default_hooks (__main__.TestAutograd) ... ok
test_saved_variable_packing_unpacking_saved_original_with_hooks (__main__.TestAutograd) ... ok
test_saved_variable_version_counter (__main__.TestAutograd) ... ok
test_saved_variables_deprecated (__main__.TestAutograd) ... ok
test_saving_variable_to_disk (__main__.TestAutograd) ... ok
test_select_expanded_v (__main__.TestAutograd) ... ok
test_select_sum (__main__.TestAutograd) ... ok
test_set_data_preserve_pyobj (__main__.TestAutograd) ... ok
test_set_data_tensorimpl_type (__main__.TestAutograd) ... ok
test_set_grad_coroutines (__main__.TestAutograd) ... ok
test_set_grad_coroutines_benign_exceptions (__main__.TestAutograd) ... ok
test_set_grad_coroutines_critical_exceptions (__main__.TestAutograd) ... ok
test_set_grad_coroutines_exit (__main__.TestAutograd) ... ok
test_set_grad_enabled (__main__.TestAutograd) ... ok
test_set_grad_generator_functions (__main__.TestAutograd) ... ok
test_set_grad_generator_functions_recursive (__main__.TestAutograd) ... ok
test_setitem (__main__.TestAutograd) ... ok
test_setitem_mask (__main__.TestAutograd) ... ok
test_setting_default_saved_variable_hooks_twice_should_fail (__main__.TestAutograd) ... ok
test_shape (__main__.TestAutograd) ... ok
test_sharded_grad (__main__.TestAutograd) ... ok
test_simple_reentrant (__main__.TestAutograd) ... ok
test_sinc (__main__.TestAutograd) ... ok
test_slice_expanded_v (__main__.TestAutograd) ... ok
test_slogdet_sign (__main__.TestAutograd) ... ok
test_sparse_gather_both_scalar (__main__.TestAutograd) ... ok
test_sparse_gather_dim0 (__main__.TestAutograd) ... ok
test_sparse_gather_dim1 (__main__.TestAutograd) ... ok
test_sparse_gather_dim_neg (__main__.TestAutograd) ... ok
test_sparse_gather_ind_scalar (__main__.TestAutograd) ... ok
test_sparse_gather_x_scalar (__main__.TestAutograd) ... ok
test_sparse_mm_backward (__main__.TestAutograd) ... ok
test_sum_to_with_empty_dim_grad (__main__.TestAutograd) ... ok
test_svd_no_singularvectors (__main__.TestAutograd) ... ok
test_symeig_no_eigenvectors (__main__.TestAutograd) ... test_autograd.py:3726: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)
  w, v = torch.symeig(A, eigenvectors=False)
ok
test_tensor_grad_warnings (__main__.TestAutograd) ... ok
test_thread_shutdown (__main__.TestAutograd) ... ok
test_too_many_grads (__main__.TestAutograd) ... ok
test_type_conversions (__main__.TestAutograd) ... ok
test_unbind (__main__.TestAutograd) ... ok
test_unrelated_inputs (__main__.TestAutograd) ... ok
test_unused_output (__main__.TestAutograd) ... ok
test_var_mean_differentiable (__main__.TestAutograd) ... ok
test_variable_traverse (__main__.TestAutograd) ... ok
test_version_counter (__main__.TestAutograd) ... ok
test_volatile_deprecated (__main__.TestAutograd) ... ok
test_view_func_for_complex_views (autograd.test_complex.TestAutogradComplex) ... ok
test_view_with_multi_output (autograd.test_complex.TestAutogradComplex) ... ok
test_GRU_grad_and_gradgrad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_LSTM_grad_and_gradgrad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_advanced_indexing_backwards_large_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_advanced_indexing_backwards_memory_format_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_atleast_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_backward_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_cdist_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_cdist_grad_p_lt_1_no_nan_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_cdist_same_inputs_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_copy__cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_copy_r_to_c_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_copysign_subgradient_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_cross_device_reentrant_autograd_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_ctc_loss_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped 'Test is flaky on Linux and Windows, typical error message:\n            https://github.com/pytorch/pytorch/issues/34870'
test_ctc_loss_cudnn_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_elu_inplace_with_neg_alpha_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_free_unneeded_tensor_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_grad_assignment_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_gradcheck_input_output_different_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_multiple_output_view_of_view_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_backprop_base_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_backprop_view_cuda (__main__.TestAutogradDeviceTypeCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:1012: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /var/lib/jenkins/pytorch/build/aten/src/ATen/core/TensorBody.h:420.)
  return self._grad
ok
test_inplace_on_view_backprop_view_of_view_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_gradcheck_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_makes_base_require_grad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_modify_base_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_multi_output_safe_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_multi_output_unsafe_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_multiple_outputs_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_non_contig_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_of_multiple_output_view_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_of_view_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_python_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inplace_on_view_then_no_grad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_inputbuffer_add_multidevice_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_leaky_relu_inplace_with_neg_slope_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_leaky_relu_inplace_with_zero_slope_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_logcumsumexp_large_value_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_lstmcell_backward_only_one_output_grad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_min_max_median_backprops_to_all_values_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_mv_grad_stride_0_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_non_differentiable_ops_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_parameter_resize_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_pdist_large_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_pin_memory_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_profiler_emit_nvtx_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_pyscalar_conversions_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_reentrant_parent_error_on_cpu_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_requires_grad_factory_cuda_float32 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_requires_grad_factory_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_rnn_backward_to_input_but_not_parameters_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_float16 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_float32 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_int16 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_int32 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_int64 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_set_requires_grad_only_for_floats_cuda_int8 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_simple_reentrant_cross_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_sparse_backward_cuda_complex128 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_sparse_backward_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_sparse_ctor_getter_backward_cuda_complex128 (__main__.TestAutogradDeviceTypeCUDA) ... [W Copy.cpp:244] Warning: Casting complex values to real discards the imaginary part (function operator())
ok
test_sparse_ctor_getter_backward_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_sparse_mask_autograd_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_strided_leaf_grad_layout_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_unused_output_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_where_functional_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_where_scalar_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_xlogy_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok
test_advanced_packing_unpacking (__main__.TestAutogradForwardMode) ... ok
test_basic_packing_unpacking (__main__.TestAutogradForwardMode) ... ok
test_default_level (__main__.TestAutogradForwardMode) ... ok
test_detach_view_tracking (__main__.TestAutogradForwardMode) ... ok
test_forward_level_cleanup (__main__.TestAutogradForwardMode) ... ok
test_grad_cleanup (__main__.TestAutogradForwardMode) ... ok
test_nested_level (__main__.TestAutogradForwardMode) ... ok
test_out_variant (__main__.TestAutogradForwardMode) ... ok
test_print (__main__.TestAutogradForwardMode) ... ok
test_size_check (__main__.TestAutogradForwardMode) ... ok
test_view_inplace_differentiable_views (__main__.TestAutogradForwardMode) ... ok
test_view_inplace_non_differentiable_views (__main__.TestAutogradForwardMode) ... ok
test_construct_standard_basis_for (__main__.TestAutogradFunctional) ... ok
test_construct_standard_basis_for_cuda (__main__.TestAutogradFunctional) ... ok
test_hessian_create_graph (__main__.TestAutogradFunctional) ... ok
test_hessian_create_graph_vectorize (__main__.TestAutogradFunctional) ... ok
test_hessian_err_check (__main__.TestAutogradFunctional) ... ok
test_hessian_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_hessian_err_check_strict_vectorize (__main__.TestAutogradFunctional) ... ok
test_hessian_err_check_vectorize (__main__.TestAutogradFunctional) ... ok
test_hessian_match_vhp_hvp (__main__.TestAutogradFunctional) ... ok
test_hessian_no_grad (__main__.TestAutogradFunctional) ... ok
test_hessian_output (__main__.TestAutogradFunctional) ... ok
test_hessian_output_vectorize (__main__.TestAutogradFunctional) ... ok
test_hessian_scalar (__main__.TestAutogradFunctional) ... ok
test_hessian_scalar_vectorize (__main__.TestAutogradFunctional) ... ok
test_hessian_vectorize_correctness_multi_input (__main__.TestAutogradFunctional) ... ok
test_hessian_vectorize_correctness_simple (__main__.TestAutogradFunctional) ... ok
test_hessian_vectorize_correctness_unrelated_outputs (__main__.TestAutogradFunctional) ... ok
test_hessian_vectorize_raises_no_warnings (__main__.TestAutogradFunctional) ... ok
test_hvp_create_graph (__main__.TestAutogradFunctional) ... ok
test_hvp_err_check (__main__.TestAutogradFunctional) ... ok
test_hvp_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_hvp_no_grad (__main__.TestAutogradFunctional) ... ok
test_hvp_output (__main__.TestAutogradFunctional) ... ok
test_hvp_scalar (__main__.TestAutogradFunctional) ... ok
test_jacobian_create_graph (__main__.TestAutogradFunctional) ... ok
test_jacobian_create_graph_vectorize (__main__.TestAutogradFunctional) ... ok
test_jacobian_err_check (__main__.TestAutogradFunctional) ... ok
test_jacobian_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_jacobian_err_check_strict_vectorize (__main__.TestAutogradFunctional) ... ok
test_jacobian_err_check_vectorize (__main__.TestAutogradFunctional) ... ok
test_jacobian_match_vjp_jvp (__main__.TestAutogradFunctional) ... ok
test_jacobian_no_grad (__main__.TestAutogradFunctional) ... ok
test_jacobian_output (__main__.TestAutogradFunctional) ... ok
test_jacobian_output_vectorize (__main__.TestAutogradFunctional) ... ok
test_jacobian_scalar (__main__.TestAutogradFunctional) ... ok
test_jacobian_scalar_vectorize (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_different_devices (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_different_dtype (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_multi_input (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_multi_input_multi_output (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_simple (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_unrelated_outputs (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_correctness_zero_dim (__main__.TestAutogradFunctional) ... ok
test_jacobian_vectorize_raises_no_warnings (__main__.TestAutogradFunctional) ... ok
test_jvp_create_graph (__main__.TestAutogradFunctional) ... ok
test_jvp_err_check (__main__.TestAutogradFunctional) ... ok
test_jvp_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_jvp_no_grad (__main__.TestAutogradFunctional) ... ok
test_jvp_output (__main__.TestAutogradFunctional) ... ok
test_jvp_scalar (__main__.TestAutogradFunctional) ... ok
test_vhp_create_graph (__main__.TestAutogradFunctional) ... ok
test_vhp_err_check (__main__.TestAutogradFunctional) ... ok
test_vhp_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_vhp_no_grad (__main__.TestAutogradFunctional) ... ok
test_vhp_output (__main__.TestAutogradFunctional) ... ok
test_vhp_scalar (__main__.TestAutogradFunctional) ... ok
test_vjp_create_graph (__main__.TestAutogradFunctional) ... ok
test_vjp_err_check (__main__.TestAutogradFunctional) ... ok
test_vjp_err_check_strict (__main__.TestAutogradFunctional) ... ok
test_vjp_no_grad (__main__.TestAutogradFunctional) ... ok
test_vjp_output (__main__.TestAutogradFunctional) ... ok
test_vjp_scalar (__main__.TestAutogradFunctional) ... ok
test_inference_mode_context_manager (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_decorator (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_existing_autograd_session (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_handle_direct_view_on_rebase (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_handle_indirect_view_on_rebase (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_inf_mode_functional_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_inf_mode_inplace_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_inf_mode_view_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_normal_mode_functional_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_normal_mode_inplace_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_inf_tensor_in_normal_mode_view_op (__main__.TestAutogradInferenceMode) ... ok
test_inference_mode_tensor_creation (__main__.TestAutogradInferenceMode) ... ok
test_mix_inference_and_normal_tensor_functional_op (__main__.TestAutogradInferenceMode) ... ok
test_mix_inference_and_normal_tensor_inplace_op (__main__.TestAutogradInferenceMode) ... ok
test_mix_inference_and_normal_tensor_view_op (__main__.TestAutogradInferenceMode) ... ok
test_normal_tensor_inplace_output_in_inference_mode (__main__.TestAutogradInferenceMode) ... ok
test_normal_tensor_inplace_output_in_normal_mode (__main__.TestAutogradInferenceMode) ... ok
test_normal_tensor_view_output_in_inference_mode (__main__.TestAutogradInferenceMode) ... ok
test_normal_tensor_view_output_in_normal_mode (__main__.TestAutogradInferenceMode) ... ok
test_cat_r_to_c (__main__.TestMultithreadAutograd) ... ok
test_dataparallel_saved_tensors_hooks (__main__.TestMultithreadAutograd) ... ok
test_fork_join_in_middle (__main__.TestMultithreadAutograd) ... ok
test_multithread_saved_tensors_hooks (__main__.TestMultithreadAutograd) ... ok
test_multithreaded_exception_propagation (__main__.TestMultithreadAutograd) ... ok
test_preserve_backtrace (__main__.TestMultithreadAutograd) ... ok
test_python_thread_in_middle (__main__.TestMultithreadAutograd) ... ok
test_simple_backward (__main__.TestMultithreadAutograd) ... ok
test_simple_backward_same_input (__main__.TestMultithreadAutograd) ... ok

----------------------------------------------------------------------
Ran 412 tests in 35.162s

OK (skipped=6, expected failures=1)

Total time based on python measurements:  44.880ms
CPU time measurement python side overhead: 2.37%
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                        torch::autograd::AccumulateGrad        17.65%      24.000us        27.21%      37.000us      18.500us             2  
                                              aten::sum        13.97%      19.000us        15.44%      21.000us      21.000us             1  
                                              aten::add        12.50%      17.000us        12.50%      17.000us      17.000us             1  
                                           SumBackward0         7.35%      10.000us        12.50%      17.000us      17.000us             1  
                                            aten::randn         6.62%       9.000us        16.18%      22.000us      11.000us             2  
                                          aten::normal_         6.62%       9.000us         6.62%       9.000us       4.500us             2  
                                            aten::copy_         5.88%       8.000us         5.88%       8.000us       4.000us             2  
                                        aten::ones_like         3.68%       5.000us         7.35%      10.000us      10.000us             1  
                                           aten::expand         3.68%       5.000us         5.15%       7.000us       7.000us             1  
                                            aten::empty         2.94%       4.000us         2.94%       4.000us       2.000us             2  
                                    aten::empty_strided         2.94%       4.000us         2.94%       4.000us       1.333us             3  
      autograd::engine::evaluate_function: SumBackward0         2.94%       4.000us        15.44%      21.000us      21.000us             1  
autograd::engine::evaluate_function: torch::autograd...         2.94%       4.000us        30.15%      41.000us      20.500us             2  
                                       aten::as_strided         2.21%       3.000us         2.21%       3.000us       1.500us             2  
                                       aten::empty_like         2.21%       3.000us         3.68%       5.000us       5.000us             1  
      autograd::engine::evaluate_function: AddBackward0         2.21%       3.000us         2.94%       4.000us       4.000us             1  
                                aten::new_empty_strided         2.21%       3.000us         3.68%       5.000us       2.500us             2  
                                            aten::fill_         0.74%       1.000us         0.74%       1.000us       0.500us             2  
                                           AddBackward0         0.74%       1.000us         0.74%       1.000us       1.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 136.000us


----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------  
                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                             Input Shapes  
----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------  
          aten::linear         1.71%      14.000us        89.16%     732.000us     732.000us             1              [[128, 20], [30, 20], [30]]  
               aten::t         1.22%      10.000us         2.44%      20.000us      20.000us             1                               [[30, 20]]  
       aten::transpose         0.97%       8.000us         1.22%      10.000us      10.000us             1                       [[30, 20], [], []]  
      aten::as_strided         0.24%       2.000us         0.24%       2.000us       2.000us             1                   [[30, 20], [], [], []]  
           aten::addmm        83.68%     687.000us        85.02%     698.000us     698.000us             1      [[30], [128, 20], [20, 30], [], []]  
          aten::expand         0.37%       3.000us         0.49%       4.000us       4.000us             1                           [[30], [], []]  
      aten::as_strided         0.12%       1.000us         0.12%       1.000us       1.000us             1                       [[30], [], [], []]  
           aten::copy_         0.73%       6.000us         0.73%       6.000us       6.000us             1               [[128, 30], [128, 30], []]  
    aten::resolve_conj         0.12%       1.000us         0.12%       1.000us       1.000us             1                              [[128, 30]]  
    aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1                              [[128, 20]]  
          aten::linear         0.49%       4.000us        10.84%      89.000us      89.000us             1              [[128, 30], [40, 30], [40]]  
               aten::t         0.49%       4.000us         0.97%       8.000us       8.000us             1                               [[40, 30]]  
       aten::transpose         0.37%       3.000us         0.49%       4.000us       4.000us             1                       [[40, 30], [], []]  
      aten::as_strided         0.12%       1.000us         0.12%       1.000us       1.000us             1                   [[40, 30], [], [], []]  
           aten::addmm         8.40%      69.000us         9.38%      77.000us      77.000us             1      [[40], [128, 30], [30, 40], [], []]  
          aten::expand         0.37%       3.000us         0.37%       3.000us       3.000us             1                           [[40], [], []]  
      aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1                       [[40], [], [], []]  
           aten::copy_         0.61%       5.000us         0.61%       5.000us       5.000us             1               [[128, 40], [128, 40], []]  
    aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1                              [[128, 40]]  
    aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1                              [[128, 30]]  
----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------  
Self CPU time total: 821.000us

Running test_binary_ufuncs ... [2021-10-12 09:43:44.104735]
Executing ['/opt/conda/bin/python3.6', 'test_binary_ufuncs.py', '-v'] ... [2021-10-12 09:43:44.104816]
test___add___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___add___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___and___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___eq___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:3113: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  result = getattr(tensor, op)(UnknownType())
ok
test___floordiv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___floordiv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ge___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___gt___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iadd___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___iand___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ifloordiv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ilshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imod___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___imul___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ior___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ipow___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___irshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___isub___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___itruediv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ixor___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___le___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___lt___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___matmul___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mod___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___mul___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ne___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___or___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___pow___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___radd___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rand___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:3113: UserWarning: __rfloordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  result = getattr(tensor, op)(UnknownType())
ok
test___rfloordiv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rfloordiv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rlshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmatmul___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmod___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rmul___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___ror___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rpow___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rrshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rshift___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rsub___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rtruediv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___rxor___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___sub___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___truediv___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test___xor___not_implemented_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_add_broadcast_empty_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_add_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_add_with_tail_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_addcmul_scalars_as_floats_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_addsub_half_tensor_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_atan2_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_atan2_edgecases_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_binary_op_mem_overlap_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_binary_op_scalar_device_unspecified_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_binary_ops_with_scalars_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_and_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_and_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_and_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_and_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_and_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_ops_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_or_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_shift_float_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_bitwise_xor_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_bool_tensor_comparison_ops_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_add_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_div_floor_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_div_no_rounding_mode_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_div_trunc_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_floor_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_methods_invocations.py:648: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)
  return self.op(*args, **kwargs)
ok
test_broadcast_python_scalar_mul_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_sub_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcast_python_scalar_true_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_add_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_div_floor_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_div_no_rounding_mode_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_div_trunc_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_floor_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_mul_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_sub_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_broadcasting_true_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_cdiv_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_cmul_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_comparison_ops_check_for_scalar_overflow_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_comparison_ops_check_for_zerodim_tensor_overflow_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_comparison_ops_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_comparison_ops_must_take_bool_output_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_comparison_ops_type_promotion_and_broadcasting_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:1998: RuntimeWarning: invalid value encountered in less
  lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np),
test_binary_ufuncs.py:1998: RuntimeWarning: invalid value encountered in less_equal
  lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np),
test_binary_ufuncs.py:1998: RuntimeWarning: invalid value encountered in greater
  lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np),
test_binary_ufuncs.py:1998: RuntimeWarning: invalid value encountered in greater_equal
  lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np),
ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bfloat16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bfloat16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bool_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_bool_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_comparison_ops_type_promotion_and_broadcasting_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_complex_scalar_pow_tensor_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_copysign_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_cpow_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_cpu_tensor_pow_cuda_scalar_tensor_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_cremainder_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_cross_device_binary_ops_cuda (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:991: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  op(a, b)
test_binary_ufuncs.py:993: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  op(b, a)
test_binary_ufuncs.py:995: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  op(a, cpu_tensor)
test_binary_ufuncs.py:997: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  op(cpu_tensor, a)
ok
test_cross_device_inplace_error_msg_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_csub_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_cuda_tensor_pow_scalar_tensor_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_cumulative_trapezoid_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_and_floordiv_script_vs_python_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_and_floordiv_vs_python_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_modes_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_nonfinite_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_nonfinite_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_nonfinite_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_nonfinite_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_div_rounding_numpy_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_divide_by_zero_rounding_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:446: RuntimeWarning: divide by zero encountered in true_divide
  expect = np.divide(an, 0)
test_binary_ufuncs.py:446: RuntimeWarning: invalid value encountered in true_divide
  expect = np.divide(an, 0)
ok
test_divide_by_zero_rounding_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_divide_by_zero_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_divide_by_zero_rounding_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_divmul_scalar_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:2830: RuntimeWarning: invalid value encountered in float_power
  expected_scalar_base = torch.from_numpy(np.float_power(i, to_np(exp)))
ok
test_float_power_cuda_bfloat16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex128_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_complex64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_float_power_exceptions_cuda (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:2856: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.float_power(base, exp, out=out)
ok
test_float_scalar_pow_float_tensor_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:732: RuntimeWarning: invalid value encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
test_binary_ufuncs.py:732: RuntimeWarning: divide by zero encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
ok
test_float_scalar_pow_float_tensor_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_out_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_out_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_scalar_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_tensor_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_floor_divide_zero_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_floor_divide_zero_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_floor_divide_zero_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_floor_divide_zero_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_floor_divide_zero_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_fmod_remainder_by_zero_float_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_by_zero_float_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_by_zero_float_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_by_zero_integral_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fmod_remainder_by_zero_integral_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fmod_remainder_by_zero_integral_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fmod_remainder_by_zero_integral_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fmod_remainder_by_zero_integral_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fmod_remainder_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_fmod_remainder_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_gcd_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_gcd_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_gcd_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_gcd_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_gcd_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_complex_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_complex_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_complex_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_complex_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cross_device_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_heaviside_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_hypot_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_hypot_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_idiv_and_ifloordiv_vs_python_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_inplace_comparison_ops_require_inputs_have_same_dtype_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_inplace_division_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_inplace_dunders_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_int_and_float_pow_cuda (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:732: RuntimeWarning: invalid value encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
ok
test_int_tensor_pow_neg_ints_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_lcm_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_lcm_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_lcm_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_ldexp_cuda (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_lerp_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:2341: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.lerp(start, end, weight, out=actual_out)
test_binary_ufuncs.py:2341: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [5, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.lerp(start, end, weight, out=actual_out)
ok
test_lerp_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_lerp_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_lerp_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logaddexp2_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logaddexp2_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logaddexp_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logaddexp_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:2256: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Copy.cpp:244.)
  self.assertEqual(expected_res.bool(), getattr(a, op)(b))
ok
test_logical_and_cuda_complex128_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex128_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_complex64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_and_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex128_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_complex64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_or_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex128_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_complex64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_logical_xor_with_nontrivial_alignment_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_long_tensor_pow_floats_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex128_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_complex_cuda_complex64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_cross_device_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_nan_and_inf_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_nan_and_inf_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_nan_and_inf_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_float_nan_and_inf_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_int_and_bool_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bfloat16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_maximum_minimum_type_promotion_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_min_max_binary_op_nan_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_min_max_binary_op_nan_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_min_max_binary_op_nan_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_mul_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_mul_intertype_scalar_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_mul_intertype_scalar_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_mul_intertype_scalar_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_muldiv_scalar_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_nextafter_bfloat16_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_nextafter_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_nextafter_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_add_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_div_floor_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_div_no_rounding_mode_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_div_trunc_rounding_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_floor_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_mul_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_sub_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_not_broadcastable_true_divide_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_out_resize_warning_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_complex_extremal_failing_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_complex_extremal_failing_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_inplace_resizing_exception_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_scalar_overloads_mem_overlap_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_pow_scalar_type_promotion_cuda (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:917: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  out_uint8_computation = torch.pow(2, input_tensor_uint8, out=torch.tensor(0, dtype=torch.int64, device=device))
test_binary_ufuncs.py:921: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  out_int64_computation = torch.pow(2, input_tensor_int64, out=torch.tensor(0, dtype=torch.int64, device=device))
ok
test_rdiv_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_rdiv_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_remainder_fmod_large_dividend_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_remainder_fmod_large_dividend_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_remainder_overflow_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_rpow_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_signed_shift_cuda_int16 (__main__.TestBinaryUfuncsCUDA)
Ensure that signed integer bit shifting works as expected. ... ok
test_signed_shift_cuda_int32 (__main__.TestBinaryUfuncsCUDA)
Ensure that signed integer bit shifting works as expected. ... ok
test_signed_shift_cuda_int64 (__main__.TestBinaryUfuncsCUDA)
Ensure that signed integer bit shifting works as expected. ... ok
test_signed_shift_cuda_int8 (__main__.TestBinaryUfuncsCUDA)
Ensure that signed integer bit shifting works as expected. ... ok
test_sub_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_bool (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_complex128 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_complex64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_float16 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_float64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_int16 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_int32 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_int64 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_int8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_cuda_uint8 (__main__.TestBinaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_sub_typing_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_tensor_pow_tensor_cuda (__main__.TestBinaryUfuncsCUDA) ... test_binary_ufuncs.py:732: RuntimeWarning: invalid value encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
test_binary_ufuncs.py:732: RuntimeWarning: divide by zero encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
ok
test_trapezoid_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_true_divide_out_cuda_bfloat16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_true_divide_out_cuda_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_bfloat16_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_float16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_xlogy_xlog1py_scalar_type_promotion_cuda (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_bool_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_float64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int16_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int32_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int64_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_int8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_bool (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_float32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_float64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_int16 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_int32 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_int64 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_int8 (__main__.TestBinaryUfuncsCUDA) ... ok
test_zeta_cuda_uint8_uint8 (__main__.TestBinaryUfuncsCUDA) ... ok

----------------------------------------------------------------------
Ran 1734 tests in 283.232s

OK (skipped=42)
Running test_buffer_protocol ... [2021-10-12 09:48:31.149015]
Executing ['/opt/conda/bin/python3.6', 'test_buffer_protocol.py', '-v'] ... [2021-10-12 09:48:31.149099]

----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK
Running test_bundled_inputs ... [2021-10-12 09:48:34.416828]
Executing ['/opt/conda/bin/python3.6', 'test_bundled_inputs.py', '-v'] ... [2021-10-12 09:48:34.416905]
test_bad_inputs (__main__.TestBundledInputs) ... ok
test_dict_args (__main__.TestBundledInputs) ... ok
test_double_augment_fail (__main__.TestBundledInputs) ... ok
test_double_augment_non_mutator (__main__.TestBundledInputs) ... ok
test_double_augment_success (__main__.TestBundledInputs) ... ok
test_large_tensor_with_inflation (__main__.TestBundledInputs) ... ok
test_multiple_methods_with_inputs (__main__.TestBundledInputs) ... ok
test_multiple_methods_with_inputs_both_defined_failure (__main__.TestBundledInputs) ... ok
test_multiple_methods_with_inputs_neither_defined_failure (__main__.TestBundledInputs) ... ok
test_non_tensors (__main__.TestBundledInputs) ... ok
test_rejected_tensors (__main__.TestBundledInputs) ... ok
test_single_tensors (__main__.TestBundledInputs) ... ok

----------------------------------------------------------------------
Ran 12 tests in 0.763s

OK
Running test_complex ... [2021-10-12 09:48:37.489350]
Executing ['/opt/conda/bin/python3.6', 'test_complex.py', '-v'] ... [2021-10-12 09:48:37.489407]
test_dtype_inference_cuda_float32 (__main__.TestComplexTensorCUDA) ... ok
test_dtype_inference_cuda_float64 (__main__.TestComplexTensorCUDA) ... ok
test_to_list_cuda_complex128 (__main__.TestComplexTensorCUDA) ... ok
test_to_list_cuda_complex64 (__main__.TestComplexTensorCUDA) ... ok

----------------------------------------------------------------------
Ran 4 tests in 4.299s

OK
Running test_cpp_api_parity ... [2021-10-12 09:48:45.140986]
Executing ['/opt/conda/bin/python3.6', 'test_cpp_api_parity.py', '-v'] ... [2021-10-12 09:48:45.141072]
test_torch_nn_AdaptiveAvgPool1d (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool1d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool1d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool1d_one_output (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool1d_one_output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_single (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_single_1x1output (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_single_1x1output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_single_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_tuple (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_tuple_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_tuple_none (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_last_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_last_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_single (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_single_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_tuple (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_tuple_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_tuple_none (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool1d (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool1d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool1d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_single (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_single_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_tuple (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_tuple_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_tuple_none (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool2d_tuple_none_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_single (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_single_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_single_nonatomic (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_single_nonatomic_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple_nonatomic (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple_nonatomic_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple_none (__main__.TestCppApiParity) ... ok
test_torch_nn_AdaptiveMaxPool3d_tuple_none_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_stride_pad (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool1d_stride_pad_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor_stride_pad (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_divisor_stride_pad_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_stride_pad (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool2d_stride_pad_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride1_pad0_gpu_input (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride1_pad0_gpu_input_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_general_output (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_general_output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride1_pad0_gpu_input (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride1_pad0_gpu_input_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_fixedkw_output (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_fixedkw_output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_general_output (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_general_output_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_input_nooverlap (__main__.TestCppApiParity) ... ok
test_torch_nn_AvgPool3d_stride_pad_gpu_input_nooverlap_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss_scalar_weights (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss_scalar_weights_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss_weights (__main__.TestCppApiParity) ... ok
test_torch_nn_BCELoss_weights_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss_scalar_weights (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss_scalar_weights_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss_weights (__main__.TestCppApiParity) ... ok
test_torch_nn_BCEWithLogitsLoss_weights_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_3d_input (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_3d_input_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_3d_input_not_affine (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_3d_input_not_affine_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_affine (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_affine_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_affine_simple_average (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_affine_simple_average_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_not_affine (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_not_affine_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_not_tracking_stats (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_not_tracking_stats_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm1d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_2d_simple_average (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_2d_simple_average_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_momentum (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_momentum_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_not_affine (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_not_affine_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_not_tracking_stats (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_not_tracking_stats_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm2d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_3d_simple_average (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_3d_simple_average_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_momentum (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_momentum_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_not_affine (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_not_affine_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_not_tracking_stats (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_not_tracking_stats_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_BatchNorm3d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CELU (__main__.TestCppApiParity) ... ok
test_torch_nn_CELU_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CELU_scalar (__main__.TestCppApiParity) ... ok
test_torch_nn_CELU_scalar_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_2d_int_target_lengths_tensors (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_2d_int_target_lengths_tensors_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_2d_lengths_tensors (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_2d_lengths_tensors_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_lengths_tensors (__main__.TestCppApiParity) ... ok
test_torch_nn_CTCLoss_lengths_tensors_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d_complex (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d_complex_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d_complex (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d_complex_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad2d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d_complex (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d_complex_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d_no_batch_dim (__main__.TestCppApiParity) ... ok
test_torch_nn_ConstantPad3d_no_batch_dim_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_circular_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_circular_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_groups (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_groups_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad1 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad1_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad1size1 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad1size1_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad2size1 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad2size1_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_same (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_same2 (__main__.TestCppApiParity) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Convolution.cpp:647.)
  self.padding, self.dilation, self.groups)
ok
test_torch_nn_Conv1d_pad_same2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_same_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_same_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_same_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_valid (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_pad_valid_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_reflect_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_reflect_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_replicate_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_replicate_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_zeros_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv1d_zeros_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_circular_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_circular_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_padded (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_padded_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_strided (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_strided_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_with_multiplier (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_depthwise_with_multiplier_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_groups (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_groups_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_groups_thnn (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_groups_thnn_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_no_bias (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_no_bias_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_same (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_same_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_same_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_same_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_valid (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_pad_valid_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_padding (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_padding_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_reflect_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_reflect_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_replicate_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_replicate_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_strided (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_strided_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_zeros_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv2d_zeros_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_1x1x1_no_bias (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_1x1x1_no_bias_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_circular_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_circular_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_dilated_strided (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_dilated_strided_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_groups (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_groups_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_no_bias (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_no_bias_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_same (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_same_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_same_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_same_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_valid (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_pad_valid_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_replicate_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_replicate_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_stride (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_stride_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_stride_padding (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_stride_padding_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_zero_batch (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_zero_batch_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_zeros_stride2_pad2 (__main__.TestCppApiParity) ... ok
test_torch_nn_Conv3d_zeros_stride2_pad2_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_groups (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_groups_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_no_bias (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose1d_no_bias_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_groups (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_groups_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_no_bias (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose2d_no_bias_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose3d (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose3d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose3d_dilated (__main__.TestCppApiParity) ... ok
test_torch_nn_ConvTranspose3d_dilated_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CosineEmbeddingLoss (__main__.TestCppApiParity) ... ok
test_torch_nn_CosineEmbeddingLoss_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CosineEmbeddingLoss_margin (__main__.TestCppApiParity) ... ok
test_torch_nn_CosineEmbeddingLoss_margin_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_ignore_index (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_ignore_index_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_ignore_index (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_ignore_index_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_sum_reduction (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_sum_reduction_cuda (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_weight (__main__.TestCppApiParity) ... ok
test_torch_nn_CrossEntropyLoss_2d_indices_target_smoothing_weight_cuda (__main__.TestCppApiParity) ... "Cannot find Symbol"
test_cpp_api_parity failed! Received signal: SIGIOT
Running test_cpp_extensions_aot_ninja ... [2021-10-12 09:49:51.861194]
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp -> None ignored
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
/var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 5
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp -> None ignored
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
/var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 3
running install
running build
running build_py
creating build
creating build/lib.linux-x86_64-3.6
creating build/lib.linux-x86_64-3.6/torch_test_cpp_extension
copying torch_test_cpp_extension/__init__.py -> build/lib.linux-x86_64-3.6/torch_test_cpp_extension
running build_ext
building 'torch_test_cpp_extension.cpp' extension
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
[1/1] c++ -MMD -MF /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/extension.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cpp.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.ort' extension
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[1/1] c++ -MMD -MF /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/ort_extension.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/ort_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=ort -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/ort_extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/ort.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.rng' extension
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[1/1] c++ -MMD -MF /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/rng_extension.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/rng_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=rng -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
In file included from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec256/vec256.h:8:0,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec.h:4,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/native/cpu/Loops.h:35,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/native/cpu/DistributionTemplates.h:7,
                 from /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp:6:
/opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec_base.h:947:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 # pragma unroll
 
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/rng_extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/rng.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.cuda' extension
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test
creating /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[1/3] c++ -MMD -MF /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
[2/3] /opt/rocm/bin/hipcc  -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
[3/3] /opt/rocm/bin/hipcc  -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -L/opt/rocm/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cuda.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.torch_library' extension
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[1/1] /opt/rocm/bin/hipcc  -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -I/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -o /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=torch_library -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -L/opt/rocm/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/torch_library.cpython-36m-x86_64-linux-gnu.so
running install_lib
creating install
creating install/opt
creating install/opt/conda
creating install/opt/conda/lib
creating install/opt/conda/lib/python3.6
creating install/opt/conda/lib/python3.6/site-packages
creating install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/ort.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cpp.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/rng.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/torch_library.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cuda.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/__init__.py -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
byte-compiling ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension/__init__.py to __init__.cpython-36.pyc
running install_egg_info
running egg_info
creating torch_test_cpp_extension.egg-info
writing torch_test_cpp_extension.egg-info/PKG-INFO
writing dependency_links to torch_test_cpp_extension.egg-info/dependency_links.txt
writing top-level names to torch_test_cpp_extension.egg-info/top_level.txt
writing manifest file 'torch_test_cpp_extension.egg-info/SOURCES.txt'
reading manifest file 'torch_test_cpp_extension.egg-info/SOURCES.txt'
writing manifest file 'torch_test_cpp_extension.egg-info/SOURCES.txt'
Copying torch_test_cpp_extension.egg-info to ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension-0.0.0-py3.6.egg-info
running install_scripts
running install
running build
running build_ext
building 'no_python_abi_suffix_test' extension
creating /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build
creating /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
[1/1] c++ -MMD -MF /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/no_python_abi_suffix_test.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c -c /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp -o /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/no_python_abi_suffix_test.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=no_python_abi_suffix_test -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
creating build/lib.linux-x86_64-3.6
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/no_python_abi_suffix_test.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/no_python_abi_suffix_test.so
running install_lib
creating install
creating install/opt
creating install/opt/conda
creating install/opt/conda/lib
creating install/opt/conda/lib/python3.6
creating install/opt/conda/lib/python3.6/site-packages
copying build/lib.linux-x86_64-3.6/no_python_abi_suffix_test.so -> ./install/opt/conda/lib/python3.6/site-packages
running install_egg_info
running egg_info
creating no_python_abi_suffix_test.egg-info
writing no_python_abi_suffix_test.egg-info/PKG-INFO
writing dependency_links to no_python_abi_suffix_test.egg-info/dependency_links.txt
writing top-level names to no_python_abi_suffix_test.egg-info/top_level.txt
writing manifest file 'no_python_abi_suffix_test.egg-info/SOURCES.txt'
reading manifest file 'no_python_abi_suffix_test.egg-info/SOURCES.txt'
writing manifest file 'no_python_abi_suffix_test.egg-info/SOURCES.txt'
Copying no_python_abi_suffix_test.egg-info to ./install/opt/conda/lib/python3.6/site-packages/no_python_abi_suffix_test-0.0.0-py3.6.egg-info
running install_scripts
Executing ['/opt/conda/bin/python3.6', 'test_cpp_extensions_aot_ninja.py', '-v'] ... [2021-10-12 09:53:10.253400]
test_backward (__main__.TestCppExtensionAOT) ... ok
test_cuda_extension (__main__.TestCppExtensionAOT) ... ok
test_extension_function (__main__.TestCppExtensionAOT) ... ok
test_extension_module (__main__.TestCppExtensionAOT) ... ok
test_no_python_abi_suffix_sets_the_correct_library_name (__main__.TestCppExtensionAOT) ... ok
test_optional (__main__.TestCppExtensionAOT) ... ok
test_add (__main__.TestORTTensor) ... ok
test_conv_backend_override (__main__.TestORTTensor) ... ok
test_unregistered (__main__.TestORTTensor) ... ok
test_zeros (__main__.TestORTTensor) ... ok
test_rng (__main__.TestRNGExtension) ... ok
test_torch_library (__main__.TestTorchLibrary) ... ok

----------------------------------------------------------------------
Ran 12 tests in 0.040s

OK
Running test_cpp_extensions_aot_no_ninja ... [2021-10-12 09:53:14.185025]
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp -> None ignored
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
/var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 5
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp -> None ignored
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
/var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/rng_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/ort_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel2.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/doubler.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cudnn_extension_hip.cpp skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension_kernel.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip skipped
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_c10d_extension.hpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h -> /var/lib/jenkins/pytorch/test/cpp_extensions/self_compiler_include_dirs_test/tmp.h ok
/var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/no_python_abi_suffix_test.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu ok
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 3
running install
running build
running build_py
creating build
creating build/lib.linux-x86_64-3.6
creating build/lib.linux-x86_64-3.6/torch_test_cpp_extension
copying torch_test_cpp_extension/__init__.py -> build/lib.linux-x86_64-3.6/torch_test_cpp_extension
running build_ext
building 'torch_test_cpp_extension.cpp' extension
creating build/temp.linux-x86_64-3.6
gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c extension.cpp -o build/temp.linux-x86_64-3.6/extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cpp.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.ort' extension
gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c ort_extension.cpp -o build/temp.linux-x86_64-3.6/ort_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=ort -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/ort_extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/ort.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.rng' extension
gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c rng_extension.cpp -o build/temp.linux-x86_64-3.6/rng_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=rng -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
In file included from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec256/vec256.h:8:0,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec.h:4,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/native/cpu/Loops.h:35,
                 from /opt/conda/lib/python3.6/site-packages/torch/include/ATen/native/cpu/DistributionTemplates.h:7,
                 from rng_extension.cpp:6:
/opt/conda/lib/python3.6/site-packages/torch/include/ATen/cpu/vec/vec_base.h:947:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 # pragma unroll
 
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/rng_extension.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/rng.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.cuda' extension
creating build/temp.linux-x86_64-3.6/var
creating build/temp.linux-x86_64-3.6/var/lib
creating build/temp.linux-x86_64-3.6/var/lib/jenkins
creating build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch
creating build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test
creating build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions
gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.o -fPIC -D__HIP_PLATFORM_HCC__=1 -g -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++14
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
/opt/rocm/bin/hipcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.hip -o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
/opt/rocm/bin/hipcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.hip -o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=cuda -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel2.o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension_kernel.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -L/opt/rocm/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cuda.cpython-36m-x86_64-linux-gnu.so
building 'torch_test_cpp_extension.torch_library' extension
/opt/rocm/bin/hipcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/lib/python3.6/site-packages/torch/include/THH -I/opt/rocm/include -I/opt/rocm/miopen/include -Iself_compiler_include_dirs_test -I/opt/conda/include/python3.6m -c /var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.cu -o build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.o -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=torch_library -D_GLIBCXX_USE_CXX11_ABI=1 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -std=c++14
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/var/lib/jenkins/pytorch/test/cpp_extensions/torch_library.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -L/opt/rocm/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-3.6/torch_test_cpp_extension/torch_library.cpython-36m-x86_64-linux-gnu.so
running install_lib
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/ort.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cpp.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/rng.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/torch_library.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
copying build/lib.linux-x86_64-3.6/torch_test_cpp_extension/cuda.cpython-36m-x86_64-linux-gnu.so -> ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension
running install_egg_info
running egg_info
writing torch_test_cpp_extension.egg-info/PKG-INFO
writing dependency_links to torch_test_cpp_extension.egg-info/dependency_links.txt
writing top-level names to torch_test_cpp_extension.egg-info/top_level.txt
reading manifest file 'torch_test_cpp_extension.egg-info/SOURCES.txt'
writing manifest file 'torch_test_cpp_extension.egg-info/SOURCES.txt'
removing './install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension-0.0.0-py3.6.egg-info' (and everything under it)
Copying torch_test_cpp_extension.egg-info to ./install/opt/conda/lib/python3.6/site-packages/torch_test_cpp_extension-0.0.0-py3.6.egg-info
running install_scripts
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
running install
running build
running build_ext
building 'no_python_abi_suffix_test' extension
Emitting ninja build file /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (8) as the number of workers...
ninja: no work to do.
g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /var/lib/jenkins/pytorch/test/cpp_extensions/no_python_abi_suffix_test/build/temp.linux-x86_64-3.6/no_python_abi_suffix_test.o -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/no_python_abi_suffix_test.so
running install_lib
copying build/lib.linux-x86_64-3.6/no_python_abi_suffix_test.so -> ./install/opt/conda/lib/python3.6/site-packages
running install_egg_info
running egg_info
writing no_python_abi_suffix_test.egg-info/PKG-INFO
writing dependency_links to no_python_abi_suffix_test.egg-info/dependency_links.txt
writing top-level names to no_python_abi_suffix_test.egg-info/top_level.txt
reading manifest file 'no_python_abi_suffix_test.egg-info/SOURCES.txt'
writing manifest file 'no_python_abi_suffix_test.egg-info/SOURCES.txt'
removing './install/opt/conda/lib/python3.6/site-packages/no_python_abi_suffix_test-0.0.0-py3.6.egg-info' (and everything under it)
Copying no_python_abi_suffix_test.egg-info to ./install/opt/conda/lib/python3.6/site-packages/no_python_abi_suffix_test-0.0.0-py3.6.egg-info
running install_scripts
Executing ['/opt/conda/bin/python3.6', 'test_cpp_extensions_aot_no_ninja.py', '-v'] ... [2021-10-12 09:57:27.080731]
test_backward (__main__.TestCppExtensionAOT) ... ok
test_cuda_extension (__main__.TestCppExtensionAOT) ... ok
test_extension_function (__main__.TestCppExtensionAOT) ... ok
test_extension_module (__main__.TestCppExtensionAOT) ... ok
test_no_python_abi_suffix_sets_the_correct_library_name (__main__.TestCppExtensionAOT) ... ok
test_optional (__main__.TestCppExtensionAOT) ... ok
test_add (__main__.TestORTTensor) ... ok
test_conv_backend_override (__main__.TestORTTensor) ... ok
test_unregistered (__main__.TestORTTensor) ... ok
test_zeros (__main__.TestORTTensor) ... ok
test_rng (__main__.TestRNGExtension) ... ok
test_torch_library (__main__.TestTorchLibrary) ... ok

----------------------------------------------------------------------
Ran 12 tests in 0.040s

OK
Running test_cpp_extensions_jit ... [2021-10-12 09:57:31.004978]
Executing ['/opt/conda/bin/python3.6', 'test_cpp_extensions_jit.py', '-v'] ... [2021-10-12 09:57:31.005050]
test_autograd_from_cpp (__main__.TestCppExtensionJIT) ... ok
test_compilation_error_formatting (__main__.TestCppExtensionJIT) ... ok
test_cpp_frontend_module_has_same_output_as_python (__main__.TestCppExtensionJIT) ... Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/cpp_frontend_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/cpp_frontend_extension/build.ninja...
Building extension module cpp_frontend_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF cpp_frontend_extension.o.d -DTORCH_EXTENSION_NAME=cpp_frontend_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /var/lib/jenkins/pytorch/test/cpp_extensions/cpp_frontend_extension.cpp -o cpp_frontend_extension.o 
[2/2] c++ cpp_frontend_extension.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpp_frontend_extension.so
ok
test_cpp_frontend_module_has_up_to_date_attributes (__main__.TestCppExtensionJIT) ... ok
test_cpp_frontend_module_python_inter_op (__main__.TestCppExtensionJIT) ... ok
test_cpp_frontend_module_python_inter_op_with_cuda (__main__.TestCppExtensionJIT) ... ok
test_crash_handler (__main__.TestCppExtensionJIT) ... Loading extension module cpp_frontend_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module cpp_frontend_extension, skipping build step...
Loading extension module cpp_frontend_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module cpp_frontend_extension, skipping build step...
Loading extension module cpp_frontend_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module cpp_frontend_extension, skipping build step...
Loading extension module cpp_frontend_extension...
ok
test_custom_compound_op_autograd (__main__.TestCppExtensionJIT) ... Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/is_python_module...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/is_python_module/build.ninja...
Building extension module is_python_module...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=is_python_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/is_python_module/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o is_python_module.so
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
/opt/conda/lib/python3.6/site-packages/torch/autograd/gradcheck.py:648: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  f'Input #{idx} requires gradient and '
ok
test_half_support (__main__.TestCppExtensionJIT) ... skipped 'Temporarily disabled'
test_inline_jit_compile_custom_op_cuda (__main__.TestCppExtensionJIT) ... skipped 'Temporarily disabled'
test_inline_jit_compile_extension_cuda (__main__.TestCppExtensionJIT) ... skipped 'Temporarily disabled'
test_inline_jit_compile_extension_multiple_sources_and_no_functions (__main__.TestCppExtensionJIT) ... Loading extension module is_python_module...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/inline_jit_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/inline_jit_extension/build.ninja...
Building extension module inline_jit_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_jit_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/inline_jit_extension/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o inline_jit_extension.so
ok
test_inline_jit_compile_extension_throws_when_functions_is_bad (__main__.TestCppExtensionJIT) ... ok
test_inline_jit_compile_extension_with_functions_as_dict (__main__.TestCppExtensionJIT) ... Loading extension module inline_jit_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_dict...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_dict/build.ninja...
Building extension module inline_jit_extension_with_functions_dict...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_jit_extension_with_functions_dict -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_dict/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o inline_jit_extension_with_functions_dict.so
ok
test_inline_jit_compile_extension_with_functions_as_list (__main__.TestCppExtensionJIT) ... Loading extension module inline_jit_extension_with_functions_dict...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_list...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_list/build.ninja...
Building extension module inline_jit_extension_with_functions_list...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_jit_extension_with_functions_list -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/inline_jit_extension_with_functions_list/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o inline_jit_extension_with_functions_list.so
ok
test_jit_compile_extension (__main__.TestCppExtensionJIT) ... Loading extension module inline_jit_extension_with_functions_list...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/jit_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/jit_extension/build.ninja...
Building extension module jit_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/3] c++ -MMD -MF jit_extension2.o.d -DTORCH_EXTENSION_NAME=jit_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/var/lib/jenkins/pytorch/test/cpp_extensions -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -g -c /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension2.cpp -o jit_extension2.o 
[2/3] c++ -MMD -MF jit_extension.o.d -DTORCH_EXTENSION_NAME=jit_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/var/lib/jenkins/pytorch/test/cpp_extensions -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -g -c /var/lib/jenkins/pytorch/test/cpp_extensions/jit_extension.cpp -o jit_extension.o 
[3/3] c++ jit_extension.o jit_extension2.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o jit_extension.so
ok
test_jit_cuda_archflags (__main__.TestCppExtensionJIT) ... skipped 'CUDA not found'
test_jit_cuda_extension (__main__.TestCppExtensionJIT) ... Loading extension module jit_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/torch_test_cuda_extension...
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -> /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp ok
/var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cu -> /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip skipped
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 1
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/torch_test_cuda_extension/build.ninja...
Building extension module torch_test_cuda_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[92mSuccessfully preprocessed all matching files.[0m
[1/3] c++ -MMD -MF cuda_extension.o.d -DTORCH_EXTENSION_NAME=torch_test_cuda_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THH -isystem /opt/rocm/include -isystem /opt/rocm/miopen/include -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /var/lib/jenkins/pytorch/test/cpp_extensions/cuda_extension.cpp -o cuda_extension.o 
[2/3] /opt/rocm/bin/hipcc  -DWITH_HIP -DTORCH_EXTENSION_NAME=torch_test_cuda_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THH -isystem /opt/rocm/include -isystem /opt/rocm/miopen/include -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -fPIC -D__HIP_PLATFORM_HCC__=1 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -O2 --amdgpu-target=gfx900 --amdgpu-target=gfx906 --amdgpu-target=gfx908 --amdgpu-target=gfx90a --amdgpu-target=gfx1030 -fno-gpu-rdc -c /var/lib/jenkins/pytorch/test/cpp_extensions/hip_extension.hip -o hip_extension.cuda.o 
[3/3] c++ cuda_extension.o hip_extension.cuda.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -lc10_hip -ltorch_cpu -ltorch_hip -ltorch -ltorch_python -L/opt/rocm/lib -lamdhip64 -o torch_test_cuda_extension.so
ok
test_jit_cudnn_extension (__main__.TestCppExtensionJIT) ... skipped 'CuDNN not found'
test_lenient_flag_handling_in_jit_extensions (__main__.TestCppExtensionJIT) ... Loading extension module torch_test_cuda_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/lenient_flag_handling_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/lenient_flag_handling_extension/build.ninja...
Building extension module lenient_flag_handling_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=lenient_flag_handling_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/var/lib/jenkins/pytorch/test/cpp_extensions -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -g -O0 -Wall -c /root/.cache/torch_extensions/py36_cpu/lenient_flag_handling_extension/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o lenient_flag_handling_extension.so
ok
test_reload_jit_extension (__main__.TestCppExtensionJIT) ... Loading extension module lenient_flag_handling_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/build.ninja...
Building extension module reloaded_jit_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=reloaded_jit_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o reloaded_jit_extension.so
Loading extension module reloaded_jit_extension...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
The input conditions for extension module reloaded_jit_extension have changed. Bumping to version 1 and re-building as reloaded_jit_extension_v1...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/build.ninja...
Building extension module reloaded_jit_extension_v1...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=reloaded_jit_extension_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o reloaded_jit_extension_v1.so
Loading extension module reloaded_jit_extension_v1...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module reloaded_jit_extension_v1, skipping build step...
Loading extension module reloaded_jit_extension_v1...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
The input conditions for extension module reloaded_jit_extension have changed. Bumping to version 2 and re-building as reloaded_jit_extension_v2...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/build.ninja...
Building extension module reloaded_jit_extension_v2...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=reloaded_jit_extension_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/reloaded_jit_extension/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o reloaded_jit_extension_v2.so
ok
test_returns_shared_library_path_when_is_python_module_is_true (__main__.TestCppExtensionJIT) ... Loading extension module reloaded_jit_extension_v2...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
The input conditions for extension module is_python_module have changed. Bumping to version 1 and re-building as is_python_module_v1...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/is_python_module/build.ninja...
Building extension module is_python_module_v1...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=is_python_module_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/is_python_module/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o is_python_module_v1.so
ok
test_set_default_type_also_changes_aten_default_type (__main__.TestCppExtensionJIT) ... Loading extension module is_python_module_v1...
Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/test_set_default_type...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/test_set_default_type/build.ninja...
Building extension module test_set_default_type...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test_set_default_type -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /root/.cache/torch_extensions/py36_cpu/test_set_default_type/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o test_set_default_type.so
ok
test_warning (__main__.TestCppExtensionJIT) ... Loading extension module test_set_default_type...
[W main.cpp:12] Warning: Error with CPUDoubleType (function foo)
[W main.cpp:12] Warning: Error with CPUDoubleType (function foo)
[W main.cpp:12] Warning: Error with CPUDoubleType (function foo)
[W main.cpp:12] Warning: Error with CPUDoubleType (function foo)
UserWarning: Error with torch.DoubleTensor (Triggered internally at  /root/.cache/torch_extensions/py36_cpu/warn_mod/main.cpp:12.)
ok

----------------------------------------------------------------------
Ran 24 tests in 231.087s

OK (skipped=5)
Running test_cuda ... [2021-10-12 10:01:25.255134]
Executing ['/opt/conda/bin/python3.6', 'test_cuda.py', '-v'] ... [2021-10-12 10:01:25.255218]
test_arithmetic_large_tensor (__main__.TestCuda) ... skipped 'was disabled due to not enough memory, but actually it always fail'
test_autocast_banned (__main__.TestCuda) ... ok
test_autocast_cache_leak (__main__.TestCuda) ... ok
test_autocast_cat_jit (__main__.TestCuda) ... ok
test_autocast_checkpointing (__main__.TestCuda) ... ok
test_autocast_custom_cast_inputs (__main__.TestCuda) ... ok
test_autocast_custom_enabled (__main__.TestCuda) ... ok
test_autocast_ignored_types (__main__.TestCuda) ... ok
test_autocast_linalg_fp16 (__main__.TestCuda) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_autocast_methods_expect_builtin_promote (__main__.TestCuda) ... ok
test_autocast_methods_fp16 (__main__.TestCuda) ... ok
test_autocast_methods_fp32 (__main__.TestCuda) ... ok
test_autocast_nn_bf16 (__main__.TestCuda) ... ok
test_autocast_nn_fp16 (__main__.TestCuda) ... ok
test_autocast_nn_fp32 (__main__.TestCuda) ... test_cuda.py:2725: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  output = getattr(module, op)(*args, **add_kwargs)
test_cuda.py:2766: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  control = getattr(module, op)(*cast(args, run_as_type), **add_kwargs)
ok
test_autocast_rnn (__main__.TestCuda) ... skipped "test doesn't currently work on the ROCm stack"
test_autocast_torch_bf16 (__main__.TestCuda) ... ok
test_autocast_torch_expect_builtin_promote (__main__.TestCuda) ... ok
test_autocast_torch_fp16 (__main__.TestCuda) ... /opt/conda/lib/python3.6/site-packages/torch/functional.py:1497: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:732.)
  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]
ok
test_autocast_torch_fp32 (__main__.TestCuda) ... ok
test_autocast_torch_need_autocast_promote (__main__.TestCuda) ... "Cannot find Symbol"
test_cuda failed! Received signal: SIGIOT
Running test_cuda_primary_ctx ... [2021-10-12 10:01:46.239055]
Executing ['/opt/conda/bin/python3.6', 'test_cuda_primary_ctx.py', '-v', '--subprocess'] ... [2021-10-12 10:01:46.239127]
test_copy (__main__.TestCudaPrimaryCtx) ... skipped "test doesn't currently work on the ROCm stack"

----------------------------------------------------------------------
Ran 1 test in 0.000s

OK (skipped=1)
test_pin_memory (__main__.TestCudaPrimaryCtx) ... skipped "test doesn't currently work on the ROCm stack"

----------------------------------------------------------------------
Ran 1 test in 0.000s

OK (skipped=1)
test_str_repr (__main__.TestCudaPrimaryCtx) ... skipped "test doesn't currently work on the ROCm stack"

----------------------------------------------------------------------
Ran 1 test in 0.000s

OK (skipped=1)
Running test_dataloader ... [2021-10-12 10:01:56.303665]
Executing ['/opt/conda/bin/python3.6', 'test_dataloader.py', '-v'] ... [2021-10-12 10:01:56.303746]
test_add_dataset (__main__.TestConcatDataset) ... ok
test_concat_raises_index_error (__main__.TestConcatDataset) ... ok
test_concat_two_non_singletons (__main__.TestConcatDataset) ... ok
test_concat_two_non_singletons_with_empty (__main__.TestConcatDataset) ... ok
test_concat_two_singletons (__main__.TestConcatDataset) ... ok
test_iterable_dataset_err (__main__.TestConcatDataset) ... ok
test_conv_after_fork (__main__.TestConvAfterFork) ... ok
test_custom_batch_pin (__main__.TestCustomPinFn) ... ok
test_custom_batch_pin_worker (__main__.TestCustomPinFn) ... ok
test_batch_sampler (__main__.TestDataLoader) ... ok
test_builtin_collection_conversion (__main__.TestDataLoader) ... ok
test_bulk_loading_nobatch (__main__.TestDataLoader) ... ok
test_chain_iterable_style_dataset (__main__.TestDataLoader) ... ok
test_default_collate_bad_numpy_types (__main__.TestDataLoader) ... ok
test_default_collate_bad_sequence_type (__main__.TestDataLoader) ... ok
test_default_collate_dtype (__main__.TestDataLoader) ... ok
test_default_collate_numpy_memmap (__main__.TestDataLoader) ... /opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/utils/tensor_numpy.cpp:187.)
  return default_collate([torch.as_tensor(b) for b in batch])
ok
test_default_collate_shared_tensor (__main__.TestDataLoader) ... ok
test_distributed_sampler_invalid_rank (__main__.TestDataLoader) ... ok
test_duplicating_data_with_drop_last (__main__.TestDataLoader) ... ok
test_error (__main__.TestDataLoader) ... ok
test_error_in_init (__main__.TestDataLoader) ... ok
test_error_workers (__main__.TestDataLoader) ... ok
test_excessive_thread_creation_warning (__main__.TestDataLoader) ... ok
test_fd_limit_exceeded (__main__.TestDataLoader) ... ok
test_get_worker_info (__main__.TestDataLoader) ... ok
test_growing_dataset (__main__.TestDataLoader) ... ok
test_invalid_assign_after_init (__main__.TestDataLoader) ... ok
test_invalid_ctor_args_combinations (__main__.TestDataLoader) ... ok
test_iterable_style_dataset (__main__.TestDataLoader) ... ok
test_iterabledataset_len (__main__.TestDataLoader) ... ok
test_large_sampler_indices (__main__.TestDataLoader) ... ok
test_len (__main__.TestDataLoader) ... ok
test_multiple_dataloaders (__main__.TestDataLoader) ... ok
test_multiprocessing_contexts (__main__.TestDataLoader) ... [W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
ok
test_no_segfault (__main__.TestDataLoader) ... ok
test_numpy (__main__.TestDataLoader) ... ok
test_numpy_gen_state (__main__.TestDataLoader) ... ok
test_numpy_scalars (__main__.TestDataLoader) ... ok
test_partial_workers (__main__.TestDataLoader)
Check that workers exit even if the iterator is not exhausted. ... ok
test_proper_exit (__main__.TestDataLoader)
There might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore ... skipped "test doesn't currently work on the ROCm stack"
test_random_sampler (__main__.TestDataLoader) ... ok
test_random_sampler_len_with_replacement (__main__.TestDataLoader) ... ok
test_sampler (__main__.TestDataLoader) ... ok
test_sampler_reproducibility (__main__.TestDataLoader) ... ok
test_segfault (__main__.TestDataLoader) ... ok
test_seqential_batch_workers (__main__.TestDataLoader) ... ok
test_seqential_batch_workers_prefetch (__main__.TestDataLoader) ... ok
test_sequential_batch (__main__.TestDataLoader) ... ok
test_sequential_nonbatch (__main__.TestDataLoader) ... ok
test_sequential_pin_memory (__main__.TestDataLoader) ... ok
test_sequential_workers (__main__.TestDataLoader) ... ok
test_shuffle (__main__.TestDataLoader) ... ok
test_shuffle_batch (__main__.TestDataLoader) ... ok
test_shuffle_batch_none (__main__.TestDataLoader) ... ok
test_shuffle_batch_workers (__main__.TestDataLoader) ... ok
test_shuffle_batch_workers_prefetch (__main__.TestDataLoader) ... ok
test_shuffle_pin_memory (__main__.TestDataLoader) ... ok
test_shuffle_reproducibility (__main__.TestDataLoader) ... ok
test_shuffle_workers (__main__.TestDataLoader) ... ok
test_timeout (__main__.TestDataLoader) ... ok
test_typing (__main__.TestDataLoader) ... ok
test_worker_init_fn (__main__.TestDataLoader) ... ok
test_worker_seed (__main__.TestDataLoader) ... ok
test_worker_seed_reproducibility (__main__.TestDataLoader) ... ok
test_basics (__main__.TestDataLoader2) ... skipped 'no dill'
test_basic_threading (__main__.TestDataLoader2_EventLoop) ... skipped 'no dill'
test_batch_sampler (__main__.TestDataLoaderPersistentWorkers) ... ok
test_builtin_collection_conversion (__main__.TestDataLoaderPersistentWorkers) ... ok
test_bulk_loading_nobatch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_chain_iterable_style_dataset (__main__.TestDataLoaderPersistentWorkers) ... ok
test_dataset_not_reset (__main__.TestDataLoaderPersistentWorkers) ... ok
test_default_collate_bad_numpy_types (__main__.TestDataLoaderPersistentWorkers) ... ok
test_default_collate_bad_sequence_type (__main__.TestDataLoaderPersistentWorkers) ... ok
test_default_collate_dtype (__main__.TestDataLoaderPersistentWorkers) ... ok
test_default_collate_numpy_memmap (__main__.TestDataLoaderPersistentWorkers) ... ok
test_default_collate_shared_tensor (__main__.TestDataLoaderPersistentWorkers) ... ok
test_distributed_sampler_invalid_rank (__main__.TestDataLoaderPersistentWorkers) ... ok
test_duplicating_data_with_drop_last (__main__.TestDataLoaderPersistentWorkers) ... ok
test_error (__main__.TestDataLoaderPersistentWorkers) ... ok
test_error_in_init (__main__.TestDataLoaderPersistentWorkers) ... ok
test_error_workers (__main__.TestDataLoaderPersistentWorkers) ... ok
test_excessive_thread_creation_warning (__main__.TestDataLoaderPersistentWorkers) ... ok
test_fd_limit_exceeded (__main__.TestDataLoaderPersistentWorkers) ... ok
test_get_worker_info (__main__.TestDataLoaderPersistentWorkers) ... ok
test_growing_dataset (__main__.TestDataLoaderPersistentWorkers) ... ok
test_invalid_assign_after_init (__main__.TestDataLoaderPersistentWorkers) ... ok
test_invalid_ctor_args_combinations (__main__.TestDataLoaderPersistentWorkers) ... ok
test_iterable_style_dataset (__main__.TestDataLoaderPersistentWorkers) ... Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7ffa0dba1c50>>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1328, in __del__
    self._shutdown_workers()
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1320, in _shutdown_workers
    if w.is_alive():
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 134, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7ffa0dba1c50>>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1328, in __del__
    self._shutdown_workers()
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1320, in _shutdown_workers
    if w.is_alive():
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 134, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
ok
test_iterabledataset_len (__main__.TestDataLoaderPersistentWorkers) ... ok
test_large_sampler_indices (__main__.TestDataLoaderPersistentWorkers) ... ok
test_len (__main__.TestDataLoaderPersistentWorkers) ... ok
test_multiple_dataloaders (__main__.TestDataLoaderPersistentWorkers) ... ok
test_multiprocessing_contexts (__main__.TestDataLoaderPersistentWorkers) ... [W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
ok
test_no_segfault (__main__.TestDataLoaderPersistentWorkers) ... ok
test_numpy (__main__.TestDataLoaderPersistentWorkers) ... ok
test_numpy_gen_state (__main__.TestDataLoaderPersistentWorkers) ... ok
test_numpy_scalars (__main__.TestDataLoaderPersistentWorkers) ... ok
test_partial_workers (__main__.TestDataLoaderPersistentWorkers)
Check that workers exit even if the iterator is not exhausted. ... ok
test_proper_exit (__main__.TestDataLoaderPersistentWorkers)
There might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore ... skipped "test doesn't currently work on the ROCm stack"
test_random_sampler (__main__.TestDataLoaderPersistentWorkers) ... ok
test_random_sampler_len_with_replacement (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sampler (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sampler_reproducibility (__main__.TestDataLoaderPersistentWorkers) ... ok
test_segfault (__main__.TestDataLoaderPersistentWorkers) ... ok
test_seqential_batch_workers (__main__.TestDataLoaderPersistentWorkers) ... ok
test_seqential_batch_workers_prefetch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sequential_batch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sequential_nonbatch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sequential_pin_memory (__main__.TestDataLoaderPersistentWorkers) ... ok
test_sequential_workers (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_batch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_batch_none (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_batch_workers (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_batch_workers_prefetch (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_pin_memory (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_reproducibility (__main__.TestDataLoaderPersistentWorkers) ... ok
test_shuffle_workers (__main__.TestDataLoaderPersistentWorkers) ... ok
test_timeout (__main__.TestDataLoaderPersistentWorkers) ... ok
test_typing (__main__.TestDataLoaderPersistentWorkers) ... ok
test_worker_init_fn (__main__.TestDataLoaderPersistentWorkers) ... ok
test_worker_seed (__main__.TestDataLoaderPersistentWorkers) ... ok
test_worker_seed_reproducibility (__main__.TestDataLoaderPersistentWorkers) ... ok
test_lengths_must_equal_dataset_size (__main__.TestDatasetRandomSplit) ... ok
test_slicing_of_subset_of_dataset (__main__.TestDatasetRandomSplit) ... ok
test_slicing_of_subset_of_subset (__main__.TestDatasetRandomSplit) ... ok
test_splits_are_mutually_exclusive (__main__.TestDatasetRandomSplit) ... ok
test_splits_generator (__main__.TestDatasetRandomSplit) ... ok
test_splits_have_correct_size (__main__.TestDatasetRandomSplit) ... ok
test_splits_indexing_type (__main__.TestDatasetRandomSplit)
Indices generated by random_split ... ok
test_splits_reproducibility (__main__.TestDatasetRandomSplit) ... ok
test_pin_memory (__main__.TestDictDataLoader) ... ok
test_sequential_batch (__main__.TestDictDataLoader) ... ok
test_ind_worker_queue (__main__.TestIndividualWorkerQueue) ... ok
test_dataloader_with_namedtuple (__main__.TestNamedTupleDataLoader) ... ok
test_set_affinity_in_worker_init (__main__.TestSetAffinity) ... ok
test_shuffle_pin_memory (__main__.TestStringDataLoader) ... ok
test_getitem (__main__.TestTensorDataset) ... ok
test_getitem_1d (__main__.TestTensorDataset) ... ok
test_len (__main__.TestTensorDataset) ... ok
test_many_tensors (__main__.TestTensorDataset) ... ok
test_single_tensor (__main__.TestTensorDataset) ... ok

----------------------------------------------------------------------
Ran 143 tests in 99.997s

OK (skipped=4)
Running test_datapipe ... [2021-10-12 10:03:38.948930]
Executing ['/opt/conda/bin/python3.6', 'test_datapipe.py', '-v'] ... [2021-10-12 10:03:38.949005]
test_as_string (__main__.TestDataChunk) ... ok
test_getitem (__main__.TestDataChunk) ... ok
test_iter (__main__.TestDataChunk) ... ok
test_len (__main__.TestDataChunk) ... ok
test_random_shuffle (__main__.TestDataChunk) ... ok
test_reverse (__main__.TestDataChunk) ... ok
test_sort (__main__.TestDataChunk) ... ok
test_batch_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_bucket_batch_datapipe (__main__.TestFunctionalIterDataPipe) ... /opt/conda/lib/python3.6/site-packages/torch/utils/data/datapipes/iter/grouping.py:188: UserWarning: `BucketBatcher` is going to be removed from PyTorch Core
  warnings.warn("`BucketBatcher` is going to be removed from PyTorch Core")
ok
test_collate_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_concat_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_demux_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_filter_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_filter_datapipe_nested_list (__main__.TestFunctionalIterDataPipe) ... /opt/conda/lib/python3.6/site-packages/torch/utils/data/datapipes/iter/selecting.py:52: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.
  warnings.warn("Lambda function is not supported for pickle, please use "
ok
test_fork_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_map_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_sampler_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_shuffle_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_unbatch_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_zip_datapipe (__main__.TestFunctionalIterDataPipe) ... ok
test_concat_datapipe (__main__.TestFunctionalMapDataPipe) ... ok
test_map_datapipe (__main__.TestFunctionalMapDataPipe) ... ok
test_mux_datapipe (__main__.TestFunctionalMapDataPipe) ... ok
test_simple_traverse (__main__.TestGraph) ... skipped 'no dill'
test_traverse_forked (__main__.TestGraph) ... skipped 'no dill'
test_demux_mux_datapipe (__main__.TestIterableDataPipeBasic) ... ok
test_groupby_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... /opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.BufferedReader name='/tmp/tmpuhz6xn11/test_tar.tar'>
  testMethod()
ok
test_listdirfiles_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... ok
test_loadfilesfromdisk_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... ok
test_readfilesfromtar_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... /opt/conda/lib/python3.6/unittest/case.py:605: ResourceWarning: unclosed file <_io.BufferedReader name='/tmp/tmpxfl7ersf/test_tar.tar'>
  testMethod()
ok
test_readfilesfromzip_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... /opt/conda/lib/python3.6/zipfile.py:1686: ResourceWarning: unclosed file <_io.BufferedReader name='/tmp/tmp9zkgkfjh/test_zip.zip'>
  self.close()
ok
test_routeddecoder_iterable_datapipe (__main__.TestIterableDataPipeBasic) ... ok
test_large_files_http_reader_iterable_datapipes (__main__.TestIterableDataPipeHttp) ... skipped 'Test on the very large file skipped                due to the CI timing constraint.'
test_stress_http_reader_iterable_datapipes (__main__.TestIterableDataPipeHttp) ... skipped 'Stress test on large amount of files skipped                    due to the CI timing constraint.'
test_old_dataloader (__main__.TestSharding) ... skipped 'no dill'
test_sharding_length (__main__.TestSharding) ... ok
test_simple_sharding (__main__.TestSharding) ... skipped 'no dill'
test_compile_time (__main__.TestTyping) ... ok
test_construct_time (__main__.TestTyping) ... ok
test_issubinstance (__main__.TestTyping) ... ok
test_reinforce (__main__.TestTyping) ... ok
test_runtime (__main__.TestTyping) ... ok
test_subtype (__main__.TestTyping) ... ok

----------------------------------------------------------------------
Ran 43 tests in 3.327s

OK (skipped=6)
Running test_dispatch ... [2021-10-12 10:03:44.626564]
Executing ['/opt/conda/bin/python3.6', 'test_dispatch.py', '-v'] ... [2021-10-12 10:03:44.626644]
test_all_invariants (__main__.TestDispatch) ... ok
test_computed_table (__main__.TestDispatch) ... ok
test_computed_table_with_ambiguous_autogradother (__main__.TestDispatch) ... ok
test_computed_table_with_autograd (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_autograd_defaultbackend (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_autograd_math (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_autograd_math_defaultbackend (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_defaultbackend (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_math (__main__.TestDispatch) ... ok
test_computed_table_with_cpu_math_autogradcpu_fallthrough (__main__.TestDispatch) ... ok
test_computed_table_with_math (__main__.TestDispatch) ... ok
test_def (__main__.TestDispatch) ... ok
test_def_impl_schema_mismatch (__main__.TestDispatch) ... ok
test_def_only (__main__.TestDispatch) ... ok
test_def_with_explicit_alias (__main__.TestDispatch) ... ok
test_def_with_inference (__main__.TestDispatch) ... ok
test_find_dangling_impls (__main__.TestDispatch) ... ok
test_find_dangling_impls_ext (__main__.TestDispatch) ... Using /root/.cache/torch_extensions/py36_cpu as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py36_cpu/dangling_impl_extension...
Emitting ninja build file /root/.cache/torch_extensions/py36_cpu/dangling_impl_extension/build.ninja...
Building extension module dangling_impl_extension...
Using envvar MAX_JOBS (8) as the number of workers...
[1/2] c++ -MMD -MF dangling_impl_extension.o.d -DTORCH_EXTENSION_NAME=dangling_impl_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.6/site-packages/torch/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.6/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.6/site-packages/torch/include/THC -isystem /opt/conda/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -g -c /var/lib/jenkins/pytorch/test/cpp_extensions/dangling_impl_extension.cpp -o dangling_impl_extension.o 
[2/2] c++ dangling_impl_extension.o -shared -L/opt/conda/lib/python3.6/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o dangling_impl_extension.so
ok
test_impl_only (__main__.TestDispatch) ... ok
test_multiple_def_alias_defaulting (__main__.TestDispatch) ... ok
test_multiple_def_alias_mismatch (__main__.TestDispatch) ... ok
test_multiple_def_error (__main__.TestDispatch) ... ok
test_multiple_fallback (__main__.TestDispatch) ... ok
test_overwrite_math (__main__.TestDispatch) ... [W OperatorEntry.cpp:129] Warning: Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: __test45643__::foo
    no debug info
  dispatch key: (catch all)
  previous kernel: fn1
       new kernel: fn2 (function registerKernel)
[W OperatorEntry.cpp:129] Warning: Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: __test45644__::foo
    no debug info
  dispatch key: (catch all)
  previous kernel: fn1
       new kernel: fn2 (function registerKernel)
ok
test_autogradother (__main__.TestPythonDispatcher) ... ok
test_basic (__main__.TestPythonDispatcher) ... ok
test_defaultbackend_autogradcpu (__main__.TestPythonDispatcher) ... ok
test_defaultbackend_math (__main__.TestPythonDispatcher) ... ok
test_duplicate_registrations (__main__.TestPythonDispatcher) ... ok
test_math_autogradcpu (__main__.TestPythonDispatcher) ... ok

----------------------------------------------------------------------
Ran 30 tests in 29.055s

OK
Loading extension module dangling_impl_extension...
Running test_foreach ... [2021-10-12 10:04:16.538355]
Executing ['/opt/conda/bin/python3.6', 'test_foreach.py', '-v'] ... [2021-10-12 10:04:16.538442]
test_add_scalar_with_empty_list_and_empty_tensor_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_bool (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_add_scalar_with_empty_list_and_empty_tensor_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_error_cases__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_list_slow_path__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_fastpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_fastpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalar_slowpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_slowpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_different_tensor_dtypes__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_different_tensor_dtypes__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_different_tensor_dtypes__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_different_tensor_dtypes__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalar_with_overlapping_tensors__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_fastpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_fastpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_scalarlist_slowpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_scalarlist_slowpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_fastpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_fastpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_binary_op_tensorlists_slowpath__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensorlists_slowpath__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_add_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_div_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_mul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_bool (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_binary_op_tensors_on_different_devices__foreach_sub_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_bool (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_maximum_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_bool (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_minmax_fastpath__foreach_minimum_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_maximum_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_maximum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_maximum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_maximum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_minimum_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_minimum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_minimum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_float_inf_nan__foreach_minimum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_bool (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_maximum_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_bool (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_minmax_slowpath__foreach_minimum_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcdiv_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_fastpath__foreach_addcmul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcdiv_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_slowpath__foreach_addcmul_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_tensors_on_different_devices__foreach_addcdiv_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_tensors_on_different_devices__foreach_addcdiv_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_tensors_on_different_devices__foreach_addcmul_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_pointwise_op_tensors_on_different_devices__foreach_addcmul_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_abs_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_acos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_acos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_acos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_acos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_acos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_asin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_asin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_asin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_asin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_asin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_atan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_atan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_atan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_atan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_atan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_ceil_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_ceil_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_ceil_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_ceil_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cosh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cosh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cosh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cosh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_cosh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erf_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erf_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erf_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erf_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erfc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erfc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erfc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_erfc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_exp_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_exp_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_exp_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_exp_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_exp_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_expm1_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_expm1_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_expm1_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_expm1_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_floor_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_floor_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_floor_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_floor_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_frac_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_frac_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_frac_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_frac_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log10_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log10_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log10_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log10_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log10_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log1p_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log1p_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log1p_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log2_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log2_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log2_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log2_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log2_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_log_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_neg_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_reciprocal_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_reciprocal_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_reciprocal_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_round_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_round_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_round_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_round_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sigmoid_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sigmoid_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sigmoid_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sinh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sinh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sinh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sinh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sinh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sqrt_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sqrt_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sqrt_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sqrt_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_sqrt_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tanh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tanh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tanh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tanh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_tanh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_trunc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_trunc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_trunc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_fastpath__foreach_trunc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_abs_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_acos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_acos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_acos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_acos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_acos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_asin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_asin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_asin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_asin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_asin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_atan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_atan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_atan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_atan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_atan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_ceil_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_ceil_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_ceil_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_ceil_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cosh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cosh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cosh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cosh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_cosh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erf_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erf_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erf_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erf_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erfc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erfc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erfc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_erfc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_exp_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_exp_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_exp_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_exp_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_exp_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_expm1_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_expm1_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_expm1_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_expm1_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_floor_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_floor_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_floor_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_floor_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_frac_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_frac_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_frac_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_frac_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log10_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log10_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log10_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log10_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log10_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log1p_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log1p_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log1p_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log2_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log2_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log2_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log2_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log2_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_log_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_neg_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_reciprocal_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_reciprocal_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_reciprocal_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_round_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_round_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_round_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_round_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sigmoid_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sigmoid_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sigmoid_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sinh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sinh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sinh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sinh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sinh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sqrt_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sqrt_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sqrt_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sqrt_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_sqrt_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tanh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tanh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tanh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tanh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_tanh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_trunc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_trunc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_trunc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_op_tensors_on_different_devices__foreach_trunc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_abs_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_acos_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_asin_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_atan_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_ceil_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cos_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_cosh_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erf_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_erfc_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_exp_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_expm1_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_floor_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_frac_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log10_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log1p_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log2_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_log_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_neg_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_reciprocal_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_round_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sigmoid_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sin_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sinh_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_sqrt_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tan_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_tanh_cuda_uint8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_bfloat16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_bool (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_complex128 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_complex64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_float16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_float32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_float64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_int16 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_int32 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_int64 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_int8 (__main__.TestForeachCUDA) ... ok
test_unary_slowpath__foreach_trunc_cuda_uint8 (__main__.TestForeachCUDA) ... ok

----------------------------------------------------------------------
Ran 1178 tests in 263.546s

OK (skipped=144)
Running test_function_schema ... [2021-10-12 10:08:43.708598]
Executing ['/opt/conda/bin/python3.6', 'test_function_schema.py', '-v'] ... [2021-10-12 10:08:43.708684]
test_backward_compatible_arguments (__main__.TestFunctionSchema) ... ok
test_backward_compatible_outputs (__main__.TestFunctionSchema) ... ok
test_backward_compatible_structure (__main__.TestFunctionSchema) ... ok
test_backward_compatible_with_smart_serialization (__main__.TestFunctionSchema) ... ok
test_schema_error (__main__.TestFunctionSchema) ... ok
test_serialize_and_deserialize (__main__.TestFunctionSchema) ... ok
test_string_optional_parameter_default_value (__main__.TestFunctionSchema) ... test_function_schema.py:113: DeprecationWarning: Please use assertEqual instead.
  self.assertEquals(schema_a, schema_b)
ok

----------------------------------------------------------------------
Ran 7 tests in 0.078s

OK
Running test_functional_autograd_benchmark ... [2021-10-12 10:08:46.090178]
Executing ['/opt/conda/bin/python3.6', 'test_functional_autograd_benchmark.py', '-v'] ... [2021-10-12 10:08:46.090257]
test_fast_tasks (__main__.TestFunctionalAutogradBenchmark) ... Results for model resnet18 on task vjp: nans (var: nan)
Results for model ppl_simple_reg on task vjp: nans (var: nan)
Results for model ppl_robust_reg on task vjp: nans (var: nan)
Results for model wav2letter on task vjp: nans (var: nan)
Results for model transformer on task vjp: nans (var: nan)
Results for model multiheadattn on task vjp: nans (var: nan)
ok
test_slow_tasks (__main__.TestFunctionalAutogradBenchmark) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'

----------------------------------------------------------------------
Ran 2 tests in 185.527s

OK (skipped=1)
Running test_functional_optim ... [2021-10-12 10:11:53.800053]
Executing ['/opt/conda/bin/python3.6', 'test_functional_optim.py', '-v'] ... [2021-10-12 10:11:53.800136]
test_functional_optim_parity_adam (__main__.TestFunctionalOptimParity) ... ok
test_functional_optim_parity_adam_w (__main__.TestFunctionalOptimParity) ... ok
test_functional_optim_parity_sgd (__main__.TestFunctionalOptimParity) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.263s

OK
Running test_futures ... [2021-10-12 10:11:56.544972]
Executing ['/opt/conda/bin/python3.6', 'test_futures.py', '-v'] ... [2021-10-12 10:11:56.545050]
test_add_done_callback_error_is_ignored (__main__.TestFuture) ... [E pybind_utils.h:201] Got the following error when running the callback: ValueError: Expected error

At:
  test_futures.py(234): raise_value_error
  /opt/conda/lib/python3.6/site-packages/torch/futures/__init__.py(245): set_result
  test_futures.py(227): _test_add_done_callback_error_ignored
  test_futures.py(236): test_add_done_callback_error_is_ignored
  /opt/conda/lib/python3.6/unittest/case.py(605): run
  /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py(1124): run
  /opt/conda/lib/python3.6/unittest/case.py(653): __call__
  /opt/conda/lib/python3.6/unittest/suite.py(122): run
  /opt/conda/lib/python3.6/unittest/suite.py(84): __call__
  /opt/conda/lib/python3.6/unittest/suite.py(122): run
  /opt/conda/lib/python3.6/unittest/suite.py(84): __call__
  /opt/conda/lib/python3.6/unittest/runner.py(176): run
  /opt/conda/lib/python3.6/unittest/main.py(256): runTests
  /opt/conda/lib/python3.6/unittest/main.py(95): __init__
  /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py(347): run_tests
  test_futures.py(329): <module>

ok
test_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok
test_add_done_callback_no_arg_error_is_ignored (__main__.TestFuture) ... [E pybind_utils.h:201] Got the following error when running the callback: TypeError: no_arg() takes 0 positional arguments but 1 was given
ok
test_add_done_callback_simple (__main__.TestFuture) ... ok
test_chained_then (__main__.TestFuture) ... ok
test_collect_all (__main__.TestFuture) ... ok
test_done (__main__.TestFuture) ... ok
test_done_exception (__main__.TestFuture) ... ok
test_interleaving_then_and_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok
test_interleaving_then_and_add_done_callback_propagates_error (__main__.TestFuture) ... [E pybind_utils.h:201] Got the following error when running the callback: ValueError: Expected error

At:
  test_futures.py(278): raise_value_error
  /opt/conda/lib/python3.6/site-packages/torch/futures/__init__.py(245): set_result
  test_futures.py(283): test_interleaving_then_and_add_done_callback_propagates_error
  /opt/conda/lib/python3.6/unittest/case.py(605): run
  /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py(1124): run
  /opt/conda/lib/python3.6/unittest/case.py(653): __call__
  /opt/conda/lib/python3.6/unittest/suite.py(122): run
  /opt/conda/lib/python3.6/unittest/suite.py(84): __call__
  /opt/conda/lib/python3.6/unittest/suite.py(122): run
  /opt/conda/lib/python3.6/unittest/suite.py(84): __call__
  /opt/conda/lib/python3.6/unittest/runner.py(176): run
  /opt/conda/lib/python3.6/unittest/main.py(256): runTests
  /opt/conda/lib/python3.6/unittest/main.py(95): __init__
  /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py(347): run_tests
  test_futures.py(329): <module>

ok
test_mark_future_twice (__main__.TestFuture) ... ok
test_pickle_future (__main__.TestFuture) ... ok
test_set_exception (__main__.TestFuture) ... ok
test_set_exception_multithreading (__main__.TestFuture) ... ok
test_then (__main__.TestFuture) ... ok
test_then_no_arg (__main__.TestFuture) ... ok
test_then_raise (__main__.TestFuture) ... ok
test_then_wrong_arg (__main__.TestFuture) ... ok
test_wait (__main__.TestFuture) ... ok
test_wait_all (__main__.TestFuture) ... ok
test_wait_multi_thread (__main__.TestFuture) ... ok

----------------------------------------------------------------------
Ran 21 tests in 0.634s

OK
[1, 2]
Running test_fx ... [2021-10-12 10:11:59.451780]
Executing ['/opt/conda/bin/python3.6', 'test_fx.py', '-v'] ... [2021-10-12 10:11:59.451851]
test_const_fold_basic_one_attr_name_collision (fx.test_fx_const_fold.TestConstFold) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:583: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
  warnings.warn("Attempted to insert a get_attr Node with no "
ok
test_const_fold_basic_one_attr_no_name_collision (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_basic_placeholder_reordered (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_basic_two_attr (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_basic_two_attr_three_input (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_multi_const_folded_attrs (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_noop (fx.test_fx_const_fold.TestConstFold) ... ok
test_const_fold_submod_hierarchy (fx.test_fx_const_fold.TestConstFold) ... ok
test_retain_node_meta (fx.test_fx_const_fold.TestConstFold) ... ok
test_param_dim_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_param_ndim_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_param_nelement_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_param_numel_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_param_shape_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_param_size_const (fx.test_fx_param_shape_control_flow.TestConstParamShapeInControlFlow) ... ok
test_dead_chain (fx.test_dce_pass.TestDCE) ... ok
test_dead_getattr (fx.test_dce_pass.TestDCE) ... ok
test_dead_placeholder (fx.test_dce_pass.TestDCE) ... ok
test_dead_placeholder_with_user (fx.test_dce_pass.TestDCE) ... ok
test_keep_module_with_side_effects (fx.test_dce_pass.TestDCE) ... ok
test_keep_torch_assert (fx.test_dce_pass.TestDCE) ... ok
test_simple (fx.test_dce_pass.TestDCE) ... ok
test_all_input_nodes (__main__.TestFX) ... ok
test_annotation_with_future (__main__.TestFX) ... skipped '`__future__` feature `annotations` is not defined in Python <3.7'
test_annotations_empty_tuple (__main__.TestFX) ... ok
test_annotations_with_forward_references (__main__.TestFX) ... ok
test_annotations_with_no_forward_references (__main__.TestFX) ... ok
test_annotations_with_non_torch_reference_and_internal_forward_references (__main__.TestFX) ... ok
test_annotations_with_non_torch_reference_and_no_internal_forward_references (__main__.TestFX) ... ok
test_args_kwargs (__main__.TestFX) ... ok
test_args_kwargs_no_self (__main__.TestFX) ... ok
test_ast_rewriter_reassigns_submodules (__main__.TestFX) ... ok
test_ast_rewriter_rewrites_assert (__main__.TestFX) ... ok
test_ast_rewriter_rewrites_assert_with_message (__main__.TestFX) ... ok
test_ast_rewriter_wrap (__main__.TestFX) ... ok
test_ast_rewriter_wrap_fn_directly (__main__.TestFX) ... ok
test_ast_rewriter_wrap_with_submodule (__main__.TestFX) ... ok
test_ast_rewriter_wrapped_via_decorator (__main__.TestFX) ... ok
test_ast_rewriter_wrapped_via_decorator_and_transformed (__main__.TestFX) ... ok
test_autowrap_functions (__main__.TestFX) ... ok
test_construct_root_dict (__main__.TestFX) ... ok
test_copy_it (__main__.TestFX) ... ok
test_copy_no_remap (__main__.TestFX) ... ok
test_cpatcher (__main__.TestFX) ... ok
test_ctx_mgr (__main__.TestFX) ... ok
test_custom_import (__main__.TestFX) ... ok
test_custom_proxy_dynamic_value (__main__.TestFX) ... ok
test_custom_proxy_input_dependent_control_flow (__main__.TestFX) ... ok
test_custom_proxy_type (__main__.TestFX) ... ok
test_custom_proxy_type_literal (__main__.TestFX) ... ok
test_custom_traceback_not_raised_when_exception_source_is_submodule (__main__.TestFX) ... ok
test_custom_traceback_raised_when_exception_source_is_graphmodule (__main__.TestFX) ... ok
test_deepcopy_graph_with_tracer_cls (__main__.TestFX) ... ok
test_deepcopy_graphmodule_with_transform (__main__.TestFX) ... ok
test_deepcopy_recursion_depth (__main__.TestFX) ... ok
test_deepcopy_with_submods_params (__main__.TestFX) ... ok
test_dict (__main__.TestFX) ... ok
test_direct_param_use (__main__.TestFX) ... ok
test_disallow_override (__main__.TestFX) ... ok
test_ellipsis (__main__.TestFX) ... ok
test_empty_graph_codegen (__main__.TestFX) ... ok
test_erase_node_error (__main__.TestFX) ... ok
test_example_shape_prop (__main__.TestFX) ... ok
test_find_uses (__main__.TestFX) ... ok
test_fn_type_annotation_empty (__main__.TestFX) ... ok
test_fn_type_annotations (__main__.TestFX) ... ok
test_fx_create_arg (__main__.TestFX) ... ok
test_fx_shifts (__main__.TestFX) ... ok
test_get_torch_func_signature (__main__.TestFX) ... ok
test_getitem (__main__.TestFX) ... skipped 'Will be checked in test_getitem_subproc'
test_getitem_subproc (__main__.TestFX) ... graph():
    %x : [#users=2] = placeholder[target=x]
    %add : [#users=1] = call_function[target=operator.add](args = (%x, 1), kwargs = {})
    %mul : [#users=0] = call_function[target=operator.mul](args = (%add, 7), kwargs = {})
    %attr_1 : [#users=1] = get_attr[target=attr_1]
    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %attr_1), kwargs = {})
    return add_1
graph():
    %x : [#users=2] = placeholder[target=x]
    %add : [#users=1] = call_function[target=operator.add](args = (%x, 1), kwargs = {})
    %attr_1 : [#users=1] = get_attr[target=attr_1]
    %mul : [#users=0] = call_function[target=operator.mul](args = (%add, %attr_1), kwargs = {})
    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, 11), kwargs = {})
    return add_1
graph():
    %x : [#users=1] = placeholder[target=x]
    %y : [#users=0] = placeholder[target=y]
    %add : [#users=1] = call_function[target=operator.add](args = (%x, 7), kwargs = {})
    return add
graph():
    %x : [#users=1] = placeholder[target=x]
    %y : [#users=1] = placeholder[target=y]
    %add : [#users=0] = call_function[target=operator.add](args = (%y, 2), kwargs = {})
    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, 7), kwargs = {})
    return add_1
graph():
    %a : torch.Tensor [#users=2] = placeholder[target=a]
    %relu : [#users=0] = call_module[target=relu](args = (%a,), kwargs = {})
    %mul : [#users=1] = call_function[target=operator.mul](args = (%a, 2), kwargs = {})
    return mul
graph():
    %a : torch.Tensor [#users=2] = placeholder[target=a]
    %equal : [#users=1] = call_function[target=torch.equal](args = (%a, %a), kwargs = {})
    %_assert : [#users=0] = call_function[target=torch._assert](args = (%equal, a must equal a), kwargs = {})
    %mul : [#users=1] = call_function[target=operator.mul](args = (%a, 2), kwargs = {})
    return mul
graph():
    %x : [#users=2] = placeholder[target=x]
    %add : [#users=0] = call_function[target=operator.add](args = (%x, 1), kwargs = {})
    %attr_1 : [#users=1] = get_attr[target=attr_1]
    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %attr_1), kwargs = {})
    return add_1
testing print patch
ok
test_graph_edit_with_proxy (__main__.TestFX) ... ok
test_graph_fns (__main__.TestFX) ... ok
test_graph_module (__main__.TestFX) ... ok
test_graph_module_init_buffer_param_copied_dict_init (__main__.TestFX) ... ok
test_graph_module_init_buffer_param_copied_mod_init (__main__.TestFX) ... ok
test_graph_module_replicate_for_dp (__main__.TestFX) ... ok
test_graph_unique_names (__main__.TestFX) ... ok
test_graph_unique_names_manual (__main__.TestFX) ... ok
test_inf_nan (__main__.TestFX) ... ok
test_inf_nan_kwds (__main__.TestFX) ... ok
test_inline_graph (__main__.TestFX) ... ok
test_insertion_point (__main__.TestFX) ... ok
test_interpreter (__main__.TestFX) ... ok
test_interpreter_gc_values (__main__.TestFX) ... ok
test_interpreter_noop_resnet18 (__main__.TestFX) ... ok
test_interpreter_onthefly_swap (__main__.TestFX) ... ok
test_interpreter_partial_eval (__main__.TestFX) ... ok
test_interpreter_run_node_override (__main__.TestFX) ... ok
test_interpreter_star_args (__main__.TestFX) ... ok
test_leaf_module (__main__.TestFX) ... ok
test_matmul_tracing (__main__.TestFX) ... ok
test_module_deepcopy_edit_nodes (__main__.TestFX) ... ok
test_move_before (__main__.TestFX) ... ok
test_multi_insert_point (__main__.TestFX) ... ok
test_multiple_default_args (__main__.TestFX) ... ok
test_namedtuple_return_qualname (__main__.TestFX) ... ok
test_namedtuple_return_trace (__main__.TestFX) ... ok
test_native_callable (__main__.TestFX) ... skipped 'non-portable load_library call used in test'
test_no_mutation (__main__.TestFX) ... ok
test_node_tagging (__main__.TestFX) ... ok
test_nonetype_annotation (__main__.TestFX) ... ok
test_partial_trace (__main__.TestFX) ... /opt/conda/lib/python3.6/site-packages/torch/fx/_symbolic_trace.py:468: UserWarning: Was not able to add assertion to guarantee correct inputs to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.
  "Was not able to add assertion to guarantee correct inputs to "
ok
test_pickle_custom_import (__main__.TestFX) ... ok
test_pickle_graphmodule (__main__.TestFX) ... ok
test_pickle_nonetype_annotation (__main__.TestFX) ... ok
test_pickle_torch_custom_ops (__main__.TestFX) ... ok
test_pretty_print (__main__.TestFX) ... ok
test_pretty_print_graph (__main__.TestFX) ... ok
test_pretty_print_node (__main__.TestFX) ... ok
test_pretty_print_targets (__main__.TestFX) ... ok
test_pytree (__main__.TestFX) ... ok
test_pytree_concrete (__main__.TestFX) ... ok
test_randn (__main__.TestFX) ... ok
test_reassign_args_kwargs_uses (__main__.TestFX) ... ok
test_regular_and_default_args (__main__.TestFX) ... ok
test_remove_uses (__main__.TestFX) ... ok
test_replace_input (__main__.TestFX) ... ok
test_replace_uses (__main__.TestFX) ... ok
test_reserved_getattr (__main__.TestFX)
Ensure that we do not name any nodes with a reserved builtin like `getattr` ... ok
test_return_tuple (__main__.TestFX) ... ok
test_return_type_exists (__main__.TestFX) ... ok
test_script_method_trace (__main__.TestFX) ... ok
test_script_tensor_constant (__main__.TestFX) ... ok
test_sequential (__main__.TestFX) ... ok
test_shape_prop_aggregate (__main__.TestFX) ... ok
test_shape_prop_layout (__main__.TestFX) ... ok
test_shape_prop_layout_3d (__main__.TestFX) ... ok
test_single_default_arg (__main__.TestFX) ... ok
test_snake_case (__main__.TestFX) ... ok
test_sqrt (__main__.TestFX) ... ok
test_stack_traces (__main__.TestFX) ... ok
test_string_literal_return (__main__.TestFX) ... ok
test_submodule_manipulation_API (__main__.TestFX) ... ok
test_symbolic_trace_assert (__main__.TestFX) ... ok
test_symbolic_trace_sequential (__main__.TestFX) ... ok
test_tensor_attribute (__main__.TestFX) ... ok
test_tensor_constant (__main__.TestFX) ... ok
test_throw_out_variant (__main__.TestFX) ... ok
test_torch_custom_ops (__main__.TestFX) ... ok
test_torch_fx_getattr (__main__.TestFX) ... ok
test_torch_fx_len (__main__.TestFX) ... ok
test_torchbind_class_attribute_in_fx (__main__.TestFX) ... skipped 'torch.classes._TorchScriptTesting._StackString is registered, skipping'
test_torchbind_class_attribute_in_fx_tensor_arg (__main__.TestFX) ... skipped 'torch.classes._TorchScriptTesting._ReLUClass is registered, skipping'
test_trace_dict_int_keys (__main__.TestFX) ... ok
test_trace_dict_proxy_keys (__main__.TestFX) ... ok
test_trace_fn_constant (__main__.TestFX) ... ok
test_trace_function (__main__.TestFX) ... ok
test_tracing_graphmodules_as_leaf_submodules (__main__.TestFX) ... ok
test_transformer_multi_outputs (__main__.TestFX) ... ok
test_transformer_noop (__main__.TestFX) ... ok
test_transformer_op_swap (__main__.TestFX) ... ok
test_typename_print (__main__.TestFX) ... ok
test_unpack (__main__.TestFX) ... ok
test_unpack_dict_better_error (__main__.TestFX) ... ok
test_unpack_list_better_error (__main__.TestFX) ... ok
test_update_args_api (__main__.TestFX) ... ok
test_update_args_kwargs_yells_at_you (__main__.TestFX) ... ok
test_update_kwargs_api (__main__.TestFX) ... ok
test_user_friendly_call_provenance_with_function (__main__.TestFX) ... ok
test_user_friendly_call_provenance_with_module (__main__.TestFX) ... ok
test_wrap (__main__.TestFX) ... ok
test_wrap_fn_directly (__main__.TestFX) ... ok
test_wrap_with_submodule (__main__.TestFX) ... ok
test_wrapped_method (__main__.TestFX) ... ok
test_wrapped_retrace (__main__.TestFX) ... ok
test_wrapped_via_decorator (__main__.TestFX) ... ok
test_wrapped_via_decorator_and_transformed (__main__.TestFX) ... ok
test_wrong_target_type (__main__.TestFX) ... ok
test_wrong_topo (__main__.TestFX) ... ok
test_class_member_back_compat (__main__.TestFXAPIBackwardCompatibility) ... ok
test_function_back_compat (__main__.TestFXAPIBackwardCompatibility) ... ok
test_public_api_surface (__main__.TestFXAPIBackwardCompatibility) ... ok
test_nn_functional_adaptive_avg_pool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_avg_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_avg_pool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool1d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool2d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_adaptive_max_pool3d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_affine_grid (__main__.TestFunctionalTracing) ... ok
test_nn_functional_alpha_dropout (__main__.TestFunctionalTracing) ... ok
test_nn_functional_avg_pool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_avg_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_avg_pool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_batch_norm (__main__.TestFunctionalTracing) ... ok
test_nn_functional_bilinear (__main__.TestFunctionalTracing) ... ok
test_nn_functional_binary_cross_entropy (__main__.TestFunctionalTracing) ... ok
test_nn_functional_binary_cross_entropy_with_logits (__main__.TestFunctionalTracing) ... ok
test_nn_functional_celu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_celu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_channel_shuffle (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv_tbc (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv_transpose1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv_transpose2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_conv_transpose3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_cosine_embedding_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_cosine_similarity (__main__.TestFunctionalTracing) ... ok
test_nn_functional_cross_entropy (__main__.TestFunctionalTracing) ... ok
test_nn_functional_ctc_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_dropout (__main__.TestFunctionalTracing) ... ok
test_nn_functional_dropout2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_dropout3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_elu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_elu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_embedding (__main__.TestFunctionalTracing) ... ok
test_nn_functional_embedding_bag (__main__.TestFunctionalTracing) ... ok
test_nn_functional_feature_alpha_dropout (__main__.TestFunctionalTracing) ... ok
test_nn_functional_fold (__main__.TestFunctionalTracing) ... ok
test_nn_functional_fractional_max_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_fractional_max_pool2d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_fractional_max_pool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_fractional_max_pool3d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_gaussian_nll_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_glu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_grid_sample (__main__.TestFunctionalTracing) ... ok
test_nn_functional_group_norm (__main__.TestFunctionalTracing) ... ok
test_nn_functional_gumbel_softmax (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hardshrink (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hardsigmoid (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hardswish (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hardtanh (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hardtanh_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_hinge_embedding_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_huber_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_instance_norm (__main__.TestFunctionalTracing) ... ok
test_nn_functional_interpolate (__main__.TestFunctionalTracing) ... ok
test_nn_functional_kl_div (__main__.TestFunctionalTracing) ... ok
test_nn_functional_l1_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_layer_norm (__main__.TestFunctionalTracing) ... ok
test_nn_functional_leaky_relu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_leaky_relu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_linear (__main__.TestFunctionalTracing) ... ok
test_nn_functional_local_response_norm (__main__.TestFunctionalTracing) ... ok
test_nn_functional_log_softmax (__main__.TestFunctionalTracing) ... ok
test_nn_functional_logsigmoid (__main__.TestFunctionalTracing) ... ok
test_nn_functional_lp_pool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_lp_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_margin_ranking_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool1d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool2d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_pool3d_with_indices (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_unpool1d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_unpool2d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_max_unpool3d (__main__.TestFunctionalTracing) ... ok
test_nn_functional_mish (__main__.TestFunctionalTracing) ... ok
test_nn_functional_mse_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_multi_head_attention_forward (__main__.TestFunctionalTracing) ... ok
test_nn_functional_multi_margin_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_multilabel_margin_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_multilabel_soft_margin_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_nll_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_normalize (__main__.TestFunctionalTracing) ... ok
test_nn_functional_one_hot (__main__.TestFunctionalTracing) ... ok
test_nn_functional_pad (__main__.TestFunctionalTracing) ... ok
test_nn_functional_pairwise_distance (__main__.TestFunctionalTracing) ... ok
test_nn_functional_pdist (__main__.TestFunctionalTracing) ... ok
test_nn_functional_pixel_shuffle (__main__.TestFunctionalTracing) ... ok
test_nn_functional_pixel_unshuffle (__main__.TestFunctionalTracing) ... ok
test_nn_functional_poisson_nll_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_prelu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_relu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_relu6 (__main__.TestFunctionalTracing) ... ok
test_nn_functional_relu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_rrelu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_rrelu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_selu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_selu_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_silu (__main__.TestFunctionalTracing) ... ok
test_nn_functional_smooth_l1_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_soft_margin_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_softmax (__main__.TestFunctionalTracing) ... ok
test_nn_functional_softmin (__main__.TestFunctionalTracing) ... ok
test_nn_functional_softplus (__main__.TestFunctionalTracing) ... ok
test_nn_functional_softshrink (__main__.TestFunctionalTracing) ... ok
test_nn_functional_threshold (__main__.TestFunctionalTracing) ... ok
test_nn_functional_threshold_ (__main__.TestFunctionalTracing) ... ok
test_nn_functional_triplet_margin_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_triplet_margin_with_distance_loss (__main__.TestFunctionalTracing) ... ok
test_nn_functional_unfold (__main__.TestFunctionalTracing) ... ok
test_nn_functional_upsample (__main__.TestFunctionalTracing) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
ok
test_nn_functional_upsample_bilinear (__main__.TestFunctionalTracing) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3847: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
ok
test_nn_functional_upsample_nearest (__main__.TestFunctionalTracing) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3792: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
ok
test_get_torch_func_signature_exhaustive___getitem___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___radd___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rdiv___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rmatmul___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rmod___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rmul___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rpow___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive___rsub___cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_abs_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_acos_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_acosh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_add_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addbmm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addcdiv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addcmul_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addmm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addmm_decomposed_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addmv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_addr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_all_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_amax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_amin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_aminmax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_angle_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_any_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_argmax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_argmin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_asin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_asinh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_atan2_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_atan_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_atanh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_baddbmm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_bitwise_left_shift_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_bitwise_right_shift_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_bmm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_broadcast_to_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cat_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cdist_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_ceil_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cholesky_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_cholesky_inverse_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_chunk_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_clamp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_clamp_scalar_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_clone_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_complex_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_conj_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_conj_physical_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_contiguous_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_copysign_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_corrcoef_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cos_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cosh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_count_nonzero_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cov_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cross_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cummax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cummin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cumprod_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cumsum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_cumulative_trapezoid_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_deg2rad_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_diag_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_diag_embed_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_diagonal_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_diff_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_digamma_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_dist_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_div_floor_rounding_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_div_no_rounding_mode_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_div_trunc_rounding_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_dot_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_dsplit_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_dstack_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_eig_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_einsum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_eq_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_erf_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_erfc_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_erfinv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_exp2_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_exp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_expand_as_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_expand_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_expm1_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fft_fft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_fftn_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_hfft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_ifft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_ifftn_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_ihfft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_irfft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_irfftn_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_rfft_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fft_rfftn_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_fill__cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_flip_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fliplr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_flipud_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_float_power_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_floor_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_floor_divide_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fmax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fmin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fmod_autodiffed_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_fmod_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_frac_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_frexp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_gather_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_ge_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_geqrf_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_gradient_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_gt_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_hsplit_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_hstack_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_hypot_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_i0_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_igamma_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_igamma_grad_other_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_igammac_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_igammac_grad_other_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_index_add_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_index_copy_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_index_fill_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_index_put_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_index_select_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_inner_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_inverse_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_isin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_kron_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_kthvalue_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_le_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_lerp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_lgamma_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_cholesky_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_cholesky_ex_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_cond_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_det_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_eig_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_eigh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_eigvals_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_eigvalsh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_householder_product_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_inv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_inv_ex_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_lstsq_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_matrix_norm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_matrix_power_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_matrix_rank_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_matrix_rank_hermitian_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_multi_dot_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_norm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_pinv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_pinv_hermitian_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_qr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_slogdet_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_solve_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_svd_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_linalg_svdvals_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_tensorinv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_linalg_vector_norm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log10_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log1p_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log2_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log_softmax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_log_softmax_dtype_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logaddexp2_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logaddexp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logcumsumexp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logdet_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_logical_not_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logit_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_logsumexp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_lt_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_lu_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_lu_solve_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_lu_unpack_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_masked_fill_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_masked_scatter_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_masked_select_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_matmul_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_matrix_exp_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_max_binary_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_max_reduction_no_dim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_max_reduction_with_dim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_maximum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mean_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_median_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_meshgrid_list_of_tensors_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_meshgrid_variadic_tensors_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Skipped!'
test_get_torch_func_signature_exhaustive_min_binary_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_min_reduction_no_dim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_min_reduction_with_dim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_minimum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mode_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_movedim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_msort_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mul_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mv_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nan_to_num_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nanmedian_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nanquantile_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nansum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_narrow_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_ne_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_neg_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nextafter_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_adaptive_avg_pool2d_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_avg_pool2d_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_conv_transpose2d_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_cosine_similarity_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_gelu_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_grid_sample_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_hardshrink_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_hardswish_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_hardtanh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_area_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_bicubic_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_bilinear_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_linear_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_nearest_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_interpolate_trilinear_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_layer_norm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_leaky_relu_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_logsigmoid_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_mse_loss_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_nll_loss_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_normalize_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_pad_circular_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_pad_constant_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_pad_reflect_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_pad_replicate_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_relu6_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_relu_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_softplus_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_nn_functional_unfold_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_norm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_norm_fro_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_norm_inf_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_norm_nuc_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_ormqr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'cuSOLVER not available'
test_get_torch_func_signature_exhaustive_outer_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_permute_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_pinverse_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_polar_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_polygamma_polygamma_n_0_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_polygamma_polygamma_n_1_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_polygamma_polygamma_n_2_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_polygamma_polygamma_n_3_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_polygamma_polygamma_n_4_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_positive_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_pow_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_prod_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_put_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_qr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_quantile_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_rad2deg_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_ravel_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_reciprocal_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_remainder_autodiffed_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_remainder_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_renorm_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_repeat_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_reshape_as_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_reshape_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_resize__cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_resize_as__cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_resolve_conj_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_resolve_neg_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_roll_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_rot90_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_round_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_rsqrt_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_rsub_rsub_scalar_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_rsub_rsub_tensor_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_scatter_add_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_scatter_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_select_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sgn_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sigmoid_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sign_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_signbit_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sin_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sinc_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sinh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_softmax_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_softmax_with_dtype_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_solve_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_sort_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_entr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_erfcx_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_i0e_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_i1_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_i1e_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_ndtr_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_ndtri_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_xlog1py_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_zeta_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_special_zeta_grad_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_split_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_split_list_args_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_split_with_sizes_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sqrt_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_square_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_squeeze_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_stack_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_std_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_std_mean_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sub_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_sum_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_svd_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_symeig_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_get_torch_func_signature_exhaustive_t_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_take_along_dim_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_take_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_tan_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_tanh_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_tensor_split_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_tensordot_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Skipped!'
test_get_torch_func_signature_exhaustive_tile_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_to_sparse_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_topk_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_trace_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_transpose_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_trapezoid_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_trapz_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_triangular_solve_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_tril_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_triu_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_true_divide_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_trunc_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_unfold_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Skipped!'
test_get_torch_func_signature_exhaustive_unsqueeze_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_var_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_var_mean_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_vdot_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_view_as_complex_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_view_as_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_view_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_vsplit_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_vstack_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_where_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_xlogy_cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_get_torch_func_signature_exhaustive_zero__cuda_float32 (__main__.TestOperatorSignaturesCUDA) ... skipped 'Only runs on cpu'
test_subgraph_rewriter_annotations_int (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_correct_output_replacement (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_graph_argument_order (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_internal_pattern_nodes_cannot_have_users_that_are_not_matched (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_multiple_pattern_match (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_pattern_is_entire_graph (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_pattern_output_pattern_node_can_have_users_that_are_not_matched (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_placeholder_matching (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_preserves_logic (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_replaces_referenced_submodules (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_single_pattern_match (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_traced_as_callable (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_subgraph_rewriter_with_oneliner_pattern (fx.test_subgraph_rewriter.TestSubgraphRewriter) ... ok
test_torchvision_models_alexnet (__main__.TestVisionTracing) ... ok
test_torchvision_models_densenet121 (__main__.TestVisionTracing) ... ok
test_torchvision_models_densenet161 (__main__.TestVisionTracing) ... ok
test_torchvision_models_densenet169 (__main__.TestVisionTracing) ... ok
test_torchvision_models_densenet201 (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_fasterrcnn_mobilenet_v3_large_320_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_fasterrcnn_mobilenet_v3_large_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_fasterrcnn_resnet50_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_keypointrcnn_resnet50_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_maskrcnn_resnet50_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_detection_retinanet_resnet50_fpn (__main__.TestVisionTracing) ... ok
test_torchvision_models_googlenet (__main__.TestVisionTracing) ... /root/.local/lib/python3.6/site-packages/torchvision/models/googlenet.py:79: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)
/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
ok
test_torchvision_models_inception_v3 (__main__.TestVisionTracing) ... /root/.local/lib/python3.6/site-packages/torchvision/models/inception.py:82: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)
ok
test_torchvision_models_mnasnet0_5 (__main__.TestVisionTracing) ... ok
test_torchvision_models_mnasnet0_75 (__main__.TestVisionTracing) ... ok
test_torchvision_models_mnasnet1_0 (__main__.TestVisionTracing) ... ok
test_torchvision_models_mnasnet1_3 (__main__.TestVisionTracing) ... ok
test_torchvision_models_mobilenet_v2 (__main__.TestVisionTracing) ... ok
test_torchvision_models_mobilenet_v3_large (__main__.TestVisionTracing) ... ok
test_torchvision_models_mobilenet_v3_small (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnet101 (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnet152 (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnet18 (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnet34 (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnet50 (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnext101_32x8d (__main__.TestVisionTracing) ... ok
test_torchvision_models_resnext50_32x4d (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_deeplabv3_mobilenet_v3_large (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_deeplabv3_resnet101 (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_deeplabv3_resnet50 (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_fcn_resnet101 (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_fcn_resnet50 (__main__.TestVisionTracing) ... ok
test_torchvision_models_segmentation_lraspp_mobilenet_v3_large (__main__.TestVisionTracing) ... ok
test_torchvision_models_shufflenet_v2_x0_5 (__main__.TestVisionTracing) ... ok
test_torchvision_models_shufflenet_v2_x1_0 (__main__.TestVisionTracing) ... ok
test_torchvision_models_shufflenet_v2_x1_5 (__main__.TestVisionTracing) ... ok
test_torchvision_models_shufflenet_v2_x2_0 (__main__.TestVisionTracing) ... ok
test_torchvision_models_squeezenet1_0 (__main__.TestVisionTracing) ... ok
test_torchvision_models_squeezenet1_1 (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg11 (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg11_bn (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg13 (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg13_bn (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg16 (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg16_bn (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg19 (__main__.TestVisionTracing) ... ok
test_torchvision_models_vgg19_bn (__main__.TestVisionTracing) ... ok
test_torchvision_models_video_mc3_18 (__main__.TestVisionTracing) ... ok
test_torchvision_models_video_r2plus1d_18 (__main__.TestVisionTracing) ... ok
test_torchvision_models_video_r3d_18 (__main__.TestVisionTracing) ... ok
test_torchvision_models_wide_resnet101_2 (__main__.TestVisionTracing) ... ok
test_torchvision_models_wide_resnet50_2 (__main__.TestVisionTracing) ... ok

----------------------------------------------------------------------
Ran 707 tests in 98.363s

OK (skipped=356)



def forward(self, x, y_1):
    eq = y_1 == True;  y_1 = None
    _assert = torch._assert(eq, 'y has been specialized to have value True');  eq = None
    mul = 2 * x;  x = None
    return mul
    
Running test_fx_experimental ... [2021-10-12 10:13:42.773288]
Executing ['/opt/conda/bin/python3.6', 'test_fx_experimental.py', '-v'] ... [2021-10-12 10:13:42.773370]
test_annotate_returns_with_schema (__main__.TestFXExperimental) ... ok
test_call_to_assert_no_msg (__main__.TestFXExperimental) ... ok
test_call_to_assert_with_empty_msg (__main__.TestFXExperimental) ... ok
test_call_to_assert_with_msg (__main__.TestFXExperimental) ... ok
test_call_to_assert_with_multiline_message (__main__.TestFXExperimental) ... ok
test_conv_bn_fusion (__main__.TestFXExperimental) ... ok
test_cost_aware_partition (__main__.TestFXExperimental) ... ok
test_fetch (__main__.TestFXExperimental) ... ok
test_find_single_partition (__main__.TestFXExperimental) ... ok
test_lack_of_devices (__main__.TestFXExperimental) ... ok
test_large_node_error (__main__.TestFXExperimental) ... ok
test_merge_matmuls (__main__.TestFXExperimental) ... ok
test_normalize_args (__main__.TestFXExperimental) ... ok
test_normalize_args_preserve_meta (__main__.TestFXExperimental) ... ok
test_normalize_binary_operators (__main__.TestFXExperimental) ... <string>:5: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)
<string>:5: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
<string>:5: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
ok
test_normalize_modules_exhaustive (__main__.TestFXExperimental) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Convolution.cpp:647.)
  self.padding, self.dilation, self.groups)
ok
test_optimize_for_inference_cpu (__main__.TestFXExperimental) ... ok
test_optimize_for_inference_cpu_torchvision (__main__.TestFXExperimental) ... ok
test_partition_device_mapping (__main__.TestFXExperimental) ... ok
test_partition_latency (__main__.TestFXExperimental) ... ok
test_partition_node_manipulation (__main__.TestFXExperimental) ... ok
test_saturate_host (__main__.TestFXExperimental) ... ok
test_serialize_graph (__main__.TestFXExperimental) ... ok
test_size_based_partition (__main__.TestFXExperimental) ... ok
test_sparse_nn_partition (__main__.TestFXExperimental) ... ok
test_subgraph_creation (__main__.TestFXExperimental) ... ok
test_subgraph_trivial_resnet (__main__.TestFXExperimental) ... ok
test_subgraph_uniquename (__main__.TestFXExperimental) ... ok
test_to_folder (__main__.TestFXExperimental) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph_module.py:410: UserWarning: Was not able to save the following children modules as reprs -saved as pickled files instead: ['seq']
  warnings.warn("Was not able to save the following children modules as reprs -"
ok
test_traceable_function_with_nonstandard_name (__main__.TestFXExperimental) ... ok
test_type_matches (__main__.TestFXExperimental) ... ok
test_normalize_operator_exhaustive___getitem___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___radd___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rdiv___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rmatmul___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rmod___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rmul___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rpow___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive___rsub___cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_abs_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_acos_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_acosh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_add_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addbmm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addcdiv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addcmul_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addmm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addmm_decomposed_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addmv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_addr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_all_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_amax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_amin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_aminmax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_angle_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_any_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_argmax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_argmin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_asin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_asinh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_atan2_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_atan_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_atanh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_baddbmm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_bitwise_left_shift_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_bitwise_right_shift_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_bmm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_broadcast_to_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cat_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cdist_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_ceil_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cholesky_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_cholesky_inverse_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_chunk_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_clamp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_clamp_scalar_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_clone_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_complex_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_conj_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_conj_physical_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_contiguous_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_copysign_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_corrcoef_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cos_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cosh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_count_nonzero_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cov_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cross_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cummax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cummin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cumprod_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cumsum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_cumulative_trapezoid_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_deg2rad_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_diag_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_diag_embed_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_diagonal_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_diff_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_digamma_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_dist_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_div_floor_rounding_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_div_no_rounding_mode_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_div_trunc_rounding_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_dot_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_dsplit_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_dstack_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_eig_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_einsum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_eq_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_erf_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_erfc_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_erfinv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_exp2_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_exp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_expand_as_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_expand_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_expm1_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fft_fft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_fftn_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_hfft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_ifft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_ifftn_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_ihfft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_irfft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_irfftn_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_rfft_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fft_rfftn_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_fill__cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_flip_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fliplr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_flipud_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_float_power_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_floor_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_floor_divide_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fmax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fmin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fmod_autodiffed_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_fmod_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_frac_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_frexp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_gather_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_ge_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_geqrf_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_gradient_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_gt_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_hsplit_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_hstack_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_hypot_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_i0_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_igamma_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_igamma_grad_other_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_igammac_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_igammac_grad_other_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_index_add_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_index_copy_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_index_fill_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_index_put_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_index_select_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_inner_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_inverse_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_isin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_kron_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_kthvalue_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_le_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_lerp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_lgamma_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_cholesky_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_cholesky_ex_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_cond_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_det_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_eig_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_eigh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_eigvals_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_eigvalsh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_householder_product_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_inv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_inv_ex_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_lstsq_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_matrix_norm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_matrix_power_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_matrix_rank_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_matrix_rank_hermitian_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_multi_dot_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_norm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_pinv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_pinv_hermitian_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_qr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_slogdet_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_solve_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_svd_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_linalg_svdvals_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_tensorinv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_linalg_vector_norm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log10_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log1p_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log2_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log_softmax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_log_softmax_dtype_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logaddexp2_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logaddexp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logcumsumexp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logdet_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_logical_not_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logit_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_logsumexp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_lt_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_lu_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_lu_solve_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_lu_unpack_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_masked_fill_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_masked_scatter_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_masked_select_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_matmul_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_matrix_exp_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_max_binary_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_max_reduction_no_dim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_max_reduction_with_dim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_maximum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mean_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_median_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_meshgrid_list_of_tensors_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Skipped!'
test_normalize_operator_exhaustive_meshgrid_variadic_tensors_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Skipped!'
test_normalize_operator_exhaustive_min_binary_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_min_reduction_no_dim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_min_reduction_with_dim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_minimum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mode_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_movedim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_msort_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mul_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mv_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nan_to_num_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nanmedian_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nanquantile_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nansum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_narrow_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_ne_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_neg_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nextafter_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_adaptive_avg_pool2d_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_avg_pool2d_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_conv_transpose2d_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_cosine_similarity_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_gelu_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_grid_sample_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_hardshrink_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_hardswish_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_hardtanh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_area_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_bicubic_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_bilinear_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_linear_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_nearest_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_interpolate_trilinear_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_layer_norm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_leaky_relu_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_logsigmoid_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_mse_loss_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_nll_loss_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_normalize_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_pad_circular_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_pad_constant_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_pad_reflect_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_pad_replicate_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_relu6_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_relu_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_softplus_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_nn_functional_unfold_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_norm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_norm_fro_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_norm_inf_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_norm_nuc_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_ormqr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'cuSOLVER not available'
test_normalize_operator_exhaustive_outer_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_permute_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_pinverse_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_polar_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_polygamma_polygamma_n_0_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_polygamma_polygamma_n_1_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_polygamma_polygamma_n_2_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_polygamma_polygamma_n_3_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_polygamma_polygamma_n_4_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_positive_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_pow_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_prod_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_put_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_qr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_quantile_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_rad2deg_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_ravel_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_reciprocal_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_remainder_autodiffed_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_remainder_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_renorm_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_repeat_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_reshape_as_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_reshape_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_resize__cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_resize_as__cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_resolve_conj_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_resolve_neg_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_roll_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_rot90_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_round_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_rsqrt_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_rsub_rsub_scalar_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_rsub_rsub_tensor_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_scatter_add_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_scatter_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_select_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sgn_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sigmoid_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sign_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_signbit_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sin_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sinc_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sinh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_softmax_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_softmax_with_dtype_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_solve_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_sort_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_entr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_erfcx_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_i0e_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_i1_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_i1e_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_ndtr_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_ndtri_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_xlog1py_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_zeta_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_special_zeta_grad_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_split_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_split_list_args_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_split_with_sizes_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sqrt_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_square_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_squeeze_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_stack_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_std_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_std_mean_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sub_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_sum_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_svd_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_symeig_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_normalize_operator_exhaustive_t_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_take_along_dim_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_take_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tan_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tanh_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tensor_split_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tensordot_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tile_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_to_sparse_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_topk_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_trace_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_transpose_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_trapezoid_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_trapz_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_triangular_solve_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_tril_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_triu_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_true_divide_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_trunc_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_unfold_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_unsqueeze_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_var_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_var_mean_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_vdot_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_view_as_complex_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_view_as_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_view_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_vsplit_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_vstack_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_where_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_xlogy_cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'
test_normalize_operator_exhaustive_zero__cuda_float32 (__main__.TestNormalizeOperatorsCUDA) ... skipped 'Only runs on cpu'

----------------------------------------------------------------------
Ran 382 tests in 42.618s

OK (skipped=351)
[0, 4]
[1, 2]
Running test_import_time ... [2021-10-12 10:14:29.201128]
Executing ['/opt/conda/bin/python3.6', 'test_import_time.py', '-v'] ... [2021-10-12 10:14:29.201214]
test_time_cuda_device_count (__main__.TestImportTime) ... ok
test_time_import_torch (__main__.TestImportTime) ... ok

----------------------------------------------------------------------
Ran 2 tests in 4.319s

OK
Running test_indexing ... [2021-10-12 10:14:35.868428]
Executing ['/opt/conda/bin/python3.6', 'test_indexing.py', '-v'] ... [2021-10-12 10:14:35.868515]
test_boolean_assignment_value_mismatch_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_indexing_alldims_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_indexing_onedim_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_indexing_twodim_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_indexing_weirdness_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_indexing_weirdness_tensors_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_list_indexing_cuda (__main__.NumpyTestsCUDA) ... ok
test_boolean_shape_mismatch_cuda (__main__.NumpyTestsCUDA) ... test_indexing.py:1334: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/IndexingUtils.h:30.)
  self.assertRaisesRegex(IndexError, 'mask', lambda: arr[index])
test_indexing.py:1335: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/IndexingUtils.h:30.)
  self.assertRaisesRegex(IndexError, 'mask', lambda: arr[(slice(None), index)])
ok
test_broadcast_subspace_cuda (__main__.NumpyTestsCUDA) ... ok
test_broaderrors_indexing_cuda (__main__.NumpyTestsCUDA) ... ok
test_ellipsis_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_empty_fancy_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_empty_tuple_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_everything_returns_views_cuda (__main__.NumpyTestsCUDA) ... ok
test_index_is_larger_cuda (__main__.NumpyTestsCUDA) ... ok
test_index_no_floats_cuda (__main__.NumpyTestsCUDA) ... ok
test_none_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_single_bool_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_single_int_index_cuda (__main__.NumpyTestsCUDA) ... ok
test_trivial_fancy_out_of_bounds_cuda (__main__.NumpyTestsCUDA) ... skipped 'CUDA asserts instead of raising an exception'
test_advancedindex_big_cuda (__main__.TestIndexingCUDA) ... ok
test_advancedindex_cuda_float16 (__main__.TestIndexingCUDA) ... ok
test_advancedindex_cuda_float64 (__main__.TestIndexingCUDA) ... ok
test_basic_advanced_combined_cuda (__main__.TestIndexingCUDA) ... ok
test_bool_indices_accumulate_cuda (__main__.TestIndexingCUDA) ... ok
test_bool_indices_cuda (__main__.TestIndexingCUDA) ... ok
test_byte_mask2d_cuda (__main__.TestIndexingCUDA) ... ok
test_byte_mask_accumulate_cuda (__main__.TestIndexingCUDA) ... ok
test_byte_mask_cuda (__main__.TestIndexingCUDA) ... ok
test_byte_tensor_assignment_cuda (__main__.TestIndexingCUDA) ... ok
test_cpu_indices_cuda (__main__.TestIndexingCUDA) ... ok
test_ellipsis_tensor_cuda (__main__.TestIndexingCUDA) ... ok
test_empty_index_cuda (__main__.TestIndexingCUDA) ... ok
test_empty_ndim_index_bool_cuda (__main__.TestIndexingCUDA) ... ok
test_empty_ndim_index_cuda (__main__.TestIndexingCUDA) ... ok
test_empty_slice_cuda (__main__.TestIndexingCUDA) ... ok
test_gather_take_along_dim_cross_device_cuda_float32 (__main__.TestIndexingCUDA) ... ok
test_getitem_scalars_cuda (__main__.TestIndexingCUDA) ... ok
test_index_cuda (__main__.TestIndexingCUDA) ... ok
test_index_getitem_copy_bools_slices_cuda (__main__.TestIndexingCUDA) ... ok
test_index_put_accumulate_duplicate_indices_cuda (__main__.TestIndexingCUDA) ... ok
test_index_put_accumulate_large_tensor_cuda (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_bfloat16 (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_bool (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_complex128 (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_complex64 (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_float16 (__main__.TestIndexingCUDA) ... ok
test_index_put_src_datatype_cuda_int64 (__main__.TestIndexingCUDA) ... ok
test_index_scalar_with_bool_mask_cuda (__main__.TestIndexingCUDA) ... ok
test_index_setitem_bools_slices_cuda (__main__.TestIndexingCUDA) ... ok
test_index_src_datatype_cuda_bfloat16 (__main__.TestIndexingCUDA) ... ok
test_index_src_datatype_cuda_bool (__main__.TestIndexingCUDA) ... ok
test_index_src_datatype_cuda_float16 (__main__.TestIndexingCUDA) ... ok
test_index_src_datatype_cuda_int64 (__main__.TestIndexingCUDA) ... ok
test_int_assignment_cuda (__main__.TestIndexingCUDA) ... ok
test_int_indices2d_cuda (__main__.TestIndexingCUDA) ... ok
test_int_indices_broadcast_cuda (__main__.TestIndexingCUDA) ... ok
test_int_indices_cuda (__main__.TestIndexingCUDA) ... ok
test_invalid_device_cuda (__main__.TestIndexingCUDA) ... ok
test_invalid_index_cuda (__main__.TestIndexingCUDA) ... ok
test_jit_indexing_cuda (__main__.TestIndexingCUDA) ... ok
test_multiple_bool_indices_cuda (__main__.TestIndexingCUDA) ... ok
test_multiple_byte_mask_cuda (__main__.TestIndexingCUDA) ... ok
test_multiple_int_cuda (__main__.TestIndexingCUDA) ... ok
test_none_cuda (__main__.TestIndexingCUDA) ... ok
test_out_of_bound_index_cuda (__main__.TestIndexingCUDA) ... ok
test_setitem_expansion_error_cuda (__main__.TestIndexingCUDA) ... ok
test_setitem_scalars_cuda (__main__.TestIndexingCUDA) ... ok
test_single_int_cuda (__main__.TestIndexingCUDA) ... ok
test_step_assignment_cuda (__main__.TestIndexingCUDA) ... ok
test_step_cuda (__main__.TestIndexingCUDA) ... ok
test_take_along_dim_cuda_float32 (__main__.TestIndexingCUDA) ... "Cannot find Symbol"
test_indexing failed! Received signal: SIGIOT
Running test_jit ... [2021-10-12 10:14:55.557116]
Executing ['/opt/conda/bin/python3.6', 'test_jit.py', '-v'] ... [2021-10-12 10:14:55.557179]
test_async_future_type_python (jit.test_async.TestAsync) ... ok
test_async_grad_guard_no_grad (jit.test_async.TestAsync) ... ok
test_async_grad_guard_with_grad (jit.test_async.TestAsync) ... ok
test_async_kwargs (jit.test_async.TestAsync) ... ok
test_async_parsing (jit.test_async.TestAsync) ... ok
test_async_python (jit.test_async.TestAsync) ... ok
test_async_script (jit.test_async.TestAsync) ... ok
test_async_script_capture (jit.test_async.TestAsync) ... ok
test_async_script_error (jit.test_async.TestAsync) ... ok
test_async_script_multi_forks (jit.test_async.TestAsync) ... ok
test_async_script_multi_waits (jit.test_async.TestAsync) ... ok
test_async_script_nested (jit.test_async.TestAsync) ... ok
test_async_script_no_script_mod (jit.test_async.TestAsync) ... ok
test_async_script_trace (jit.test_async.TestAsync) ... ok
test_future_subtyping (jit.test_async.TestAsync) ... ok
test_no_future_subtype_message (jit.test_async.TestAsync) ... ok
test_trace_fork_wait (jit.test_async.TestAsync) ... ok
test_trace_fork_wait_inline (jit.test_async.TestAsync) ... ok
test_trace_fork_wait_inline_onnx (jit.test_async.TestAsync) ... ok
test_trace_fork_wait_leaking (jit.test_async.TestAsync) ... ok
test_trace_fork_wait_list_modulecalls (jit.test_async.TestAsync) ... ok
test_trace_modulecalls_with_different_output_types (jit.test_async.TestAsync) ... ok
test_aten_pow_zero_negative_exponent (jit.test_aten_pow.TestAtenPow) ... ok
test_bias_as_arg (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_bias_as_module_attr (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_chunk_constant_script_ad (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_constructed_bias (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_diff_graph_inline_threshold (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_differentiable_graph_ops_requires_grad (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... skipped 'disable until we property handle tensor lists with undefined gradients'
test_does_not_create_cycles (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_does_not_merge_unrelated (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_merge_respects_aliasing (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_merges_dense (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_merges_down (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_merges_up (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_merges_without_cycles (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_prune_grad (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... skipped "Simple Executor doesn't support gradients"
test_requires_grad_for_tensor_list (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_respects_lexical_scoping (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_simple_merge (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_simple_no_merge (jit.test_autodiff_subgraph_slicing.TestAutodiffSubgraphSlicing) ... ok
test_errors (jit.test_backends.TestBackends) ... skipped 'Non-portable load_library call used in test'
test_execution (jit.test_backends.TestBackends) ... skipped 'Non-portable load_library call used in test'
test_save_load (jit.test_backends.TestBackends) ... skipped 'Non-portable load_library call used in test'
test_errors (jit.test_backends.TestBackendsWithCompiler) ... skipped 'Non-portable load_library call used in test'
test_execution (jit.test_backends.TestBackendsWithCompiler) ... skipped 'Non-portable load_library call used in test'
test_del (jit.test_builtins.TestBuiltins) ... ok
test_del_multiple_operands (jit.test_builtins.TestBuiltins) ... ok
test_has_attr (jit.test_builtins.TestBuiltins) ... ok
test_has_attr_invalid_args (jit.test_builtins.TestBuiltins) ... ok
test_cuda_synchronize (jit.test_cuda.TestCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_current_stream (jit.test_cuda.TestCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_event_args (jit.test_cuda.TestCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_stream_args (jit.test_cuda.TestCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_streams_and_events (jit.test_cuda.TestCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_cast_overloads (jit.test_class_type.TestClassType) ... ok
test_class_attribute_wrong_type (jit.test_class_type.TestClassType) ... ok
test_class_constant (jit.test_class_type.TestClassType) ... ok
test_class_constructs_itself (jit.test_class_type.TestClassType) ... ok
test_class_inheritance (jit.test_class_type.TestClassType) ... ok
test_class_inheritance_implicit (jit.test_class_type.TestClassType) ... ok
test_class_sorting (jit.test_class_type.TestClassType) ... ok
test_class_specialization (jit.test_class_type.TestClassType) ... ok
test_class_type_as_param (jit.test_class_type.TestClassType) ... ok
test_classmethod (jit.test_class_type.TestClassType) ... ok
test_conditional_set_attr (jit.test_class_type.TestClassType) ... ok
test_custom_delete (jit.test_class_type.TestClassType) ... ok
test_default_args (jit.test_class_type.TestClassType) ... ok
test_get_attr (jit.test_class_type.TestClassType) ... ok
test_get_attr_not_initialized (jit.test_class_type.TestClassType) ... ok
test_get_with_method (jit.test_class_type.TestClassType) ... ok
test_imported_classes (jit.test_class_type.TestClassType) ... ok
test_in (jit.test_class_type.TestClassType) ... ok
test_init_compiled_first (jit.test_class_type.TestClassType) ... ok
test_interface (jit.test_class_type.TestClassType) ... ok
test_optional_type_promotion (jit.test_class_type.TestClassType) ... ok
test_out_of_order_methods (jit.test_class_type.TestClassType) ... ok
test_overloaded_fn (jit.test_class_type.TestClassType) ... ok
test_properties (jit.test_class_type.TestClassType) ... ok
test_py_class_to_ivalue_missing_attribute (jit.test_class_type.TestClassType) ... ok
test_python_interop (jit.test_class_type.TestClassType) ... ok
test_recursive_class (jit.test_class_type.TestClassType) ... ok
test_recursive_script_builtin_type_resolution (jit.test_class_type.TestClassType) ... ok
test_recursive_script_module_builtin_type_resolution (jit.test_class_type.TestClassType) ... ok
test_recursive_scripting (jit.test_class_type.TestClassType) ... ok
test_recursive_scripting_failed (jit.test_class_type.TestClassType) ... ok
test_reference_semantics (jit.test_class_type.TestClassType) ... ok
test_save_load_with_classes (jit.test_class_type.TestClassType) ... ok
test_save_load_with_classes_nested (jit.test_class_type.TestClassType) ... ok
test_save_load_with_classes_returned (jit.test_class_type.TestClassType) ... ok
test_schema_human_readable (jit.test_class_type.TestClassType) ... ok
test_self_referential_method (jit.test_class_type.TestClassType) ... ok
test_set_attr_in_method (jit.test_class_type.TestClassType) ... ok
test_set_attr_non_initialized (jit.test_class_type.TestClassType) ... ok
test_set_attr_type_mismatch (jit.test_class_type.TestClassType) ... ok
test_staticmethod (jit.test_class_type.TestClassType) ... ok
test_type_annotation (jit.test_class_type.TestClassType) ... ok
test_type_annotations (jit.test_class_type.TestClassType) ... ok
test_unresolved_class_attributes (jit.test_class_type.TestClassType) ... ok
test_unused_method (jit.test_class_type.TestClassType) ... ok
test_comparison_ops (jit.test_complex.TestComplex) ... ok
test_complex_constants_and_ops (jit.test_complex.TestComplex) ... ok
test_complex_constructor (jit.test_complex.TestComplex) ... ok
test_complex_list_sum (jit.test_complex.TestComplex) ... ok
test_complex_parse (jit.test_complex.TestComplex) ... ok
test_complexdict (jit.test_complex.TestComplex) ... ok
test_complexlist (jit.test_complex.TestComplex) ... ok
test_div (jit.test_complex.TestComplex) ... ok
test_infj_nanj_pickle (jit.test_complex.TestComplex) ... ok
test_pickle (jit.test_complex.TestComplex) ... ok
test_script (jit.test_complex.TestComplex) ... ok
test_tensor_attributes (jit.test_complex.TestComplex) ... ok
test_torch_complex_constructor_with_tensor (jit.test_complex.TestComplex) ... ok
test_calling_scripted_custom_op (jit.test_custom_operators.TestCustomOperators) ... ok
test_calling_traced_custom_op (jit.test_custom_operators.TestCustomOperators) ... ok
test_default_arguments_are_used (jit.test_custom_operators.TestCustomOperators) ... ok
test_dynamic_op_registry (jit.test_custom_operators.TestCustomOperators) ... ok
test_generic_list (jit.test_custom_operators.TestCustomOperators) ... ok
test_only_kwargs (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_an_argument_both_as_positional_and_kwarg (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_and_returning_lists (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_one_positional_but_not_the_second (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_too_few_args (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_too_many_args (jit.test_custom_operators.TestCustomOperators) ... ok
test_passing_unknown_kwargs (jit.test_custom_operators.TestCustomOperators) ... ok
test_script_graph_contains_custom_op (jit.test_custom_operators.TestCustomOperators) ... ok
test_script_graph_for_custom_ops_matches_traced_graph (jit.test_custom_operators.TestCustomOperators) ... skipped 'Need to figure out default dtype differences between fbcode and oss'
test_simply_calling_an_operator (jit.test_custom_operators.TestCustomOperators) ... ok
test_python_submodule_script (jit.test_data_parallel.TestDataParallel) ... /opt/conda/lib/python3.6/site-packages/torch/jit/_recursive.py:235: UserWarning: 'm' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.
  " but it is a non-constant {}. Consider removing it.".format(name, hint))
ok
test_shared_module (jit.test_data_parallel.TestDataParallel) ... ok
test_tensor_sharing (jit.test_data_parallel.TestDataParallel) ... ok
test_tensor_sharing_with_forward (jit.test_data_parallel.TestDataParallel) ... ok
test_traced_module (jit.test_data_parallel.TestDataParallel) ... ok
test_aug_assign (jit.test_list_dict.TestDict) ... ok
test_basic (jit.test_list_dict.TestDict) ... ok
test_clear (jit.test_list_dict.TestDict) ... ok
test_copy (jit.test_list_dict.TestDict) ... ok
test_del (jit.test_list_dict.TestDict) ... ok
test_dict_bool_conversion (jit.test_list_dict.TestDict) ... ok
test_dict_preserves_order (jit.test_list_dict.TestDict) ... ok
test_dict_to_python (jit.test_list_dict.TestDict) ... ok
test_get (jit.test_list_dict.TestDict) ... ok
test_get_boolkey (jit.test_list_dict.TestDict) ... ok
test_items (jit.test_list_dict.TestDict) ... ok
test_key_type (jit.test_list_dict.TestDict) ... ok
test_keys (jit.test_list_dict.TestDict) ... ok
test_len (jit.test_list_dict.TestDict) ... ok
test_loop (jit.test_list_dict.TestDict) ... ok
test_membership (jit.test_list_dict.TestDict) ... ok
test_mutability (jit.test_list_dict.TestDict) ... ok
test_optional_dict_construct (jit.test_list_dict.TestDict) ... ok
test_ordered_dict (jit.test_list_dict.TestDict) ... ok
test_pop (jit.test_list_dict.TestDict) ... ok
test_popitem (jit.test_list_dict.TestDict) ... ok
test_setdefault (jit.test_list_dict.TestDict) ... ok
test_type_annotation_missing_contained_type (jit.test_list_dict.TestDict) ... ok
test_update (jit.test_list_dict.TestDict) ... ok
test_update_existing_key (jit.test_list_dict.TestDict) ... ok
test_values (jit.test_list_dict.TestDict) ... ok
test_view (jit.test_list_dict.TestDict) ... ok
test_closed_over_enum_constant (jit.test_enum.TestEnum) ... ok
test_enum_as_const (jit.test_enum.TestEnum) ... ok
test_enum_as_module_attribute (jit.test_enum.TestEnum) ... ok
test_enum_comp (jit.test_enum.TestEnum) ... ok
test_enum_comp_diff_classes (jit.test_enum.TestEnum) ... ok
test_enum_explicit_script (jit.test_enum.TestEnum) ... ok
test_enum_iterate (jit.test_enum.TestEnum) ... ok
test_enum_ivalue_type (jit.test_enum.TestEnum) ... ok
test_enum_module_return (jit.test_enum.TestEnum) ... ok
test_enum_name (jit.test_enum.TestEnum) ... ok
test_enum_return (jit.test_enum.TestEnum) ... ok
test_enum_value (jit.test_enum.TestEnum) ... ok
test_enum_value_types (jit.test_enum.TestEnum) ... ok
test_heterogenous_value_type_enum_error (jit.test_enum.TestEnum) ... ok
test_non_existent_enum_value (jit.test_enum.TestEnum) ... ok
test_string_enum_as_module_attribute (jit.test_enum.TestEnum) ... ok
test_aten_fallback (jit.test_export_modes.TestExportModes) ... /opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:46: UserWarning: You are exporting the model to ONNX while in training mode with 'train' parameter not specified. The model will default to inference mode export. If you wish to export a training amenable ONNX model, specify training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE (to preserve the original model state) in torch.onnx.export().
  warnings.warn("You are exporting the model to ONNX while in training mode with "
ok
test_compressed_zipfile (jit.test_export_modes.TestExportModes) ... ok
test_directory (jit.test_export_modes.TestExportModes) ... ok
test_onnx_aten (jit.test_export_modes.TestExportModes) ... ok
test_onnx_multiple_return (jit.test_export_modes.TestExportModes) ... /opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:334: UserWarning: Model has no forward function
  warnings.warn("Model has no forward function")
ok
test_protobuf (jit.test_export_modes.TestExportModes) ... ok
test_zipfile (jit.test_export_modes.TestExportModes) ... ok
test_freeze_module (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_detach_gradient (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_in_training_mode (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_no_forward (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_return_self (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_return_sub_module (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_attr (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_attr2 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_attr3 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_tensor_attr (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_tensor_attr2 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_tensor_attr3 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_aliased_tensor_attr4 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_fork (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_fork2 (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_fork_calling_module_method (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_helperfunction (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_inplace_mutable (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_list (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_mutable_dict (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_mutable_list (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_mutable_tensor (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_nested_fork (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_nestedaliasing (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_nestedaliasingscalar (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_non_static_module_container_index (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_overlapping_attrs (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_preserve_sub_module (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_preserve_sub_module_and_mutation (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_sharedclasstype (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_submodule (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_tensor (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_tuple (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_tupleoutput_submodule (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_user_preserved_attr (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_user_preserved_method (jit.test_freezing.TestFreezing) ... ok
test_freeze_module_with_user_preserved_method2 (jit.test_freezing.TestFreezing) ... ok
test_freeze_non_module_class_getattr (jit.test_freezing.TestFreezing) ... ok
test_module_getattr_indirection (jit.test_freezing.TestFreezing) ... ok
test_module_with_shared_type_instances (jit.test_freezing.TestFreezing) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
ok
test_instancing_error (__main__.TestFrontend) ... ok
test_collapse_adjacent_conversions (jit.test_freezing.TestFrozenOptimizations) ... ok
test_conv_add_folding (jit.test_freezing.TestFrozenOptimizations) ... ok
test_conv_bn_folding (jit.test_freezing.TestFrozenOptimizations) ... ok
test_conv_hardswish (jit.test_freezing.TestFrozenOptimizations) ... ok
test_conv_to_mkldnn (jit.test_freezing.TestFrozenOptimizations) ... ok
test_conv_to_mkldnn_no_mkldnn (jit.test_freezing.TestFrozenOptimizations) ... skipped 'Testing no mkldnn'
test_freeze_conv_relu_fusion (jit.test_freezing.TestFrozenOptimizations) ... skipped 'requires CUDNN'
test_freeze_mkdlnn (jit.test_freezing.TestFrozenOptimizations) ... ok
test_freeze_remove_dropout (jit.test_freezing.TestFrozenOptimizations) ... ok
test_freeze_remove_feature_dropout (jit.test_freezing.TestFrozenOptimizations) ... ok
test_hardswish_hardsigmoid (jit.test_freezing.TestFrozenOptimizations) ... ok
test_incompatible_perf_formats (jit.test_freezing.TestFrozenOptimizations) ... ok
test_layernorm (jit.test_freezing.TestFrozenOptimizations) ... ok
test_linear_to_mkldnn (jit.test_freezing.TestFrozenOptimizations) ... ok
test_maxpool_mkldnn (jit.test_freezing.TestFrozenOptimizations) ... ok
test_mkldnn_fuser_broadcasting (jit.test_freezing.TestFrozenOptimizations) ... ok
test_mkldnn_inplace_removal (jit.test_freezing.TestFrozenOptimizations) ... ok
test_optimize_for_inference (jit.test_freezing.TestFrozenOptimizations) ... ok
test_optimize_freeze_module (jit.test_freezing.TestFrozenOptimizations) ... ok
test_pool2d_batchnorm (jit.test_freezing.TestFrozenOptimizations) ... ok
test_pool3d_batchnorm (jit.test_freezing.TestFrozenOptimizations) ... ok
test_scalar_mul (jit.test_freezing.TestFrozenOptimizations) ... ok
test_subgraph_creation (jit.test_functional_blocks.TestFunctionalBlocks) ... ok
test_check_no_type_promotion (jit.test_convert_activation.TestFunctionalToInplaceActivation) ... ok
test_functional_to_inplace_activation (jit.test_convert_activation.TestFunctionalToInplaceActivation) ... ok
test_no_functional_to_inplace (jit.test_convert_activation.TestFunctionalToInplaceActivation) ... ok
test_resnet18_correctness (jit.test_convert_activation.TestFunctionalToInplaceActivation) ... ok
test_getattr_with_default (jit.test_attr.TestGetDefaultAttr) ... ok
test_fuse_linear (jit.test_graph_rewrite_passes.TestGraphRewritePasses) ... ok
test_hash_bool (jit.test_hash.TestHash) ... ok
test_hash_device (jit.test_hash.TestHash) ... ok
test_hash_float (jit.test_hash.TestHash) ... ok
test_hash_int (jit.test_hash.TestHash) ... ok
test_hash_none (jit.test_hash.TestHash) ... ok
test_hash_string (jit.test_hash.TestHash) ... ok
test_hash_tensor (jit.test_hash.TestHash)
Tensors should hash by identity ... ok
test_hash_tuple (jit.test_hash.TestHash) ... ok
test_hash_tuple_nested_unhashable_type (jit.test_hash.TestHash) ... ok
test_forward_tuple_input (jit.test_hooks.TestHooks) ... ok
test_hook_compilation_hint (jit.test_hooks.TestHooks) ... ok
test_hook_hook_name_collision (jit.test_hooks.TestHooks) ... ok
test_hook_method_name_collision (jit.test_hooks.TestHooks) ... ok
test_module_direct_forward_invocation (jit.test_hooks.TestHooks) ... ok
test_module_forward_multiple_inputs (jit.test_hooks.TestHooks) ... ok
test_module_forward_single_input (jit.test_hooks.TestHooks) ... ok
test_module_hook_return_nothing (jit.test_hooks.TestHooks) ... ok
test_module_multiple_hooks_multiple_inputs (jit.test_hooks.TestHooks) ... ok
test_module_multiple_hooks_single_input (jit.test_hooks.TestHooks) ... ok
test_module_no_forward_input (jit.test_hooks.TestHooks) ... ok
test_module_same_hook_repeated (jit.test_hooks.TestHooks) ... ok
test_submodule_called_directly_with_hooks (jit.test_hooks.TestHooks) ... ok
test_submodule_direct_forward_invocation (jit.test_hooks.TestHooks) ... ok
test_submodule_forward_multiple_inputs (jit.test_hooks.TestHooks) ... ok
test_submodule_forward_single_input (jit.test_hooks.TestHooks) ... ok
test_submodule_forward_single_input_return_not_tupled (jit.test_hooks.TestHooks) ... ok
test_submodule_hook_return_nothing (jit.test_hooks.TestHooks) ... ok
test_submodule_multiple_hooks_multiple_inputs (jit.test_hooks.TestHooks) ... ok
test_submodule_multiple_hooks_single_input (jit.test_hooks.TestHooks) ... ok
test_submodule_no_forward_input (jit.test_hooks.TestHooks) ... ok
test_submodule_same_hook_repeated (jit.test_hooks.TestHooks) ... ok
test_wrong_hook_signatures (jit.test_hooks.TestHooks) ... ok
test_wrong_pre_hook_signatures (jit.test_hooks.TestHooks) ... ok
test_add_out_ignorable_args (jit.test_ignorable_args.TestIgnorableArgs) ... ok
test_slice_ignorable_args_for_slice (jit.test_ignorable_args.TestIgnorableArgs) ... ok
test_with_ignore_context_manager_with_inp_out (jit.test_ignore_context_manager.TestIgnoreContextManager) ... ok
test_with_ignore_context_manager_with_just_inp (jit.test_ignore_context_manager.TestIgnoreContextManager) ... ok
test_with_ignore_context_manager_with_just_out (jit.test_ignore_context_manager.TestIgnoreContextManager) ... ok
test_inplace_to_functional_activation (jit.test_convert_activation.TestInplaceToFunctionalActivation) ... ok
test_resnet18_correctness (jit.test_convert_activation.TestInplaceToFunctionalActivation) ... ok
test_bool (jit.test_isinstance.TestIsinstance) ... ok
test_dict (jit.test_isinstance.TestIsinstance) ... ok
test_dict_nested (jit.test_isinstance.TestIsinstance) ... ok
test_dict_no_contained_type (jit.test_isinstance.TestIsinstance) ... ok
test_dict_tensor (jit.test_isinstance.TestIsinstance) ... ok
test_empty_container_special_cases (jit.test_isinstance.TestIsinstance) ... ok
test_empty_container_throws_warning_in_eager (jit.test_isinstance.TestIsinstance) ... ok
test_float (jit.test_isinstance.TestIsinstance) ... ok
test_if_else (jit.test_isinstance.TestIsinstance) ... ok
test_in_if (jit.test_isinstance.TestIsinstance) ... ok
test_in_while_loop (jit.test_isinstance.TestIsinstance) ... ok
test_int (jit.test_isinstance.TestIsinstance) ... ok
test_list (jit.test_isinstance.TestIsinstance) ... ok
test_list_nested (jit.test_isinstance.TestIsinstance) ... ok
test_list_no_contained_type (jit.test_isinstance.TestIsinstance) ... ok
test_list_tensor (jit.test_isinstance.TestIsinstance) ... ok
test_list_tensor_type_true (jit.test_isinstance.TestIsinstance) ... ok
test_nontuple_container_rhs_throws_in_eager (jit.test_isinstance.TestIsinstance) ... ok
test_optional (jit.test_isinstance.TestIsinstance) ... ok
test_optional_nested (jit.test_isinstance.TestIsinstance) ... ok
test_optional_no_contained_type (jit.test_isinstance.TestIsinstance) ... ok
test_optional_none (jit.test_isinstance.TestIsinstance) ... ok
test_tensor_type_false (jit.test_isinstance.TestIsinstance) ... ok
test_tuple (jit.test_isinstance.TestIsinstance) ... ok
test_tuple_nested (jit.test_isinstance.TestIsinstance) ... ok
test_tuple_no_contained_type (jit.test_isinstance.TestIsinstance) ... ok
test_tuple_rhs (jit.test_isinstance.TestIsinstance) ... ok
test_tuple_tensor (jit.test_isinstance.TestIsinstance) ... ok
test_type_refinement (jit.test_isinstance.TestIsinstance) ... ok
test_ModuleList (__main__.TestJit) ... ok
test_Sequential (__main__.TestJit) ... ok
test_add_relu_fusion (__main__.TestJit) ... ok
test_arg_configurations (__main__.TestJit)
Different arg configurations should trigger different traces ... skipped 'Need to be adjusted to Graph Executor'
test_attrs (__main__.TestJit) ... ok
test_batchnorm (__main__.TestJit) ... ok
test_big (__main__.TestJit) ... skipped 'Requires a lot of RAM'
test_constant_insertion (__main__.TestJit) ... ok
test_constant_prop_aliasing_type (__main__.TestJit) ... ok
test_constant_prop_exception (__main__.TestJit) ... ok
test_constant_prop_if_constant (__main__.TestJit) ... ok
test_constant_prop_if_inline (__main__.TestJit) ... ok
test_constant_prop_loop_constant (__main__.TestJit) ... ok
test_constant_prop_nested (__main__.TestJit) ... ok
test_constant_prop_none (__main__.TestJit) ... ok
test_constant_prop_print (__main__.TestJit) ... ok
test_constant_prop_rand (__main__.TestJit) ... ok
test_constant_prop_remove_output (__main__.TestJit) ... ok
test_constant_prop_simple (__main__.TestJit) ... ok
test_constants_pkl (__main__.TestJit) ... ok
test_cpp (__main__.TestJit) ... skipped 'covered by test_cpp_cuda'
test_cse (__main__.TestJit) ... ok
test_cse_not_introduce_aliasing (__main__.TestJit) ... ok
test_cu_escaped_number (__main__.TestJit) ... ok
test_cuda_export_restore (__main__.TestJit) ... ok
test_debug_flush_compilation_cache (__main__.TestJit) ... ok
test_decompose_addmm (__main__.TestJit) ... ok
test_device_not_equal (__main__.TestJit) ... ok
test_diff_subgraph_clones_constants (__main__.TestJit) ... ok
test_disabled (__main__.TestJit) ... ok
test_dropout (__main__.TestJit) ... ok
test_dropout_cuda (__main__.TestJit) ... ok
test_dropout_func_requires_grad (__main__.TestJit) ... ok
test_dropout_module_requires_grad (__main__.TestJit) ... skipped 'Testing differentiable graph'
test_einsum (__main__.TestJit) ... ok
test_element_size (__main__.TestJit) ... ok
test_expand_fold_quant_inputs (__main__.TestJit) ... ok
test_expand_quantlint (__main__.TestJit) ... ok
test_export_batchnorm (__main__.TestJit) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_export_dropout (__main__.TestJit) ... ok
test_export_lstm (__main__.TestJit) ... ok
test_export_opnames (__main__.TestJit) ... ok
test_export_rnn (__main__.TestJit) ... ok
test_flags (__main__.TestJit) ... skipped 'Need to instrument GraphExecutors a bit more'
test_function_default_values (__main__.TestJit) ... ok
test_hide_source_ranges_context_manager (__main__.TestJit) ... ok
test_import_method (__main__.TestJit) ... ok
test_inferred_as_tensor (__main__.TestJit) ... ok
test_layout (__main__.TestJit) ... ok
test_module_default_values (__main__.TestJit) ... ok
test_mutable_default_values (__main__.TestJit) ... ok
test_nn_conv (__main__.TestJit) ... ok
test_nn_padding (__main__.TestJit) ... ok
test_no_erroneous_warnings (__main__.TestJit) ... ok
test_non_ascii_string (__main__.TestJit) ... skipped 'temporarily disable the test for fwd compatibility'
test_numel (__main__.TestJit) ... ok
test_pattern_based_module_rewrite (__main__.TestJit) ... ok
test_pattern_based_rewrite (__main__.TestJit) ... ok
test_pattern_based_rewrite_with_source_range_preserved (__main__.TestJit) ... ok
test_peephole_optimize_shape_ops (__main__.TestJit) ... skipped "Simple executor doesn't have shape information"
test_pretty_printer (__main__.TestJit) ... ok
test_print_classes_module (__main__.TestJit) ... ok
test_print_op_module (__main__.TestJit) ... ok
test_print_torch_ops_modules (__main__.TestJit) ... ok
test_profiler (__main__.TestJit) ... ok
test_python_bindings (__main__.TestJit) ... ok
test_python_ir (__main__.TestJit) ... ok
test_python_ivalue (__main__.TestJit) ... ok
test_pytorch_jit_env_off (__main__.TestJit) ... ok
test_recursive_cse (__main__.TestJit) ... ok
test_restore_device (__main__.TestJit) ... ok
test_restore_device_cuda (__main__.TestJit) ... ok
test_restore_shared_storage_on_cuda (__main__.TestJit) ... ok
test_script_autograd_grad (__main__.TestJit) ... ok
test_script_backward (__main__.TestJit) ... ok
test_script_backward_twice (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/jit/_script.py:1215: UserWarning: `optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead
  "`optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead"
ok
test_script_tensor_type (__main__.TestJit) ... ok
test_shape_analysis_broadcast (__main__.TestJit) ... ok
test_shape_analysis_masked_select (__main__.TestJit) ... ok
test_shape_analysis_unsqueeze_in_loop (__main__.TestJit) ... ok
test_sparse_csr_tensors (__main__.TestJit) ... ok
test_sparse_tensors (__main__.TestJit) ... ok
test_torch_complex (__main__.TestJit) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:139: UserWarning: An output with one or more elements was resized since it had shape [3, 4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  return callable(*args, **kwargs)
/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:139: UserWarning: An output with one or more elements was resized since it had shape [5, 2], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  return callable(*args, **kwargs)
ok
test_torch_load_error (__main__.TestJit) ... skipped 'TODO: re-enable with https://github.com/pytorch/pytorch/pull/29339'
test_torch_load_zipfile_check (__main__.TestJit) ... ok
test_torch_ops_kwonly (__main__.TestJit) ... ok
test_torch_ops_overloaded (__main__.TestJit) ... ok
test_torch_sum (__main__.TestJit) ... ok
test_trace_retains_train (__main__.TestJit) ... ok
test_train_eval (__main__.TestJit) ... ok
test_unchecked_cast (__main__.TestJit) ... ok
test_unique_state_dict (__main__.TestJit) ... ok
test_verify (__main__.TestJit) ... skipped 'verify needs to be updated to work with GraphExecutors'
test_warnings (__main__.TestJit) ... ok
test_generate_autocast_jit_trace_model (__main__.TestJitAutocast) ... ok
test_nchw_autocast_jit_trace_model (__main__.TestJitAutocast) ... ok
test_nhwc_autocast_jit_trace_model (__main__.TestJitAutocast) ... ok
test_nn_adaptive_avg_pool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_adaptive_avg_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_adaptive_avg_pool3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_adaptive_max_pool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_adaptive_max_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_adaptive_max_pool3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_affine_grid (__main__.TestJitGeneratedFunctional) ... ok
test_nn_alpha_dropout (__main__.TestJitGeneratedFunctional) ... ok
test_nn_avg_pool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_avg_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_avg_pool3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_inference (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_size_zero (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_size_zero_inference (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_training (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_only_bias_inference (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_only_bias_training (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_only_weight_inference (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_only_weight_training (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_weight_and_bias_inference (__main__.TestJitGeneratedFunctional) ... ok
test_nn_batch_norm_with_weight_and_bias_training (__main__.TestJitGeneratedFunctional) ... ok
test_nn_bilinear (__main__.TestJitGeneratedFunctional) ... ok
test_nn_binary_cross_entropy (__main__.TestJitGeneratedFunctional) ... ok
test_nn_binary_cross_entropy_size_average (__main__.TestJitGeneratedFunctional) ... ok
test_nn_binary_cross_entropy_with_logits (__main__.TestJitGeneratedFunctional) ... ok
test_nn_celu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_celu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv_tbc (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv_transpose1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv_transpose2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_conv_transpose3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_cosine_embedding_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_cosine_similarity (__main__.TestJitGeneratedFunctional) ... ok
test_nn_cross_entropy (__main__.TestJitGeneratedFunctional) ... ok
test_nn_ctc_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_dropout (__main__.TestJitGeneratedFunctional) ... ok
test_nn_dropout2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_dropout3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_elu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_elu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_embedding (__main__.TestJitGeneratedFunctional) ... ok
test_nn_embedding_bag (__main__.TestJitGeneratedFunctional) ... ok
test_nn_feature_alpha_dropout (__main__.TestJitGeneratedFunctional) ... ok
test_nn_fold (__main__.TestJitGeneratedFunctional) ... ok
test_nn_fractional_max_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_glu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_grid_sample (__main__.TestJitGeneratedFunctional) ... ok
test_nn_group_norm (__main__.TestJitGeneratedFunctional) ... ok
test_nn_gumbel_softmax (__main__.TestJitGeneratedFunctional) ... ok
test_nn_gumbel_softmax_hard (__main__.TestJitGeneratedFunctional) ... ok
test_nn_hardshrink (__main__.TestJitGeneratedFunctional) ... ok
test_nn_hardtanh (__main__.TestJitGeneratedFunctional) ... ok
test_nn_hardtanh_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_hinge_embedding_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_huber_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_huber_loss_with_grad (__main__.TestJitGeneratedFunctional) ... ok
test_nn_instance_norm (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_3d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_3d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_4d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_4d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_4d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_5d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_5d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_area_5d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bicubic_4d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bicubic_4d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bicubic_4d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bicubic_4d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bicubic_4d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bilinear_4d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bilinear_4d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bilinear_4d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bilinear_4d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_bilinear_4d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_linear_3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_linear_3d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_linear_3d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_linear_3d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_linear_3d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_3d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_3d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_3d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_3d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_4d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_4d_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_4d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_4d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_4d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_5d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_5d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_5d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_nearest_5d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_trilinear_5d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_trilinear_5d_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_trilinear_5d_with_scale_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_trilinear_5d_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_interpolate_trilinear_5d_with_size_not_recompute_scale_factor (__main__.TestJitGeneratedFunctional) ... ok
test_nn_kl_div (__main__.TestJitGeneratedFunctional) ... ok
test_nn_l1_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_l1_loss_with_grad (__main__.TestJitGeneratedFunctional) ... ok
test_nn_layer_norm (__main__.TestJitGeneratedFunctional) ... ok
test_nn_layer_norm_with_only_bias (__main__.TestJitGeneratedFunctional) ... ok
test_nn_layer_norm_with_only_weight (__main__.TestJitGeneratedFunctional) ... ok
test_nn_layer_norm_with_weight_and_bias (__main__.TestJitGeneratedFunctional) ... ok
test_nn_leaky_relu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_leaky_relu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_linear (__main__.TestJitGeneratedFunctional) ... ok
test_nn_linear_addmm (__main__.TestJitGeneratedFunctional) ... ok
test_nn_local_response_norm (__main__.TestJitGeneratedFunctional) ... ok
test_nn_log_softmax (__main__.TestJitGeneratedFunctional) ... ok
test_nn_lp_pool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_lp_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_margin_ranking_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_pool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_pool1d_with_indices (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_pool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_pool2d_with_indices (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_pool3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_unpool1d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_unpool2d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_max_unpool3d (__main__.TestJitGeneratedFunctional) ... ok
test_nn_mse_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_mse_loss_with_grad (__main__.TestJitGeneratedFunctional) ... ok
test_nn_multi_margin_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_multilabel_margin_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_multilabel_soft_margin_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_nll_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_normalize (__main__.TestJitGeneratedFunctional) ... ok
test_nn_pad (__main__.TestJitGeneratedFunctional) ... ok
test_nn_pairwise_distance (__main__.TestJitGeneratedFunctional) ... ok
test_nn_pdist (__main__.TestJitGeneratedFunctional) ... ok
test_nn_pixel_shuffle (__main__.TestJitGeneratedFunctional) ... ok
test_nn_pixel_unshuffle (__main__.TestJitGeneratedFunctional) ... ok
test_nn_poisson_nll_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_poisson_nll_loss_full (__main__.TestJitGeneratedFunctional) ... ok
test_nn_relu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_relu6 (__main__.TestJitGeneratedFunctional) ... ok
test_nn_relu6_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_relu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_rrelu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_rrelu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_selu (__main__.TestJitGeneratedFunctional) ... ok
test_nn_selu_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_sigmoid (__main__.TestJitGeneratedFunctional) ... ok
test_nn_smooth_l1_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_smooth_l1_loss_with_grad (__main__.TestJitGeneratedFunctional) ... ok
test_nn_soft_margin_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_softmax (__main__.TestJitGeneratedFunctional) ... ok
test_nn_softmax_with_all_args (__main__.TestJitGeneratedFunctional) ... ok
test_nn_softmin (__main__.TestJitGeneratedFunctional) ... ok
test_nn_softplus (__main__.TestJitGeneratedFunctional) ... ok
test_nn_softsign (__main__.TestJitGeneratedFunctional) ... ok
test_nn_tanh (__main__.TestJitGeneratedFunctional) ... ok
test_nn_tanhshrink (__main__.TestJitGeneratedFunctional) ... ok
test_nn_threshold (__main__.TestJitGeneratedFunctional) ... ok
test_nn_threshold_inplace (__main__.TestJitGeneratedFunctional) ... ok
test_nn_triplet_margin_loss (__main__.TestJitGeneratedFunctional) ... ok
test_nn_unfold (__main__.TestJitGeneratedFunctional) ... ok
test_nn_upsample_with_scale (__main__.TestJitGeneratedFunctional) ... ok
test_nn_upsample_with_size (__main__.TestJitGeneratedFunctional) ... ok
test_nn_AdaptiveAvgPool1d (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool1d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool1d_one_output (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool2d_single (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool2d_single_1x1output (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool2d_tuple (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool2d_tuple_none (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool3d_last_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool3d_single (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool3d_tuple (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveAvgPool3d_tuple_none (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool1d (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool1d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool2d_single (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool2d_tuple (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool2d_tuple_none (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_single (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_single_nonatomic (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_tuple (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_tuple_nonatomic (__main__.TestJitGeneratedModule) ... ok
test_nn_AdaptiveMaxPool3d_tuple_none (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool1d (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool1d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool1d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool1d_stride_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_divisor (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_divisor_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_divisor_stride_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool2d_stride_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride1_pad0_gpu_input (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride_pad_gpu_fixedkw_output (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride_pad_gpu_general_output (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_divisor_stride_pad_gpu_input_nooverlap (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride1_pad0_gpu_input (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride_pad_gpu_fixedkw_output (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride_pad_gpu_general_output (__main__.TestJitGeneratedModule) ... ok
test_nn_AvgPool3d_stride_pad_gpu_input_nooverlap (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_scalar_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_weights_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_BCELoss_weights_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_legacy_enum (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_scalar_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_BCEWithLogitsLoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_3d_input (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_3d_input_not_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_affine_simple_average (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_not_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_not_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm1d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d_2d_simple_average (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d_momentum (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d_not_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d_not_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm2d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d_3d_simple_average (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d_momentum (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d_not_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d_not_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_BatchNorm3d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_Bilinear (__main__.TestJitGeneratedModule) ... ok
test_nn_CELU (__main__.TestJitGeneratedModule) ... ok
test_nn_CELU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_CELU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_CTCLoss_2d_int_target_lengths_intlists (__main__.TestJitGeneratedModule) ... skipped 'module test skipped on JIT'
test_nn_CTCLoss_2d_int_target_lengths_tensors (__main__.TestJitGeneratedModule) ... ok
test_nn_CTCLoss_2d_lengths_tensors (__main__.TestJitGeneratedModule) ... ok
test_nn_CTCLoss_lengths_intlists (__main__.TestJitGeneratedModule) ... skipped 'module test skipped on JIT'
test_nn_CTCLoss_lengths_tensors (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad1d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad1d_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad1d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad2d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad3d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad3d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ConstantPad3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_circular_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_groups (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad1 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad1size1 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad2size1 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad_same (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad_same2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad_same_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_pad_valid (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_reflect_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_replicate_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv1d_zeros_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_circular_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_depthwise (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_depthwise_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_depthwise_padded (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_depthwise_strided (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_depthwise_with_multiplier (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_groups (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_groups_thnn (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_pad_same (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_pad_same_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_pad_valid (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_padding (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_reflect_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_replicate_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_strided (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv2d_zeros_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_1x1x1_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_circular_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_dilated_strided (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_groups (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_pad_same (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_pad_same_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_pad_valid (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_replicate_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_stride_padding (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_zero_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_Conv3d_zeros_stride2_pad2 (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose1d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose1d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose1d_groups (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose1d_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose2d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose2d_groups (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose2d_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose3d (__main__.TestJitGeneratedModule) ... ok
test_nn_ConvTranspose3d_dilated (__main__.TestJitGeneratedModule) ... ok
test_nn_CosineEmbeddingLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_CosineEmbeddingLoss_margin (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_indices_target_smoothing (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_indices_target_smoothing_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_indices_target_smoothing_sum_reduction (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_indices_target_smoothing_weight (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_prob_target (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_prob_target_smoothing (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_prob_target_smoothing_sum_reduction (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_prob_target_smoothing_weight (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_prob_target_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_2d_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_indices_target_smoothing (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_indices_target_smoothing_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_indices_target_smoothing_sum_reduction (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_indices_target_smoothing_sum_reduction_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_prob_target (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_prob_target_smoothing (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_prob_target_smoothing_sum_reduction (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_3d_prob_target_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_4d_prob_target (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_4d_prob_target_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_dim_is_3 (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_higher_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossEntropyLoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_CrossMapLRN2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ELU (__main__.TestJitGeneratedModule) ... ok
test_nn_ELU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ELU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Embedding (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_max (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_max_padding_idx (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_mean_padding_idx (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_sparse (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_EmbeddingBag_sum_padding_idx (__main__.TestJitGeneratedModule) ... ok
test_nn_Embedding_discontiguous (__main__.TestJitGeneratedModule) ... ok
test_nn_Embedding_sparse (__main__.TestJitGeneratedModule) ... ok
test_nn_Flatten (__main__.TestJitGeneratedModule) ... ok
test_nn_Flatten_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Fold (__main__.TestJitGeneratedModule) ... ok
test_nn_Fold_int_input (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_ratio (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_ratio_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_ratio_no_batch_dim_no_random_samples (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_size (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_size_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool2d_size_no_batch_dim_no_random_samples (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool3d_asymsize (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool3d_ratio (__main__.TestJitGeneratedModule) ... ok
test_nn_FractionalMaxPool3d_size (__main__.TestJitGeneratedModule) ... ok
test_nn_GELU (__main__.TestJitGeneratedModule) ... ok
test_nn_GELU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_GELU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_GLU (__main__.TestJitGeneratedModule) ... ok
test_nn_GLU_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_GLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_GRUCell (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_1d_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_1d_affine_GN (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_1d_affine_large_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_1d_no_affine_IN (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_1d_no_affine_LN (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_2d_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_2d_affine_large_feature (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_2d_no_affine_IN (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_2d_no_affine_LN (__main__.TestJitGeneratedModule) ... ok
test_nn_GroupNorm_2d_no_affine_large_feature (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardshrink (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardshrink_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardshrink_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardsigmoid_no_batch_dim (__main__.TestJitGeneratedModule) ... skipped 'module test skipped on JIT'
test_nn_Hardswish_no_batch_dim (__main__.TestJitGeneratedModule) ... skipped 'module test skipped on JIT'
test_nn_Hardtanh (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardtanh_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Hardtanh_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_margin (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_margin_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_HingeEmbeddingLoss_scalar_margin (__main__.TestJitGeneratedModule) ... ok
test_nn_HuberLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_HuberLoss_delta (__main__.TestJitGeneratedModule) ... ok
test_nn_HuberLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_HuberLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_HuberLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm1d (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm1d_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm2d (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm2d_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm3d (__main__.TestJitGeneratedModule) ... ok
test_nn_InstanceNorm3d_tracking_stats (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_log_target (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_reduce_log_target (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_no_reduce_scalar_log_target (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_scalar_log_target (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_with_log_target_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_KLDivLoss_with_target_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_reduce_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_L1Loss_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_LPPool1d (__main__.TestJitGeneratedModule) ... ok
test_nn_LPPool1d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_LPPool1d_norm (__main__.TestJitGeneratedModule) ... ok
test_nn_LPPool2d (__main__.TestJitGeneratedModule) ... ok
test_nn_LPPool2d_norm (__main__.TestJitGeneratedModule) ... ok
test_nn_LSTMCell (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_1d_elementwise_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_1d_empty_elementwise_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_1d_no_elementwise_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_3d_elementwise_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_3d_no_affine_large_feature (__main__.TestJitGeneratedModule) ... ok
test_nn_LayerNorm_3d_no_elementwise_affine (__main__.TestJitGeneratedModule) ... ok
test_nn_LeakyReLU (__main__.TestJitGeneratedModule) ... ok
test_nn_LeakyReLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_LeakyReLU_with_negval (__main__.TestJitGeneratedModule) ... ok
test_nn_LeakyReLU_with_negval_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_LeakyReLU_with_zero_negval (__main__.TestJitGeneratedModule) ... ok
test_nn_Linear (__main__.TestJitGeneratedModule) ... ok
test_nn_Linear_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Linear_no_bias (__main__.TestJitGeneratedModule) ... ok
test_nn_LocalResponseNorm_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_LocalResponseNorm_2d_uneven_pad (__main__.TestJitGeneratedModule) ... ok
test_nn_LocalResponseNorm_3d_custom_params (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSigmoid (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSigmoid_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSigmoid_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSoftmax (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSoftmax_multiparam (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSoftmax_multiparam_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_LogSoftmax_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_prec (__main__.TestJitGeneratedModule) ... ok
test_nn_MSELoss_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_MarginRankingLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_MarginRankingLoss_margin (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool1d (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool1d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool2d_3d_input (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool2d_4d_input (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool3d (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool3d_stride (__main__.TestJitGeneratedModule) ... ok
test_nn_MaxPool3d_stride_padding (__main__.TestJitGeneratedModule) ... ok
test_nn_Mish (__main__.TestJitGeneratedModule) ... ok
test_nn_Mish_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Mish_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_0d_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_1d_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_index_neg (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelMarginLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelSoftMarginLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelSoftMarginLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelSoftMarginLoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiLabelSoftMarginLoss_weights_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_1d_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_margin (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_margin_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_p (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_p_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiMarginLoss_weights_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_MultiheadAttention (__main__.TestJitGeneratedModule) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_nn_NLLLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss2d_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss2d_no_reduce_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss2d_no_reduce_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLossNd_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLossNd_no_reduce_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLossNd_no_reduce_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_2d_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_2d_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_dim_is_3 (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_higher_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_reduce_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_reduce_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_reduce_weights_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_no_reduce_weights_ignore_index_neg (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_weights (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_weights_ignore_index (__main__.TestJitGeneratedModule) ... ok
test_nn_NLLLoss_weights_ignore_index_neg (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_1d_multiparam (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_2d_multiparam (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_3d_multiparam (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_PReLU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding122112_3dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding1221_2dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding12_1dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding2322_2dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding31_1dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding322112_3dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding332122_3dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding3331_2dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_Padding33_1dcircular (__main__.TestJitGeneratedModule) ... ok
test_nn_PixelShuffle (__main__.TestJitGeneratedModule) ... ok
test_nn_PixelUnshuffle (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_full_loss (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_full_loss_no_log_input (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_full_loss (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_full_loss_no_log_input (__main__.TestJitGeneratedModule) ... ok
test_nn_PoissonNLLLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_RNNCell (__main__.TestJitGeneratedModule) ... ok
test_nn_RReLU (__main__.TestJitGeneratedModule) ... ok
test_nn_RReLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_RReLU_with_up_down (__main__.TestJitGeneratedModule) ... ok
test_nn_RReLU_with_up_down_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU6 (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU6_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU6_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ReLU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad1d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad1d_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad1d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad2d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad3d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad3d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReflectionPad3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad1d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad1d_batch (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad1d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad2d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad3d (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad3d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ReplicationPad3d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_SELU (__main__.TestJitGeneratedModule) ... ok
test_nn_SELU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_SELU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_SiLU (__main__.TestJitGeneratedModule) ... ok
test_nn_SiLU_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_SiLU_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Sigmoid (__main__.TestJitGeneratedModule) ... ok
test_nn_Sigmoid_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Sigmoid_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_beta (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_no_reduce_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_SmoothL1Loss_zero_beta (__main__.TestJitGeneratedModule) ... ok
test_nn_SoftMarginLoss (__main__.TestJitGeneratedModule) ... ok
test_nn_SoftMarginLoss_no_batch_dim_mean (__main__.TestJitGeneratedModule) ... ok
test_nn_SoftMarginLoss_no_batch_dim_none (__main__.TestJitGeneratedModule) ... ok
test_nn_SoftMarginLoss_no_batch_dim_sum (__main__.TestJitGeneratedModule) ... ok
test_nn_SoftMarginLoss_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmax (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmax2d (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmax2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmax_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmax_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmin (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmin_multidim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmin_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softmin_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Softplus (__main__.TestJitGeneratedModule) ... ok
test_nn_Softplus_beta (__main__.TestJitGeneratedModule) ... ok
test_nn_Softplus_beta_threshold (__main__.TestJitGeneratedModule) ... ok
test_nn_Softplus_beta_threshold_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Softplus_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softshrink (__main__.TestJitGeneratedModule) ... ok
test_nn_Softshrink_lambda (__main__.TestJitGeneratedModule) ... ok
test_nn_Softshrink_lambda_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Softshrink_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softsign (__main__.TestJitGeneratedModule) ... ok
test_nn_Softsign_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Softsign_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanh (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanh_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanh_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanhshrink (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanhshrink_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Tanhshrink_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Threshold_large_value (__main__.TestJitGeneratedModule) ... ok
test_nn_Threshold_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Threshold_threshold_value (__main__.TestJitGeneratedModule) ... ok
test_nn_Threshold_threshold_value_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_Transformer (__main__.TestJitGeneratedModule) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_nn_TransformerDecoderLayer_gelu_activation (__main__.TestJitGeneratedModule) ... ok
test_nn_TransformerDecoderLayer_relu_activation (__main__.TestJitGeneratedModule) ... ok
test_nn_TransformerEncoderLayer_gelu_activation (__main__.TestJitGeneratedModule) ... ok
test_nn_TransformerEncoderLayer_relu_activation (__main__.TestJitGeneratedModule) ... ok
test_nn_Transformer_multilayer_coder (__main__.TestJitGeneratedModule) ... ok
test_nn_Unflatten_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_Unfold (__main__.TestJitGeneratedModule) ... ok
test_nn_Unfold_int_input (__main__.TestJitGeneratedModule) ... ok
test_nn_ZeroPad2d (__main__.TestJitGeneratedModule) ... ok
test_nn_ZeroPad2d_complex (__main__.TestJitGeneratedModule) ... ok
test_nn_ZeroPad2d_negative_dims (__main__.TestJitGeneratedModule) ... ok
test_nn_ZeroPad2d_no_batch_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_2d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_scale_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_scale_tuple_shared_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_scale_tuple_skewed_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_scale_tuple_skewed_2d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_tuple_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bicubic_tuple_2d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_2d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_scale_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_scale_tuple_shared_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_scale_tuple_skewed_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_scale_tuple_skewed_2d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_tuple_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_bilinear_tuple_2d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_1d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_1d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_scale_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_scale_1d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_linear_tuple_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_1d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_2d_launch_configs (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_2d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_3d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_scale_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_scale_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_scale_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_tuple_1d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_tuple_2d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_nearest_tuple_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_3d_zero_dim (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_scale_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_scale_3d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_tuple_3d (__main__.TestJitGeneratedModule) ... ok
test_nn_interpolate_trilinear_tuple_3d_align_corners (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_dim0 (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_dim3 (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_lastdim (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_spatial (__main__.TestJitGeneratedModule) ... ok
test_nn_log_softmax_spatial_special (__main__.TestJitGeneratedModule) ... ok
test_nn_multimarginloss_1d_input_0d_target_no_reduce (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_functional_dim0 (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_functional_dim3 (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_functional_scalar (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_lastdim (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_lastdim_dtype (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_spatial (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_spatial_dtype (__main__.TestJitGeneratedModule) ... ok
test_nn_softmax_spatial_special (__main__.TestJitGeneratedModule) ... ok
test_checkscriptassertraisesregex (jit.test_jit_utils.TestJitUtils) ... ok
test_get_callable_argument_names_hybrid (jit.test_jit_utils.TestJitUtils) ... skipped 'POSITIONAL_ONLY arguments are not supported before 3.8'
test_get_callable_argument_names_keyword_only (jit.test_jit_utils.TestJitUtils) ... ok
test_get_callable_argument_names_positional_only (jit.test_jit_utils.TestJitUtils) ... skipped 'POSITIONAL_ONLY arguments are not supported before 3.8'
test_get_callable_argument_names_positional_or_keyword (jit.test_jit_utils.TestJitUtils) ... ok
test_get_callable_argument_names_var_keyword (jit.test_jit_utils.TestJitUtils) ... ok
test_get_callable_argument_names_var_positional (jit.test_jit_utils.TestJitUtils) ... ok
test_comprehension_iterable (jit.test_list_dict.TestList) ... ok
test_comprehension_out_type_not_in_type (jit.test_list_dict.TestList) ... ok
test_comprehensions_basic (jit.test_list_dict.TestList) ... ok
test_comprehensions_basic_float (jit.test_list_dict.TestList) ... ok
test_comprehensions_two_comps (jit.test_list_dict.TestList) ... ok
test_copy_list_immutable (jit.test_list_dict.TestList) ... ok
test_copy_list_mutable (jit.test_list_dict.TestList) ... ok
test_del (jit.test_list_dict.TestList) ... ok
test_dict_keyword_is_correctly_typed (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_dict_comprehension (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_dict_comprehension_and_kwargs (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_empty_dict_comprehension (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_empty_iterable (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_internal_aggregate_function (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_iterable (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_kwargs (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_kwargs_using_container_values (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_mapping (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_mapping_and_kwargs (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_mismatched_annotations (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_nested_call (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_previously_declared_variable (jit.test_list_dict.TestList) ... ok
test_dict_keyword_with_previously_declared_variable_and_kwargs (jit.test_list_dict.TestList) ... ok
test_extend_list_immutable (jit.test_list_dict.TestList) ... ok
test_extend_list_mutable (jit.test_list_dict.TestList) ... ok
test_in_check (jit.test_list_dict.TestList) ... ok
test_list_bool_conversion (jit.test_list_dict.TestList) ... ok
test_list_count (jit.test_list_dict.TestList) ... ok
test_list_count_not_existing (jit.test_list_dict.TestList) ... ok
test_list_gather (jit.test_list_dict.TestList) ... ok
test_list_index (jit.test_list_dict.TestList) ... ok
test_list_index_not_existing (jit.test_list_dict.TestList) ... ok
test_list_keyword (jit.test_list_dict.TestList) ... ok
test_list_len (jit.test_list_dict.TestList) ... ok
test_list_literal (jit.test_list_dict.TestList) ... ok
test_list_none (jit.test_list_dict.TestList) ... ok
test_list_ops (jit.test_list_dict.TestList) ... ok
test_list_slice (jit.test_list_dict.TestList) ... ok
test_list_sort (jit.test_list_dict.TestList) ... ok
test_list_unification_hint (jit.test_list_dict.TestList) ... ok
test_min_bool_list (jit.test_list_dict.TestList) ... ok
test_min_max_list (jit.test_list_dict.TestList) ... ok
test_min_max_single_list (jit.test_list_dict.TestList) ... ok
test_mutable_list_append (jit.test_list_dict.TestList) ... ok
test_mutable_list_append_2 (jit.test_list_dict.TestList) ... ok
test_mutable_list_append_if (jit.test_list_dict.TestList) ... ok
test_mutable_list_append_if_else (jit.test_list_dict.TestList) ... ok
test_mutable_list_append_loop (jit.test_list_dict.TestList) ... ok
test_mutable_list_append_loop_if (jit.test_list_dict.TestList) ... ok
test_mutable_list_clear (jit.test_list_dict.TestList) ... ok
test_mutable_list_clear_empty (jit.test_list_dict.TestList) ... ok
test_mutable_list_function_inline (jit.test_list_dict.TestList) ... ok
test_mutable_list_insert (jit.test_list_dict.TestList) ... ok
test_mutable_list_insert_neg_out_of_bounds (jit.test_list_dict.TestList) ... ok
test_mutable_list_insert_negative (jit.test_list_dict.TestList) ... ok
test_mutable_list_insert_out_of_bounds (jit.test_list_dict.TestList) ... ok
test_mutable_list_nested_loop (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop2 (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_at (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_at2 (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_at_negative (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_at_negative2 (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_empty (jit.test_list_dict.TestList) ... ok
test_mutable_list_pop_slice (jit.test_list_dict.TestList) ... ok
test_mutable_list_remove (jit.test_list_dict.TestList) ... ok
test_mutable_list_remove2 (jit.test_list_dict.TestList) ... ok
test_mutable_list_remove_not_existing (jit.test_list_dict.TestList) ... ok
test_mutable_list_remove_tensor (jit.test_list_dict.TestList) ... ok
test_mutable_list_reverse (jit.test_list_dict.TestList) ... ok
test_mutable_list_reverse_empty (jit.test_list_dict.TestList) ... ok
test_mutable_tensor_list_reverse (jit.test_list_dict.TestList) ... ok
test_no_element_type_annotation (jit.test_list_dict.TestList) ... ok
test_slice_index (jit.test_list_dict.TestList) ... ok
test_tensor_list_count (jit.test_list_dict.TestList) ... ok
test_tensor_list_count_not_existing (jit.test_list_dict.TestList) ... ok
test_tensor_list_index (jit.test_list_dict.TestList) ... ok
test_tensor_list_index_not_existing (jit.test_list_dict.TestList) ... ok
test_to_list (jit.test_list_dict.TestList)
Unit tests for Tensor.tolist() function. ... ok
test_to_list_gpu (jit.test_list_dict.TestList)
GPU tests for Tensor.tolist() function. ... ok
test_bump_numeric_counter (jit.test_logging.TestLogging) ... ok
test_counter_aggregation (jit.test_logging.TestLogging) ... ok
test_logging_levels_set (jit.test_logging.TestLogging) ... ok
test_time_measurement_counter (jit.test_logging.TestLogging) ... ok
test_time_measurement_counter_script (jit.test_logging.TestLogging) ... ok
test_trace_numeric_counter (jit.test_logging.TestLogging) ... ok
test_always_alive_values (jit.test_freezing.TestMKLDNNReinplacing) ... ok
test_merge_liveness (jit.test_freezing.TestMKLDNNReinplacing) ... ok
test_successful (jit.test_freezing.TestMKLDNNReinplacing) ... ok
test_switch_inputs_to_inplace (jit.test_freezing.TestMKLDNNReinplacing) ... ok
test_broadcasting_list (jit.test_misc.TestMisc) ... ok
test_dataclass_error (jit.test_misc.TestMisc) ... skipped '`dataclasses` module not present on < 3.7'
test_export_opnames_interface (jit.test_misc.TestMisc) ... ok
test_future_isinstance (jit.test_misc.TestMisc) ... ok
test_if_returning_any (jit.test_misc.TestMisc) ... ok
test_joined_str (jit.test_misc.TestMisc) ... ok
test_kwarg_support (jit.test_misc.TestMisc) ... ok
test_str_refine_any (jit.test_misc.TestMisc) ... ok
test_subexpression_Dict_int_Future (jit.test_misc.TestMisc) ... ok
test_subexpression_Future_annotate (jit.test_misc.TestMisc) ... ok
test_subexpression_List_Future (jit.test_misc.TestMisc) ... ok
test_subexpression_Optional (jit.test_misc.TestMisc) ... ok
test_subexpression_Tuple_int_int_Future (jit.test_misc.TestMisc) ... ok
test_tuple_subscripted_assign (jit.test_misc.TestMisc) ... ok
test_call_script_fn_from_traced_module (jit.test_tracer.TestMixTracingScripting) ... ok
test_call_script_module_from_traced_module (jit.test_tracer.TestMixTracingScripting) ... ok
test_call_traced_fn_from_script_fn (jit.test_tracer.TestMixTracingScripting) ... ok
test_call_traced_mod_from_script_fn (jit.test_tracer.TestMixTracingScripting) ... ok
test_call_tracing_fn_from_script_module (jit.test_tracer.TestMixTracingScripting) ... ok
test_call_tracing_mod_from_script_module (jit.test_tracer.TestMixTracingScripting) ... ok
test_script_inline_trace_multiple_args (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_dict_mix_script (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_hierarchy (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_linear (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_mixed_by_script_with_dict_output (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_of_script (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_parameter (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_returning_dict_with_tensor_tuples (jit.test_tracer.TestMixTracingScripting)
Tracing over a module returning a dictionary whose values are tuples of tensors ... ok
test_trace_script (jit.test_tracer.TestMixTracingScripting) ... ok
test_trace_script_returning_complex_dict (jit.test_tracer.TestMixTracingScripting)
Tracing over a script function returning a dictionary should work. ... ok
test_trace_with_size (jit.test_tracer.TestMixTracingScripting) ... ok
test_traced_module_contains_scripted_interface_types (jit.test_tracer.TestMixTracingScripting) ... ok
test_traced_module_implements_interface (jit.test_tracer.TestMixTracingScripting) ... ok
test_tracing_indexing (jit.test_tracer.TestMixTracingScripting) ... ok
test_tracing_slicing (jit.test_tracer.TestMixTracingScripting) ... ok
test_alexnet (jit.test_models.TestModels) ... ok
test_dcgan_models (jit.test_models.TestModels) ... ok
test_dcgan_models_cuda (jit.test_models.TestModels) ... ok
test_mnist (jit.test_models.TestModels) ... ok
test_mnist_cuda (jit.test_models.TestModels) ... "Cannot find Symbol"
test_jit failed! Received signal: SIGIOT
Running test_jit_disabled ... [2021-10-12 10:16:25.147288]
Executing ['/opt/conda/bin/python3.6', 'test_jit_disabled.py', '-v'] ... [2021-10-12 10:16:25.147358]
test_attribute (__main__.TestJitDisabled) ... ok
test_recursive_script (__main__.TestJitDisabled) ... ok
test_script_module_construction (__main__.TestJitDisabled) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.186s

OK
Running test_jit_fuser_te ... [2021-10-12 10:16:27.564318]
Executing ['/opt/conda/bin/python3.6', 'test_jit_fuser_te.py', '-v'] ... [2021-10-12 10:16:27.564392]
test_autodiff_fallback (jit.test_fuser_common.TestFuserCommon) ... ok
test_failures_matmul_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported___getitem___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported___rmatmul___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported___rpow___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported___rsub___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_acosh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_addbmm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_addcdiv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_addmm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_addmv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_addr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_all_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_amax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_amin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_aminmax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_angle_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_any_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_argmax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_argmin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_asinh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_atanh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_baddbmm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_bitwise_left_shift_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_bitwise_right_shift_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_bmm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_broadcast_to_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cat_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cdist_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cholesky_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cholesky_inverse_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_chunk_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_clone_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_complex_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_conj_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_conj_physical_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_contiguous_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_copysign_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_corrcoef_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_count_nonzero_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cov_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cross_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cummax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cummin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cumprod_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cumsum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_cumulative_trapezoid_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_deg2rad_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_diag_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_diag_embed_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_diagonal_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_diff_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_digamma_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_dist_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_dot_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_dsplit_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_dstack_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_eig_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_einsum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_erfinv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_exp2_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_fft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_fftn_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_hfft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_ifft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_ifftn_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_ihfft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_irfft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_irfftn_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_rfft_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fft_rfftn_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fill__cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_flip_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fliplr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_flipud_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_float_power_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_floor_divide_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fmax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_fmin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_frac_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_frexp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_gather_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_geqrf_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_gradient_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_hsplit_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_hstack_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_hypot_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_i0_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_igamma_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_igamma_grad_other_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_igammac_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_igammac_grad_other_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_index_add_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_index_copy_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_index_fill_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_index_put_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_index_select_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_inner_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_inverse_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_isin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_kron_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_kthvalue_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_cholesky_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_cholesky_ex_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_cond_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_det_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_eig_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_eigh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_eigvals_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_eigvalsh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_householder_product_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_inv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_inv_ex_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_lstsq_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_matrix_norm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_matrix_power_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_matrix_rank_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_matrix_rank_hermitian_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_multi_dot_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_norm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_pinv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_pinv_hermitian_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_qr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_slogdet_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_solve_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_svd_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_svdvals_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_tensorinv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_linalg_vector_norm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_log_softmax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_log_softmax_dtype_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logaddexp2_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logaddexp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logcumsumexp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logdet_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logical_not_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logit_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_logsumexp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_lu_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_lu_solve_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_lu_unpack_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_masked_scatter_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_masked_select_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_matrix_exp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_max_reduction_no_dim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_max_reduction_with_dim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_maximum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_median_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_meshgrid_list_of_tensors_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_meshgrid_variadic_tensors_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_min_reduction_no_dim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_min_reduction_with_dim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_minimum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_mode_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_movedim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_msort_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_mv_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nan_to_num_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nanmedian_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nanquantile_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nansum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_narrow_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nextafter_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_adaptive_avg_pool2d_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_avg_pool2d_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_conv_transpose2d_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_cosine_similarity_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_grid_sample_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_area_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_bicubic_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_bilinear_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_linear_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_nearest_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_interpolate_trilinear_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_layer_norm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_logsigmoid_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_mse_loss_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_nll_loss_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_normalize_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_pad_circular_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_pad_constant_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_pad_reflect_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_pad_replicate_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_nn_functional_unfold_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_norm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_norm_fro_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_norm_inf_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_norm_nuc_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_ormqr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_outer_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_pinverse_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polar_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polygamma_polygamma_n_0_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polygamma_polygamma_n_1_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polygamma_polygamma_n_2_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polygamma_polygamma_n_3_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_polygamma_polygamma_n_4_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_positive_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_prod_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_put_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_qr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_quantile_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_rad2deg_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_ravel_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_renorm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_repeat_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_reshape_as_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_resize__cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_resize_as__cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_resolve_conj_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_resolve_neg_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_roll_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_rot90_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_rsub_rsub_scalar_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_rsub_rsub_tensor_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_scatter_add_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_scatter_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_select_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_sgn_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_sign_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_signbit_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_sinc_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_softmax_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_softmax_with_dtype_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_solve_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_sort_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_entr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_erfcx_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_i0e_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_i1_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_i1e_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_ndtr_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_ndtri_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_xlog1py_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_zeta_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_special_zeta_grad_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_split_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_split_list_args_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_split_with_sizes_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_square_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_squeeze_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_stack_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_std_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_std_mean_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_svd_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_symeig_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_t_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_take_along_dim_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_take_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_tensor_split_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_tensordot_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_tile_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_to_sparse_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_topk_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_trace_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_trapezoid_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_trapz_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_triangular_solve_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_tril_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_triu_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_unfold_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_var_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_var_mean_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_vdot_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_view_as_complex_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_view_as_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_vsplit_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_vstack_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_xlogy_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_unsupported_zero__cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working___radd___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working___rdiv___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working___rmod___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working___rmul___cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_abs_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_acos_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_add_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_addcmul_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_addmm_decomposed_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_asin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_atan2_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_atan_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_ceil_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_clamp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_clamp_scalar_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_cos_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_cosh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_div_floor_rounding_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_div_no_rounding_mode_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_div_trunc_rounding_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_eq_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_erf_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_erfc_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_exp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_expand_as_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_expand_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_expm1_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_floor_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_fmod_autodiffed_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_fmod_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_ge_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_gt_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_le_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_lerp_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_lgamma_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_log10_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_log1p_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_log2_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_log_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_lt_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_masked_fill_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_max_binary_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_mean_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_min_binary_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_mm_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_mul_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_ne_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_neg_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_gelu_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_hardshrink_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_hardswish_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_hardtanh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_leaky_relu_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_relu6_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_relu_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_nn_functional_softplus_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_permute_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_pow_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_reciprocal_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_remainder_autodiffed_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_remainder_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_reshape_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_round_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_rsqrt_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sigmoid_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sin_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sinh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sqrt_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sub_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_sum_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_tan_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_tanh_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_transpose_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_true_divide_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_trunc_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_unsqueeze_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_view_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_working_where_cuda_float32 (__main__.TestNNCOpInfoCUDA) ... skipped 'Compiles with TensorExprKernel'
test_abs (__main__.TestTEFuser) ... ok
test_adaptive_avg_pool2d (__main__.TestTEFuser) ... ok
test_add_bool (__main__.TestTEFuser) ... ok
test_addcmul (__main__.TestTEFuser) ... ok
test_arg_configurations_smoke (__main__.TestTEFuser) ... ok
test_batch_norm (__main__.TestTEFuser) ... ok
test_binary_div_ops (__main__.TestTEFuser) ... ok
test_binary_ops (__main__.TestTEFuser) ... ok
test_binary_pow (__main__.TestTEFuser) ... ok
test_binary_tensor_scalar_ops (__main__.TestTEFuser) ... ok
test_bitwise_ops (__main__.TestTEFuser) ... ok
test_broadcast (__main__.TestTEFuser) ... ok
test_cat_2k_args (__main__.TestTEFuser) ... ok
test_checks_cat_inputs (__main__.TestTEFuser) ... ok
test_chunk (__main__.TestTEFuser) ... ok
test_chunk_correctness (__main__.TestTEFuser) ... ok
test_chunk_distributes (__main__.TestTEFuser) ... ok
test_chunk_motion_deduplicates_inputs (__main__.TestTEFuser) ... ok
test_chunk_mul_one (__main__.TestTEFuser) ... ok
test_chunk_multiple (__main__.TestTEFuser) ... ok
test_clamp (__main__.TestTEFuser) ... ok
test_clamp_double (__main__.TestTEFuser) ... ok
test_clamp_int (__main__.TestTEFuser) ... ok
test_comparison_eq_ne (__main__.TestTEFuser) ... ok
test_comparison_ge_le (__main__.TestTEFuser) ... ok
test_comparison_gt_lt (__main__.TestTEFuser) ... ok
test_concat (__main__.TestTEFuser) ... ok
test_concat_invariant (__main__.TestTEFuser) ... ok
test_conv2d (__main__.TestTEFuser) ... ok
test_conv2d_depthwise (__main__.TestTEFuser) ... skipped 'Too slow to run with the TE interpreter'
test_cuda_half (__main__.TestTEFuser) ... skipped 'no half support with profiling on'
test_dims (__main__.TestTEFuser) ... ok
test_disabled (__main__.TestTEFuser) ... ok
test_div_bool (__main__.TestTEFuser) ... ok
test_dynamic_cat (__main__.TestTEFuser) ... ok
test_eq_unsqueeze_type_as (__main__.TestTEFuser) ... ok
test_erf (__main__.TestTEFuser) ... ok
test_exp (__main__.TestTEFuser) ... ok
test_fusion_reuse_multi_gpu (__main__.TestTEFuser) ... ok
test_hardsigmoid_fwd_bwd (__main__.TestTEFuser) ... ok
test_hardswish_fwd_bwd (__main__.TestTEFuser) ... ok
test_isnan (__main__.TestTEFuser) ... ok
test_kernel_cache_multi_gpu (__main__.TestTEFuser) ... ok
test_lerp (__main__.TestTEFuser) ... ok
test_list_ops (__main__.TestTEFuser) ... skipped "FIXME: fuser doesn't include ListConstruct nodes to the group causing a failure"
test_lstm (__main__.TestTEFuser) ... ok
test_lstm_concat (__main__.TestTEFuser) ... ok
test_lstm_gates_permutations (__main__.TestTEFuser) ... ok
test_lstm_traced (__main__.TestTEFuser) ... ok
test_masked_fill (__main__.TestTEFuser) ... skipped 'Temporarily disabled'
test_matmul (__main__.TestTEFuser) ... ok
test_milstm (__main__.TestTEFuser) ... ok
test_minmax (__main__.TestTEFuser) ... ok
test_minmax_int_ops (__main__.TestTEFuser) ... ok
test_mul_bool (__main__.TestTEFuser) ... ok
test_neg_pow (__main__.TestTEFuser) ... ok
test_nonzero_device_cuda (__main__.TestTEFuser) ... ok
test_nop (__main__.TestTEFuser) ... ok
test_rand_broadcast_cuda (__main__.TestTEFuser) ... skipped 'rand_like is not supported yet'
test_rand_cuda (__main__.TestTEFuser) ... skipped 'rand_like is not supported yet'
test_rand_diamond (__main__.TestTEFuser) ... skipped 'rand_like is not supported yet'
test_relu (__main__.TestTEFuser) ... ok
test_relu_fwd_bwd (__main__.TestTEFuser) ... ok
test_remove_output_used_only_in_size (__main__.TestTEFuser) ... ok
test_scalar (__main__.TestTEFuser) ... ok
test_scalar_arg (__main__.TestTEFuser) ... ok
test_scalar_only_inputs (__main__.TestTEFuser) ... ok
test_small_constant (__main__.TestTEFuser) ... ok
test_sub_gt_and (__main__.TestTEFuser) ... ok
test_sum_dim (__main__.TestTEFuser) ... ok
test_sum_keepdim_cast (__main__.TestTEFuser) ... ok
test_sum_simple (__main__.TestTEFuser) ... ok
test_superslomo (__main__.TestTEFuser) ... ok
test_tensor_scalar_ops (__main__.TestTEFuser) ... ok
test_ternary_norm_ops (__main__.TestTEFuser) ... ok
test_ternary_ops (__main__.TestTEFuser) ... ok
test_threshold (__main__.TestTEFuser) ... ok
test_to_device (__main__.TestTEFuser) ... ok
test_torch_to (__main__.TestTEFuser) ... ok
test_type_as_cat (__main__.TestTEFuser) ... ok
test_typecheck (__main__.TestTEFuser) ... ok
test_unary_ops (__main__.TestTEFuser) ... ok
test_unrolled_cat (__main__.TestTEFuser) ... ok
test_unsqueeze_size_calculation (__main__.TestTEFuser) ... ok
test_unsqueeze_var_dim (__main__.TestTEFuser) ... ok
test_unsupported_dtypes (__main__.TestTEFuser) ... /opt/conda/lib/python3.6/site-packages/torch/jit/_trace.py:493: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Copy.cpp:244.)
  orig.double(),
ok
test_where_and_typing (__main__.TestTEFuser) ... ok
test_where_ops (__main__.TestTEFuser) ... ok
test_zero_element_tensors (__main__.TestTEFuser) ... ok

----------------------------------------------------------------------
Ran 441 tests in 476.225s

OK (skipped=358)
Running test_license ... [2021-10-12 10:24:28.921251]
Executing ['/opt/conda/bin/python3.6', 'test_license.py', '-v'] ... [2021-10-12 10:24:28.921333]
test_distinfo_license (__main__.TestLicense)
If run when pytorch is installed via a wheel, the license will be in ... skipped 'no installation in site-package to test'
test_license_for_wheel (__main__.TestLicense) ... skipped 'can only be run in a source tree'

----------------------------------------------------------------------
Ran 2 tests in 0.000s

OK (skipped=2)
Running test_linalg ... [2021-10-12 10:24:31.145641]
Executing ['/opt/conda/bin/python3.6', 'test_linalg.py', '-v'] ... [2021-10-12 10:24:31.145719]
test_addbmm_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addbmm_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_addbmm_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_addbmm_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_addbmm_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addbmm_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addmm_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addmm_sizes_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_addmm_sizes_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_addmm_sizes_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addmm_sizes_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addmv_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addmv_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_addmv_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_addmv_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addmv_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addmv_rowmajor_colmajor_incx_incy_lda_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addmv_rowmajor_colmajor_incx_incy_lda_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_addmv_rowmajor_colmajor_incx_incy_lda_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addmv_rowmajor_colmajor_incx_incy_lda_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_bool_cuda_bool (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_float_and_complex_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_integral_cuda_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_integral_cuda_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_integral_cuda_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_integral_cuda_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_integral_cuda_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bfloat16_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_bool_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex128_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_complex64_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float16_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float32_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_float64_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int16_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int32_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int64_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_int8_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bfloat16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_bool_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex128_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_complex64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_float64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int16_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int32_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int64_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_int8_uint8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_bool (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_complex128 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_complex64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_float16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_float32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_float64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_int16 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_int32 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_int64 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_int8 (__main__.TestLinalgCUDA) ... ok
test_addr_type_promotion_cuda_uint8_uint8_uint8 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_bfloat16 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_baddbmm_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_blas_alpha_beta_empty_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_blas_alpha_beta_empty_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_blas_alpha_beta_empty_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_blas_alpha_beta_empty_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_blas_alpha_beta_empty_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_blas_empty_cuda (__main__.TestLinalgCUDA) ... ok
test_blas_mv_large_input_cuda (__main__.TestLinalgCUDA) ... skipped 'Only runs on cpu'
test_blas_nan_out_cuda_bfloat16 (__main__.TestLinalgCUDA) ... test_linalg.py:5333: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  self.assertEqual(torch.mv(nm, _m), torch.mv(nm, _m, out=_m_out))
ok
test_blas_nan_out_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_blas_nan_out_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_blas_nan_out_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_blas_nan_out_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_blas_nan_out_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_bmm_cuda_bfloat16 (__main__.TestLinalgCUDA) ... test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [1, 23, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [1, 0, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [1, 0, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [0, 23, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [0, 23, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [0, 0, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 12], which does not match the required output shape [0, 0, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [10, 23, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [10, 0, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [10, 0, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [0, 23, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [0, 23, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [0, 0, 12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
test_linalg.py:6279: UserWarning: An output with one or more elements was resized since it had shape [10, 23, 12], which does not match the required output shape [0, 0, 0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.bmm(b1, b2, out=res2)
ok
test_bmm_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_bmm_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_bmm_cuda_float16 (__main__.TestLinalgCUDA) ... ok
test_bmm_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_bmm_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_broadcast_batched_matmul_cuda (__main__.TestLinalgCUDA) ... test_linalg.py:7423: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [24, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.matmul(l, r, out=out)
test_linalg.py:7423: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [3, 8, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.matmul(l, r, out=out)
test_linalg.py:7423: UserWarning: An output with one or more elements was resized since it had shape [3, 8, 1], which does not match the required output shape [24, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.matmul(l, r, out=out)
test_linalg.py:7423: UserWarning: An output with one or more elements was resized since it had shape [3, 1], which does not match the required output shape [3, 1, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.matmul(l, r, out=out)
test_linalg.py:7423: UserWarning: An output with one or more elements was resized since it had shape [3, 8, 1], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.matmul(l, r, out=out)
ok
test_broadcast_fused_matmul_cuda (__main__.TestLinalgCUDA) ... ok
test_chain_matmul_cuda_float64 (__main__.TestLinalgCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/functional.py:1497: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:732.)
  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]
ok
test_cholesky_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_non_pd_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_non_pd_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_non_pd_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_non_pd_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_out_info_error_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_out_info_error_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_out_info_error_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_ex_out_info_error_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_hermitian_grad_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_hermitian_grad_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_inverse_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_autograd_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_autograd_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_broadcasting_cuda_complex128 (__main__.TestLinalgCUDA) ... test_linalg.py:3099: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
L = torch.cholesky(A)
should be replaced with
L = torch.linalg.cholesky(A)
and
U = torch.cholesky(A, upper=True)
should be replaced with
U = torch.linalg.cholesky(A).transpose(-2, -1).conj().
This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)
  L = torch.cholesky(A, upper)
ok
test_cholesky_solve_batched_broadcasting_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_broadcasting_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_broadcasting_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_many_batches_cuda_complex128 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cholesky_solve_batched_many_batches_cuda_complex64 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cholesky_solve_batched_many_batches_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cholesky_solve_batched_many_batches_cuda_float64 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cholesky_solve_batched_non_contiguous_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_non_contiguous_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_non_contiguous_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_batched_non_contiguous_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_out_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_out_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_out_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cholesky_solve_out_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cond_cuda_complex128 (__main__.TestLinalgCUDA) ... 
Intel MKL ERROR: Parameter 4 was incorrect on entry to DLASCL.

Intel MKL ERROR: Parameter 5 was incorrect on entry to DLASCL.
ok
test_cond_cuda_complex64 (__main__.TestLinalgCUDA) ... 
Intel MKL ERROR: Parameter 4 was incorrect on entry to DLASCL.

Intel MKL ERROR: Parameter 5 was incorrect on entry to DLASCL.
ok
test_cond_cuda_float32 (__main__.TestLinalgCUDA) ... 
Intel MKL ERROR: Parameter 4 was incorrect on entry to DLASCL.

Intel MKL ERROR: Parameter 5 was incorrect on entry to DLASCL.
ok
test_cond_cuda_float64 (__main__.TestLinalgCUDA) ... 
Intel MKL ERROR: Parameter 4 was incorrect on entry to DLASCL.

Intel MKL ERROR: Parameter 5 was incorrect on entry to DLASCL.
ok
test_cond_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_cond_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_cond_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_cond_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_cross_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'Only runs on cpu'
test_cross_errors_cuda (__main__.TestLinalgCUDA) ... ok
test_cross_with_and_without_dim_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'Only runs on cpu'
test_det_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_det_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_det_logdet_slogdet_batched_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_det_logdet_slogdet_cuda_float64 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_dot_invalid_args_cuda (__main__.TestLinalgCUDA) ... ok
test_dot_vs_numpy_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_dot_vs_numpy_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eig_check_magma_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_eig_compare_backends_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eig_compare_backends_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eig_compare_backends_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eig_compare_backends_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eig_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eig_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eig_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eig_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eig_numpy_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eig_numpy_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigh_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigh_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigh_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigh_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigh_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigh_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigh_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigh_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigh_hermitian_grad_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigh_hermitian_grad_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigh_lower_uplo_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigh_lower_uplo_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigh_lower_uplo_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigh_lower_uplo_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigh_non_contiguous_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigh_non_contiguous_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigh_non_contiguous_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigh_non_contiguous_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvals_compare_backends_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigvals_compare_backends_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigvals_compare_backends_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigvals_compare_backends_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvals_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigvals_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigvals_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigvals_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvals_numpy_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigvals_numpy_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... test_linalg.py:1105: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Copy.cpp:244.)
  out = torch.empty_like(t).to(real_dtype)
ok
test_eigvalsh_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_non_contiguous_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_non_contiguous_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_non_contiguous_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_eigvalsh_non_contiguous_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_einsum_corner_cases_cuda (__main__.TestLinalgCUDA) ... ok
test_einsum_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_einsum_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_einsum_error_cases_cuda (__main__.TestLinalgCUDA) ... ok
test_einsum_random_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_einsum_random_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_einsum_sublist_format_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_einsum_sublist_format_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_geqrf_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_geqrf_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_geqrf_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_geqrf_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_householder_product_cuda_complex128 (__main__.TestLinalgCUDA) ... skipped 'cuSOLVER not available'
test_householder_product_cuda_complex64 (__main__.TestLinalgCUDA) ... skipped 'cuSOLVER not available'
test_householder_product_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'cuSOLVER not available'
test_householder_product_cuda_float64 (__main__.TestLinalgCUDA) ... skipped 'cuSOLVER not available'
test_householder_product_errors_and_warnings_cuda (__main__.TestLinalgCUDA) ... skipped 'cuSOLVER not available'
test_inner_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_inner_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_inv_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_inv_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_inv_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_inv_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_inv_ex_info_device_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_inv_ex_info_device_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_inv_ex_info_device_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_inv_ex_info_device_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_inv_ex_singular_cuda_complex128 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inv_ex_singular_cuda_complex64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inv_ex_singular_cuda_float32 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inv_ex_singular_cuda_float64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inverse_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_inverse_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_inverse_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_inverse_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_inverse_errors_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_inverse_errors_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_inverse_errors_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_inverse_errors_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_inverse_errors_large_cuda_complex128 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inverse_errors_large_cuda_complex64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inverse_errors_large_cuda_float32 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inverse_errors_large_cuda_float64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_inverse_many_batches_cuda_complex128 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_inverse_many_batches_cuda_complex64 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_inverse_many_batches_cuda_float32 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_inverse_many_batches_cuda_float64 (__main__.TestLinalgCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_kron_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_kron_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_kron_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_kron_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_kron_empty_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_kron_empty_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_kron_empty_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_kron_empty_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_kron_errors_and_warnings_cuda_complex128 (__main__.TestLinalgCUDA) ... test_linalg.py:1268: UserWarning: An output with one or more elements was resized since it had shape [3, 3], which does not match the required output shape [6, 6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.kron(a, b, out=out)
ok
test_kron_errors_and_warnings_cuda_complex64 (__main__.TestLinalgCUDA) ... test_linalg.py:1268: UserWarning: An output with one or more elements was resized since it had shape [3, 3], which does not match the required output shape [6, 6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.kron(a, b, out=out)
ok
test_kron_errors_and_warnings_cuda_float32 (__main__.TestLinalgCUDA) ... test_linalg.py:1268: UserWarning: An output with one or more elements was resized since it had shape [3, 3], which does not match the required output shape [6, 6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.kron(a, b, out=out)
ok
test_kron_errors_and_warnings_cuda_float64 (__main__.TestLinalgCUDA) ... test_linalg.py:1268: UserWarning: An output with one or more elements was resized since it had shape [3, 3], which does not match the required output shape [6, 6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.kron(a, b, out=out)
ok
test_kron_non_contiguous_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_kron_non_contiguous_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_kron_non_contiguous_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_kron_non_contiguous_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_lapack_empty_cuda (__main__.TestLinalgCUDA) ... test_linalg.py:7938: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
L, _ = torch.eig(A)
should be replaced with
L_complex = torch.linalg.eigvals(A)
and
L, V = torch.eig(A, eigenvectors=True)
should be replaced with
L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2894.)
  for shape in args))
test_linalg.py:7938: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)
  for shape in args))
test_linalg.py:7938: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1937.)
  for shape in args))
test_linalg.py:7967: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.
torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).
To get the qr decomposition consider using torch.linalg.qr.
The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.
The unpacking of the solution, as in
X, _ = torch.lstsq(B, A).solution[:A.size(1)]
should be replaced with
X = torch.linalg.lstsq(A, B).solution (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:3668.)
  self.assertRaises(RuntimeError, lambda: torch.lstsq(torch.randn(0, 0), torch.randn(0, 0)))
ok
test_linalg_lstsq_batch_broadcasting_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_batch_broadcasting_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_batch_broadcasting_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_batch_broadcasting_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_input_checks_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_input_checks_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_input_checks_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_linalg_lstsq_input_checks_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_linalg_qr_autograd_errors_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_linalg_svd_compute_uv_cuda_complex128 (__main__.TestLinalgCUDA) ... ok
test_linalg_svd_compute_uv_cuda_complex64 (__main__.TestLinalgCUDA) ... ok
test_linalg_svd_compute_uv_cuda_float32 (__main__.TestLinalgCUDA) ... ok
test_linalg_svd_compute_uv_cuda_float64 (__main__.TestLinalgCUDA) ... ok
test_linear_algebra_scalar_raises_cuda (__main__.TestLinalgCUDA) ... ok
test_lobpcg_basic_cuda_float64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_lobpcg_ortho_cuda_float64 (__main__.TestLinalgCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_lobpcg_scipy_cuda_float64 (__main__.TestLinalgCUDA)
Compare torch and scipy.sparse.linalg implementations of lobpcg ... skipped 'Scipy not found or older than 1.4.1'
test_lobpcg_torchscript_cuda_float64 (__main__.TestLinalgCUDA) ... skipped 'Only runs on cpu'
test_lstsq_cuda_float64 (__main__.TestLinalgCUDA) ... test_linalg.py:7861: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.
torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).
To get the qr decomposition consider using torch.linalg.qr.
The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.
The unpacking of the solution, as in
X, _ = torch.lstsq(B, A).solution[:A.size(1)]
should be replaced with
X = torch.linalg.lstsq(A, B).solution (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/BatchLinearAlgebra.cpp:3108.)
  res1 = torch.lstsq(b, a)[0]
ok
test_lu_cuda_complex128 (__main__.TestLinalgCUDA) ... "Cannot find Symbol"
test_linalg failed! Received signal: SIGIOT
Running test_logging ... [2021-10-12 10:34:30.253265]
Executing ['/opt/conda/bin/python3.6', 'test_logging.py', '-v'] ... [2021-10-12 10:34:30.253323]
testApiUsage (__main__.LoggingTest) ... test_logging.py:13: DeprecationWarning: Please use assertRegex instead.
  self.assertRegexpMatches(s, "PYTORCH_API_USAGE.*import")
test_logging.py:16: DeprecationWarning: Please use assertNotRegex instead.
  self.assertNotRegexpMatches(s, "PYTORCH_API_USAGE")
ok

----------------------------------------------------------------------
Ran 1 test in 3.415s

OK
Running test_mkldnn ... [2021-10-12 10:34:35.989248]
Executing ['/opt/conda/bin/python3.6', 'test_mkldnn.py', '-v'] ... [2021-10-12 10:34:35.989328]
test_0_dimension_tensor (__main__.TestMkldnn) ... ok
test_adaptive_avg_pool2d (__main__.TestMkldnn) ... ok
test_adaptive_avg_pool2d_bf16 (__main__.TestMkldnn) ... ok
test_add (__main__.TestMkldnn) ... ok
test_autograd_from_mkldnn (__main__.TestMkldnn) ... ok
test_autograd_to_mkldnn (__main__.TestMkldnn) ... ok
test_avg_pool2d (__main__.TestMkldnn) ... ok
test_avg_pool2d_bf16 (__main__.TestMkldnn) ... ok
test_avg_pool2d_stride_none (__main__.TestMkldnn) ... ok
test_avg_pool3d (__main__.TestMkldnn) ... ok
test_avg_pool3d_bf16 (__main__.TestMkldnn) ... ok
test_batch_norm_2d (__main__.TestMkldnn) ... /opt/conda/lib/python3.6/site-packages/torch/jit/_trace.py:736: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.
  "The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is."
ok
test_batch_norm_2d_bf16 (__main__.TestMkldnn) ... ok
test_batch_norm_3d (__main__.TestMkldnn) ... ok
test_batch_norm_3d_bf16 (__main__.TestMkldnn) ... ok
test_clone (__main__.TestMkldnn) ... ok
test_conv1d (__main__.TestMkldnn) ... ok
test_conv1d_bf16 (__main__.TestMkldnn) ... ok
test_conv2d (__main__.TestMkldnn) ... ok
test_conv2d_bf16 (__main__.TestMkldnn) ... ok
test_conv2d_legacy_jit_model (__main__.TestMkldnn) ... ok
test_conv3d (__main__.TestMkldnn) ... ok
test_conv3d_bf16 (__main__.TestMkldnn) ... ok
test_conversion (__main__.TestMkldnn) ... ok
test_copy (__main__.TestMkldnn) ... ok
test_detach (__main__.TestMkldnn) ... ok
test_empty (__main__.TestMkldnn) ... ok
test_gelu (__main__.TestMkldnn) ... ok
test_gelu_bf16 (__main__.TestMkldnn) ... ok
test_is_mkldnn (__main__.TestMkldnn) ... ok
test_is_mkldnn_jit (__main__.TestMkldnn) ... ok
test_legacy_new_failure (__main__.TestMkldnn) ... ok
test_linear (__main__.TestMkldnn) ... ok
test_linear_backward (__main__.TestMkldnn) ... ok
test_linear_bf16 (__main__.TestMkldnn) ... ok
test_linear_non_contiguous_weight (__main__.TestMkldnn) ... ok
test_max_pool2d (__main__.TestMkldnn) ... ok
test_max_pool2d_bf16 (__main__.TestMkldnn) ... ok
test_max_pool2d_stride_none (__main__.TestMkldnn) ... ok
test_max_pool3d (__main__.TestMkldnn) ... ok
test_max_pool3d_bf16 (__main__.TestMkldnn) ... ok
test_max_pool_unsupported (__main__.TestMkldnn) ... ok
test_mul (__main__.TestMkldnn) ... ok
test_relu (__main__.TestMkldnn) ... ok
test_relu_ (__main__.TestMkldnn) ... ok
test_relu_bf16 (__main__.TestMkldnn) ... ok
test_relu_inplace_bf16 (__main__.TestMkldnn) ... ok
test_repr (__main__.TestMkldnn) ... ok
test_reshape (__main__.TestMkldnn) ... ok
test_reshape_backward (__main__.TestMkldnn) ... ok
test_reshape_blocked_format (__main__.TestMkldnn) ... ok
test_resnet18 (__main__.TestMkldnn) ... ok
test_resnext50_32x4d (__main__.TestMkldnn) ... ok
test_set_data_tensorimpl_type (__main__.TestMkldnn) ... ok
test_sigmoid (__main__.TestMkldnn) ... ok
test_softmax (__main__.TestMkldnn) ... ok
test_tanh (__main__.TestMkldnn) ... ok
test_transpose (__main__.TestMkldnn) ... ok
test_unsupported (__main__.TestMkldnn) ... ok
test_view (__main__.TestMkldnn) ... ok
test_zero_ (__main__.TestMkldnn) ... ok

----------------------------------------------------------------------
Ran 61 tests in 10.979s

OK
Running test_mobile_optimizer ... [2021-10-12 10:34:49.424929]
Executing ['/opt/conda/bin/python3.6', 'test_mobile_optimizer.py', '-v'] ... [2021-10-12 10:34:49.425018]
test_clone_module_with_class (__main__.TestOptimizer) ... ok
test_generate_mobile_module_lints (__main__.TestOptimizer) ... ok
test_hoist_conv_packed_params (__main__.TestOptimizer) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
ok
test_mobilenet_optimize_for_mobile (__main__.TestOptimizer) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:139: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /var/lib/jenkins/pytorch/c10/core/TensorImpl.h:1378.)
  return callable(*args, **kwargs)
ok
test_optimize_for_mobile (__main__.TestOptimizer) ... ok
test_preserve_bundled_inputs_methods (__main__.TestOptimizer) ... ok
test_quantized_conv_no_asan_failures (__main__.TestOptimizer) ... ok

----------------------------------------------------------------------
Ran 7 tests in 2.205s

OK
Running test_model_dump ... [2021-10-12 10:34:54.591591]
Executing ['/opt/conda/bin/python3.6', 'test_model_dump.py', '-v'] ... [2021-10-12 10:34:54.591670]
test_inline_skeleton (__main__.TestModelDump) ... skipped 'importlib.resources was new in 3.7'
test_invalid_json (__main__.TestModelDump) ... ok
test_main (__main__.TestModelDump) ... skipped 'importlib.resources was new in 3.7'
test_memory_computation (__main__.TestModelDump) ... skipped 'importlib.resources was new in 3.7'
test_model_with_lists (__main__.TestModelDump) ... ok
test_optimized_quantized_model (__main__.TestModelDump) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/nn/quantized/modules/__init__.py:52: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return torch.quantize_per_tensor(X, float(self.scale),
/opt/conda/lib/python3.6/site-packages/torch/nn/quantized/modules/__init__.py:53: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  int(self.zero_point), self.dtype)
ok
test_quantized_model (__main__.TestModelDump) ... ok
test_scripted_model (__main__.TestModelDump) ... ok
test_traced_model (__main__.TestModelDump) ... ok

----------------------------------------------------------------------
Ran 9 tests in 0.628s

OK (skipped=3)
Running test_module_init ... [2021-10-12 10:34:57.763736]
Executing ['/opt/conda/bin/python3.6', 'test_module_init.py', '-v'] ... [2021-10-12 10:34:57.763813]
/opt/conda/lib/python3.6/site-packages/torch/testing/_deprecated.py:32: FutureWarning: torch.testing.floating_types() is deprecated and will be removed in a future release. This call to floating_types(...) can be replaced with (torch.float32, torch.float64).
  warnings.warn(msg, FutureWarning)
test_nn_AdaptiveAvgPool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveAvgPool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveAvgPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveAvgPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveAvgPool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveAvgPool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveLogSoftmaxWithLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveLogSoftmaxWithLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AdaptiveMaxPool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AlphaDropout_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AlphaDropout_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_AvgPool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_BCELoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_BCELoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_BCEWithLogitsLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_BCEWithLogitsLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_BatchNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Bilinear_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Bilinear_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CELU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CELU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CTCLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CTCLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ChannelShuffle_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ChannelShuffle_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConstantPad3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Conv3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ConvTranspose3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CosineEmbeddingLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CosineEmbeddingLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CosineSimilarity_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CosineSimilarity_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CrossEntropyLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CrossEntropyLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_CrossMapLRN2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_CrossMapLRN2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Dropout_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ELU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ELU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_EmbeddingBag_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_EmbeddingBag_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Embedding_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Embedding_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_FeatureAlphaDropout_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_FeatureAlphaDropout_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Flatten_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Flatten_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Fold_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Fold_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_FractionalMaxPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_FractionalMaxPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_FractionalMaxPool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_FractionalMaxPool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GELU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GELU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GRUCell_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GRUCell_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GRU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GRU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GaussianNLLLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GaussianNLLLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_GroupNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_GroupNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardshrink_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardshrink_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardsigmoid_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardsigmoid_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardswish_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardswish_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardtanh_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Hardtanh_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_HingeEmbeddingLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_HingeEmbeddingLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_HuberLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_HuberLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Identity_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Identity_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_InstanceNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_KLDivLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_KLDivLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_L1Loss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_L1Loss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LPPool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LPPool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LPPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LPPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LSTMCell_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LSTMCell_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LSTM_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LSTM_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LayerNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LayerNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyBatchNorm1d_cuda_float32 (__main__.TestModuleInitCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_nn_LazyBatchNorm1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyBatchNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyBatchNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyBatchNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyBatchNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConv3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyConvTranspose3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyInstanceNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyLinear_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LazyLinear_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LeakyReLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LeakyReLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Linear_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Linear_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LocalResponseNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LocalResponseNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LogSigmoid_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LogSigmoid_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_LogSoftmax_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_LogSoftmax_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MSELoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MSELoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MarginRankingLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MarginRankingLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxPool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MaxUnpool3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Mish_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Mish_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ModuleDict_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ModuleDict_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ModuleList_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ModuleList_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiLabelMarginLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiLabelMarginLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiLabelSoftMarginLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiLabelSoftMarginLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiMarginLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiMarginLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiheadAttention_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_MultiheadAttention_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_NLLLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_NLLLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_PReLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_PReLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_PairwiseDistance_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_PairwiseDistance_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ParameterDict_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ParameterDict_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ParameterList_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ParameterList_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_PixelShuffle_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_PixelShuffle_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_PixelUnshuffle_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_PixelUnshuffle_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_PoissonNLLLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_PoissonNLLLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNBase_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNBase_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNCellBase_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNCellBase_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNCell_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNNCell_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNN_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_RNN_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_RReLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_RReLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReLU6_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReLU6_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReflectionPad3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ReplicationPad3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_SELU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_SELU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Sequential_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Sequential_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_SiLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_SiLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Sigmoid_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Sigmoid_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_SmoothL1Loss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_SmoothL1Loss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_SoftMarginLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_SoftMarginLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmax2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmax2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmax_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmax_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmin_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softmin_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softplus_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softplus_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softshrink_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softshrink_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softsign_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Softsign_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_SyncBatchNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_SyncBatchNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Tanh_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Tanh_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Tanhshrink_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Tanhshrink_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Threshold_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Threshold_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerDecoderLayer_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerDecoderLayer_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerDecoder_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerDecoder_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerEncoderLayer_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerEncoderLayer_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerEncoder_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TransformerEncoder_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Transformer_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Transformer_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TripletMarginLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TripletMarginLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_TripletMarginWithDistanceLoss_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_TripletMarginWithDistanceLoss_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Unflatten_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Unflatten_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Unfold_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Unfold_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_Upsample_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_Upsample_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_UpsamplingBilinear2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_UpsamplingBilinear2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_UpsamplingNearest2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_UpsamplingNearest2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_nn_ZeroPad2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_nn_ZeroPad2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_qat_Conv2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_qat_Conv2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_qat_Conv3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_qat_Conv3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_qat_Linear_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_qat_Linear_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_LSTMCell_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_LSTMCell_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_LSTM_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_LSTM_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_MultiheadAttention_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantizable_MultiheadAttention_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_BatchNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_BatchNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_BatchNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_BatchNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Conv3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ConvTranspose3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_DeQuantize_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_DeQuantize_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ELU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ELU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_FXFloatFunctional_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_FXFloatFunctional_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_FloatFunctional_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_FloatFunctional_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_GroupNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_GroupNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Hardswish_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Hardswish_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm1d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm1d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm3d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_InstanceNorm3d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_LayerNorm_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_LayerNorm_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_LeakyReLU_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_LeakyReLU_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Linear_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Linear_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_MaxPool2d_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_MaxPool2d_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_QFunctional_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_QFunctional_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Quantize_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Quantize_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ReLU6_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_ReLU6_cuda_float64 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Sigmoid_cuda_float32 (__main__.TestModuleInitCUDA) ... ok
test_quantized_Sigmoid_cuda_float64 (__main__.TestModuleInitCUDA) ... ok

----------------------------------------------------------------------
Ran 362 tests in 45.131s

OK
Running test_modules ... [2021-10-12 10:35:46.741003]
Executing ['/opt/conda/bin/python3.6', 'test_modules.py', '-v'] ... [2021-10-12 10:35:46.741084]
test_check_inplace_nn_ELU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_check_inplace_nn_ELU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_check_inplace_nn_ReLU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_check_inplace_nn_ReLU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_AvgPool1d_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_AvgPool1d_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_ELU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_ELU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_L1Loss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_L1Loss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_Linear_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_Linear_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_NLLLoss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_NLLLoss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_ReLU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_factory_kwargs_nn_ReLU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_AvgPool1d_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_AvgPool1d_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_ELU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_ELU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_L1Loss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_L1Loss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_Linear_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_Linear_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_NLLLoss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_NLLLoss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_forward_nn_ReLU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_forward_nn_ReLU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_AvgPool1d_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_AvgPool1d_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_ELU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_ELU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_L1Loss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_L1Loss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_Linear_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_Linear_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_NLLLoss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_NLLLoss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_ReLU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_pickle_nn_ReLU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_AvgPool1d_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_AvgPool1d_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_ELU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_ELU_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_L1Loss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_L1Loss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_Linear_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_Linear_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_NLLLoss_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_NLLLoss_cuda_float64 (__main__.TestModuleCUDA) ... ok
test_repr_nn_ReLU_cuda_float32 (__main__.TestModuleCUDA) ... ok
test_repr_nn_ReLU_cuda_float64 (__main__.TestModuleCUDA) ... ok

----------------------------------------------------------------------
Ran 52 tests in 19.001s

OK
Running test_namedtensor ... [2021-10-12 10:36:10.452793]
Executing ['/opt/conda/bin/python3.6', 'test_namedtensor.py', '-v'] ... [2021-10-12 10:36:10.452871]
test_aaa_must_run_first_check_experimental_warning (__main__.TestNamedTensor) ... ok
test_addcmul_addcdiv (__main__.TestNamedTensor) ... ok
test_addmm (__main__.TestNamedTensor) ... ok
test_addmv (__main__.TestNamedTensor) ... ok
test_align_as (__main__.TestNamedTensor) ... ok
test_align_tensors (__main__.TestNamedTensor) ... skipped 'Not implemented yet'
test_align_tensors_two_inputs (__main__.TestNamedTensor) ... skipped 'Not implemented yet'
test_align_to (__main__.TestNamedTensor) ... ok
test_align_to_ellipsis (__main__.TestNamedTensor) ... ok
test_any_all (__main__.TestNamedTensor) ... ok
test_as_strided (__main__.TestNamedTensor) ... ok
test_as_strided_cuda (__main__.TestNamedTensor) ... ok
test_autograd_ignores_names (__main__.TestNamedTensor) ... ok
test_autograd_smoke (__main__.TestNamedTensor) ... ok
test_autograd_warns_named_grad (__main__.TestNamedTensor) ... ok
test_bernoulli (__main__.TestNamedTensor) ... ok
test_big_tensor_repr_has_names (__main__.TestNamedTensor) ... ok
test_binary_ops (__main__.TestNamedTensor) ... test_namedtensor.py:739: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)
  return [Function(name, lambda a, b: getattr(torch, name)(a, b, *args, **kwargs))]
ok
test_bitwise_not (__main__.TestNamedTensor) ... ok
test_bmm (__main__.TestNamedTensor) ... ok
test_cat (__main__.TestNamedTensor) ... ok
test_cdist (__main__.TestNamedTensor) ... ok
test_comparison_ops (__main__.TestNamedTensor) ... ok
test_copy_transpose (__main__.TestNamedTensor) ... ok
test_cummax_cummin (__main__.TestNamedTensor) ... ok
test_detach (__main__.TestNamedTensor) ... ok
test_diagonal (__main__.TestNamedTensor) ... ok
test_dot (__main__.TestNamedTensor) ... ok
test_equal (__main__.TestNamedTensor) ... ok
test_expand (__main__.TestNamedTensor) ... ok
test_factory_coverage (__main__.TestNamedTensor) ... ok
test_factory_edge_cases (__main__.TestNamedTensor) ... ok
test_flatten (__main__.TestNamedTensor) ... ok
test_flatten_nodims (__main__.TestNamedTensor) ... ok
test_has_names (__main__.TestNamedTensor) ... ok
test_index_fill (__main__.TestNamedTensor) ... ok
test_info_smoke (__main__.TestNamedTensor) ... ok
test_logcumsumexp (__main__.TestNamedTensor) ... ok
test_logical_not (__main__.TestNamedTensor) ... ok
test_logical_ops (__main__.TestNamedTensor) ... ok
test_masked_fill (__main__.TestNamedTensor) ... ok
test_masked_select (__main__.TestNamedTensor) ... ok
test_matmul (__main__.TestNamedTensor) ... ok
test_max_pooling (__main__.TestNamedTensor) ... ok
test_max_pooling_without_names_does_not_warn (__main__.TestNamedTensor) ... ok
test_mm (__main__.TestNamedTensor) ... ok
test_mv (__main__.TestNamedTensor) ... ok
test_no_jit_script_support (__main__.TestNamedTensor) ... ok
test_no_jit_tracer_support (__main__.TestNamedTensor) ... ok
test_no_multiprocessing_support (__main__.TestNamedTensor) ... ok
test_no_pickle_support (__main__.TestNamedTensor) ... ok
test_no_save_support (__main__.TestNamedTensor) ... ok
test_noncontig_contiguous (__main__.TestNamedTensor) ... ok
test_none_names_refcount (__main__.TestNamedTensor) ... ok
test_nyi_dimname_overload_msg (__main__.TestNamedTensor) ... ok
test_out_fn_semantics (__main__.TestNamedTensor) ... ok
test_pow_special (__main__.TestNamedTensor) ... ok
test_py3_ellipsis (__main__.TestNamedTensor) ... ok
test_reduction_fns (__main__.TestNamedTensor) ... "Cannot find Symbol"
test_namedtensor failed! Received signal: SIGIOT
Running test_namedtuple_return_api ... [2021-10-12 10:36:13.944955]
Executing ['/opt/conda/bin/python3.6', 'test_namedtuple_return_api.py', '-v'] ... [2021-10-12 10:36:13.945009]
test_namedtuple_return (__main__.TestNamedTupleAPI) ... test_namedtuple_return_api.py:127: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1937.)
  ret1 = func(a, *op.input)
test_namedtuple_return_api.py:132: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1949.)
  ret2 = func(a, *op.input, out=tuple(ret1))
test_namedtuple_return_api.py:127: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)
  ret1 = func(a, *op.input)
test_namedtuple_return_api.py:132: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:785.)
  ret2 = func(a, *op.input, out=tuple(ret1))
test_namedtuple_return_api.py:127: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)
  ret1 = func(a, *op.input)
test_namedtuple_return_api.py:132: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2517.)
  ret2 = func(a, *op.input, out=tuple(ret1))
test_namedtuple_return_api.py:127: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
L, _ = torch.eig(A)
should be replaced with
L_complex = torch.linalg.eigvals(A)
and
L, V = torch.eig(A, eigenvectors=True)
should be replaced with
L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2894.)
  ret1 = func(a, *op.input)
test_namedtuple_return_api.py:127: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.
torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).
To get the qr decomposition consider using torch.linalg.qr.
The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.
The unpacking of the solution, as in
X, _ = torch.lstsq(B, A).solution[:A.size(1)]
should be replaced with
X = torch.linalg.lstsq(A, B).solution (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:3668.)
  ret1 = func(a, *op.input)
ok
test_native_functions_yaml (__main__.TestNamedTupleAPI) ... ok

----------------------------------------------------------------------
Ran 2 tests in 1.277s

OK
Running test_native_functions ... [2021-10-12 10:36:17.513574]
Executing ['/opt/conda/bin/python3.6', 'test_native_functions.py', '-v'] ... [2021-10-12 10:36:17.513657]
test_optional_filled_intlist (__main__.TestNativeFunctions) ... ok
test_optional_floatlist (__main__.TestNativeFunctions) ... ok
test_optional_floatlist_invalid (__main__.TestNativeFunctions) ... ok
test_optional_intlist (__main__.TestNativeFunctions) ... ok
test_optional_intlist_invalid (__main__.TestNativeFunctions) ... ok
test_string_defaults (__main__.TestNativeFunctions) ... ok

----------------------------------------------------------------------
Ran 6 tests in 0.105s

OK
Running test_nn ... [2021-10-12 10:36:19.906674]
Executing ['/opt/conda/bin/python3.6', 'test_nn.py', '-v'] ... [2021-10-12 10:36:19.906750]
test_to (__main__.PackedSequenceTest) ... ok
test_to_memory_format (__main__.PackedSequenceTest) ... ok
test_total_length (__main__.PackedSequenceTest) ... ok
test_type_casts (__main__.PackedSequenceTest)
Test type casting of `PackedSequence` against type casting of tensor ... ok
test_wrong_order (__main__.PackedSequenceTest) ... ok
test_add_relu (__main__.TestAddRelu) ... ok
test_add_relu_broadcasting (__main__.TestAddRelu) ... ok
test_avg_pool1d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool2d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool2d_with_zero_divisor (__main__.TestAvgPool) ... ok
test_avg_pool3d_ceil_mode (__main__.TestAvgPool) ... ok
test_avg_pool3d_with_zero_divisor (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool2d (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool2d_with_divisor (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool3d (__main__.TestAvgPool) ... ok
test_doubletensor_avg_pool3d_with_divisor (__main__.TestAvgPool) ... ok
test_constant_pad_nd (__main__.TestConstantPadNd) ... ok
test_preserves_memory_format (__main__.TestConstantPadNd) ... ok
test_pickle_softsign (__main__.TestFunctionalPickle) ... ok
test_fuse_module_eval_numerics (__main__.TestFusionEval) ... ok
test_chained_initialization (__main__.TestLazyModules) ... ok
test_invalid_functions (__main__.TestLazyModules) ... ok
test_lazy_batchnorm1d (__main__.TestLazyModules) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_lazy_batchnorm1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm1d_state (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm2d_state (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_batchnorm3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv1d (__main__.TestLazyModules) ... ok
test_lazy_conv1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv1d_state (__main__.TestLazyModules) ... ok
test_lazy_conv2d (__main__.TestLazyModules) ... ok
test_lazy_conv2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv2d_state (__main__.TestLazyModules) ... ok
test_lazy_conv3d (__main__.TestLazyModules) ... ok
test_lazy_conv3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose1d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose2d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_conv_transpose3d_state (__main__.TestLazyModules) ... ok
test_lazy_conv_transposed1d (__main__.TestLazyModules) ... ok
test_lazy_forward_hook (__main__.TestLazyModules) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_lazy_instancenorm1d (__main__.TestLazyModules) ... ok
test_lazy_instancenorm1d_pickle (__main__.TestLazyModules) ... ok
test_lazy_instancenorm1d_state (__main__.TestLazyModules) ... ok
test_lazy_instancenorm2d (__main__.TestLazyModules) ... ok
test_lazy_instancenorm2d_pickle (__main__.TestLazyModules) ... ok
test_lazy_instancenorm2d_state (__main__.TestLazyModules) ... ok
test_lazy_instancenorm3d (__main__.TestLazyModules) ... ok
test_lazy_instancenorm3d_pickle (__main__.TestLazyModules) ... ok
test_lazy_instancenorm3d_state (__main__.TestLazyModules) ... ok
test_lazy_linear_pickle (__main__.TestLazyModules) ... ok
test_lazy_module_buffer (__main__.TestLazyModules) ... ok
test_lazy_module_jit_buffer (__main__.TestLazyModules) ... ok
test_lazy_module_jit_param (__main__.TestLazyModules) ... ok
test_lazy_module_parameter (__main__.TestLazyModules) ... ok
test_lazy_pre_forward_hook (__main__.TestLazyModules) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
ok
test_lazy_share_memory_buffer (__main__.TestLazyModules) ... ok
test_lazy_share_memory_param (__main__.TestLazyModules) ... ok
test_linear (__main__.TestLazyModules) ... ok
test_linear_state (__main__.TestLazyModules) ... ok
test_materialize_device (__main__.TestLazyModules) ... ok
test_materialize_dtype (__main__.TestLazyModules) ... ok
test_optimizer_pass (__main__.TestLazyModules) ... ok
test_spectral_norm (__main__.TestLazyModules) ... ok
test_weight_norm (__main__.TestLazyModules) ... ok
test_global_and_local_hooks_order (__main__.TestModuleGlobalHooks) ... /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
ok
test_module_backward_global_hook_writeable (__main__.TestModuleGlobalHooks) ... ok
test_module_forward_forward_hook_removable (__main__.TestModuleGlobalHooks) ... ok
test_module_forward_preforward_hook_removable (__main__.TestModuleGlobalHooks) ... ok
test_module_global_forward_preforward_hook_writeable (__main__.TestModuleGlobalHooks) ... ok
test_module_global_hook_invalid_outputs (__main__.TestModuleGlobalHooks) ... ok
test_module_global_hooks (__main__.TestModuleGlobalHooks) ... ok
test_AdaptiveAvgPool1d (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_cuda (__main__.TestNN) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_AdaptiveAvgPool1d_no_batch_dim (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_no_batch_dim_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_one_output (__main__.TestNN) ... ok
test_AdaptiveAvgPool1d_one_output_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_no_batch_dim (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_no_batch_dim_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_1x1output (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_1x1output_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_last_dim (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_last_dim_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_no_batch_dim (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_no_batch_dim_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_single (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN) ... ok
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN) ... ok
test_AdaptiveLogSoftmax (__main__.TestNN) ... "Cannot find Symbol"
test_nn failed! Received signal: SIGIOT
Running test_numba_integration ... [2021-10-12 10:36:39.177171]
Executing ['/opt/conda/bin/python3.6', 'test_numba_integration.py', '-v'] ... [2021-10-12 10:36:39.177235]
test_active_device (__main__.TestNumbaIntegration)
'as_cuda_array' tensor device must match active numba context. ... skipped 'No numba.cuda'
test_array_adaptor (__main__.TestNumbaIntegration)
Torch __cuda_array_adaptor__ exposes tensor data to numba.cuda. ... skipped 'No numba.cuda'
test_conversion_errors (__main__.TestNumbaIntegration)
Numba properly detects array interface for tensor.Tensor variants. ... skipped 'No numba.cuda'
test_cuda_array_interface (__main__.TestNumbaIntegration)
torch.Tensor exposes __cuda_array_interface__ for cuda tensors. ... ok
test_from_cuda_array_interface (__main__.TestNumbaIntegration)
torch.as_tensor() and torch.tensor() supports the __cuda_array_interface__ protocol. ... skipped 'Test is temporary disabled, see https://github.com/pytorch/pytorch/issues/54418'
test_from_cuda_array_interface_active_device (__main__.TestNumbaIntegration)
torch.as_tensor() tensor device must match active numba context. ... skipped 'Test is temporary disabled, see https://github.com/pytorch/pytorch/issues/54418'
test_from_cuda_array_interface_inferred_strides (__main__.TestNumbaIntegration)
torch.as_tensor(numba_ary) should have correct inferred (contiguous) strides ... skipped 'No numba.cuda'
test_from_cuda_array_interface_lifetime (__main__.TestNumbaIntegration)
torch.as_tensor(obj) tensor grabs a reference to obj so that the lifetime of obj exceeds the tensor ... skipped 'Test is temporary disabled, see https://github.com/pytorch/pytorch/issues/54418'

----------------------------------------------------------------------
Ran 8 tests in 0.011s

OK (skipped=7)
Running test_numpy_interop ... [2021-10-12 10:36:42.500850]
Executing ['/opt/conda/bin/python3.6', 'test_numpy_interop.py', '-v'] ... [2021-10-12 10:36:42.500929]
test_ctor_with_numpy_scalar_ctor_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_from_list_of_ndarray_warning_cuda (__main__.TestNumPyInteropCUDA) ... ok
test_from_numpy_cuda (__main__.TestNumPyInteropCUDA) ... ok
test_has_storage_numpy_cuda (__main__.TestNumPyInteropCUDA) ... ok
test_multiplication_numpy_scalar_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_interface_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_numpy_index_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_numpy_non_writeable_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_numpy_scalar_cmp_cuda_bfloat16 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_bool (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_complex128 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_complex64 (__main__.TestNumPyInteropCUDA) ... test_numpy_interop.py:421: ComplexWarning: Casting complex values to real discards the imaginary part
  self.assertFalse(t == a)
ok
test_numpy_scalar_cmp_cuda_float16 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_float32 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_float64 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_int16 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_int32 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_int64 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_int8 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_scalar_cmp_cuda_uint8 (__main__.TestNumPyInteropCUDA) ... ok
test_numpy_unresizable_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_parse_numpy_int_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'
test_to_numpy_bool_cuda (__main__.TestNumPyInteropCUDA) ... ok
test_to_numpy_cuda (__main__.TestNumPyInteropCUDA) ... skipped 'Only runs on cpu'

----------------------------------------------------------------------
Ran 24 tests in 6.276s

OK (skipped=8)
Running test_ops ... [2021-10-12 10:36:52.153165]
Executing ['/opt/conda/bin/python3.6', 'test_ops.py', '-v'] ... [2021-10-12 10:36:52.153244]
test_dtypes___getitem___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___radd___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rand___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rdiv___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rmatmul___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rmod___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rmul___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___ror___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rpow___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rsub___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes___rxor___cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_abs_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_acos_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_acosh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_add_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addbmm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addcdiv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addcmul_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addmm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addmm_decomposed_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addmv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_addr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_all_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_amax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_amin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_aminmax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_angle_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_any_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_argmax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_argmin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_asin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_asinh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_atan2_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_atan_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_atanh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_baddbmm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_bitwise_and_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_bitwise_left_shift_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_bitwise_not_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_bitwise_right_shift_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_bmm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_broadcast_to_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cat_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cdist_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_ceil_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cholesky_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cholesky_inverse_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_chunk_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_clamp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_clamp_scalar_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_clone_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_complex_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_conj_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_conj_physical_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_contiguous_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_copysign_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_corrcoef_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cos_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cosh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_count_nonzero_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cov_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cross_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cummax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cummin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cumprod_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cumsum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_cumulative_trapezoid_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_deg2rad_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_diag_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_diag_embed_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_diagonal_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_diff_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_digamma_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_dist_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_div_floor_rounding_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_div_no_rounding_mode_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_div_trunc_rounding_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_dot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_dsplit_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_dstack_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_eig_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_einsum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_eq_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_erf_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_erfc_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_erfinv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_exp2_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_exp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_expand_as_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_expand_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_expm1_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_fft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_fftn_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_hfft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_ifft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_ifftn_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_ihfft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_irfft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_irfftn_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_rfft_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fft_rfftn_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fill__cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_flip_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fliplr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_flipud_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_float_power_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_floor_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_floor_divide_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fmax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fmin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fmod_autodiffed_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_fmod_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_frac_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_frexp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_gather_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_ge_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_geqrf_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_gradient_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_gt_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_histogram_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_hsplit_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_hstack_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_hypot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_i0_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_igamma_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_igamma_grad_other_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_igammac_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_igammac_grad_other_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_imag_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_index_add_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_index_copy_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_index_fill_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_index_put_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_index_select_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_inner_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_inverse_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_isin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_kron_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_kthvalue_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_le_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lerp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lgamma_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_cholesky_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_cholesky_ex_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_cond_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_det_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_det_singular_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_linalg_eig_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_eigh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_eigvals_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_eigvalsh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_householder_product_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_inv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_inv_ex_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_lstsq_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_matrix_norm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_matrix_power_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_matrix_rank_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_matrix_rank_hermitian_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_multi_dot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_norm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_pinv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_pinv_hermitian_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_qr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_slogdet_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_solve_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_svd_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_svdvals_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_tensorinv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_linalg_vector_norm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log10_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log1p_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log2_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log_softmax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_log_softmax_dtype_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logaddexp2_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logaddexp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logcumsumexp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logdet_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logical_not_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logit_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_logsumexp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lt_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lu_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lu_solve_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_lu_unpack_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_masked_fill_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_masked_scatter_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_masked_select_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_matmul_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_matrix_exp_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_max_binary_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_max_reduction_no_dim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_max_reduction_with_dim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_maximum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mean_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_median_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_meshgrid_list_of_tensors_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_meshgrid_variadic_tensors_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_min_binary_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_min_reduction_no_dim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_min_reduction_with_dim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_minimum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mode_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_movedim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_msort_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mul_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mv_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mvlgamma_mvlgamma_p_1_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_mvlgamma_mvlgamma_p_3_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_mvlgamma_mvlgamma_p_5_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_nan_to_num_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nanmedian_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nanquantile_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nansum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_narrow_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_ne_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_neg_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nextafter_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_adaptive_avg_pool2d_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_avg_pool2d_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_conv_transpose2d_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_cosine_similarity_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_gelu_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_grid_sample_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_hardshrink_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_hardswish_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_hardtanh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_area_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_bicubic_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_bilinear_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_linear_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_nearest_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_interpolate_trilinear_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_layer_norm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_leaky_relu_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_logsigmoid_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_mse_loss_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_nll_loss_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_normalize_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_one_hot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_pad_circular_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_pad_constant_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_pad_reflect_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_pad_replicate_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_relu6_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_relu_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_softplus_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_nn_functional_unfold_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_norm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_norm_fro_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_norm_inf_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_norm_nuc_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_ormqr_cuda (__main__.TestCommonCUDA) ... skipped 'cuSOLVER not available'
test_dtypes_outer_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_permute_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_pinverse_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_polar_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_polygamma_polygamma_n_0_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_polygamma_polygamma_n_1_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_2_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_3_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_4_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_positive_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_pow_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_prod_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_put_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_qr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_quantile_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_rad2deg_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_ravel_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_real_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_reciprocal_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_remainder_autodiffed_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_remainder_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_renorm_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_repeat_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_reshape_as_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_reshape_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_resize__cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_resize_as__cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_resolve_conj_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_resolve_neg_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_roll_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_rot90_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_round_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_rsqrt_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_rsub_rsub_scalar_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_rsub_rsub_tensor_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_scatter_add_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_scatter_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_select_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sgn_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sigmoid_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_sign_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_signbit_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sin_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sinc_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sinh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_softmax_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_softmax_with_dtype_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_solve_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sort_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_entr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_erfcx_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_i0e_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_i1_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_i1e_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_ndtr_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_ndtri_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_polygamma_special_polygamma_n_0_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_xlog1py_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_zeta_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_special_zeta_grad_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_split_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_split_list_args_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_split_with_sizes_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sqrt_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_square_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_squeeze_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_stack_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_std_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_std_mean_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_sub_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_sum_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_svd_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_symeig_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_t_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_take_along_dim_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_take_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tan_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tanh_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tensor_split_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tensordot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tile_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_to_sparse_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_topk_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_trace_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_transpose_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_trapezoid_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_trapz_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_triangular_solve_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_tril_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_triu_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_true_divide_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_trunc_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_unfold_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_unsqueeze_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_var_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_var_mean_cuda (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_dtypes_vdot_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_view_as_complex_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_view_as_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_view_as_real_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_view_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_vsplit_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_vstack_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_where_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_xlogy_cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_dtypes_zero__cuda (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___getitem___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___getitem___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___radd___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___radd___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rand___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rdiv___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rdiv___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rmatmul___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rmod___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rmod___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rmul___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rmul___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___ror___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rpow___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rpow___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rsub___cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rsub___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices___rxor___cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_abs_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_abs_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_acos_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_acos_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_acosh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_acosh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_add_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_add_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addbmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addcdiv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addcmul_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addcmul_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addmm_decomposed_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addmv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_addr_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_all_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_all_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_amax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_amax_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_amin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_amin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_aminmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_aminmax_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_angle_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_angle_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_any_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_any_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_argmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_argmax_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_argmin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_argmin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_asin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_asin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_asinh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_asinh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atan2_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atan2_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atan_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atan_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atanh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_atanh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_baddbmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_and_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_left_shift_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_left_shift_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_not_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_right_shift_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bitwise_right_shift_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_bmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_broadcast_to_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_broadcast_to_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cat_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cat_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cdist_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ceil_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cholesky_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cholesky_inverse_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_chunk_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_chunk_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clamp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clamp_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clamp_scalar_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clamp_scalar_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clone_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_clone_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_complex_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_conj_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_conj_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_conj_physical_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_conj_physical_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_contiguous_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_contiguous_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_copysign_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_copysign_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_corrcoef_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_corrcoef_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cos_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cos_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cosh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cosh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_count_nonzero_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_count_nonzero_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cov_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cov_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cross_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cross_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cummax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cummax_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cummin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cummin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumprod_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumprod_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumsum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumsum_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumulative_trapezoid_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_cumulative_trapezoid_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_deg2rad_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_deg2rad_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diag_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diag_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diag_embed_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diag_embed_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diagonal_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diagonal_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diff_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_diff_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_digamma_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_digamma_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dist_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_floor_rounding_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_floor_rounding_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_no_rounding_mode_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_no_rounding_mode_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_trunc_rounding_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_div_trunc_rounding_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dot_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dsplit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dsplit_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dstack_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_dstack_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_eig_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_einsum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_eq_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_eq_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erf_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erf_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erfc_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erfc_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erfinv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_erfinv_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_exp2_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_exp2_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_exp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_exp_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expand_as_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expand_as_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expand_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expand_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expm1_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_expm1_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_fft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_fftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_hfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_ifft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_ifftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_ihfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_irfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_irfftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_rfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fft_rfftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fill__cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fill__cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_flip_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_flip_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fliplr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fliplr_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_flipud_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_flipud_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_float_power_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_float_power_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_floor_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_floor_divide_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_floor_divide_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmax_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmod_autodiffed_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmod_autodiffed_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmod_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_fmod_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_frac_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_frexp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gather_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gather_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ge_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ge_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_geqrf_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gradient_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gradient_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gt_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_gt_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_hsplit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_hsplit_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_hstack_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_hstack_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_hypot_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_i0_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_i0_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_igamma_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_igamma_grad_other_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_igammac_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_igammac_grad_other_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_add_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_add_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_copy_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_copy_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_fill_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_fill_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_put_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_put_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_select_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_index_select_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_inner_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_inverse_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_isin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_isin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_kron_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_kron_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_kthvalue_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_kthvalue_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_le_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_le_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lerp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lgamma_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lgamma_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_cholesky_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_cholesky_ex_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_cond_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_det_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_eig_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_eigh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_eigvals_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_eigvalsh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_householder_product_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_inv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_inv_ex_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_lstsq_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_matrix_norm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_matrix_power_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_matrix_rank_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_matrix_rank_hermitian_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_multi_dot_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_norm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_pinv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_pinv_hermitian_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_qr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_slogdet_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_solve_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_svd_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_svdvals_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_tensorinv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_linalg_vector_norm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log10_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log10_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log1p_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log1p_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log2_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log2_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log_softmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log_softmax_dtype_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_log_softmax_dtype_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logaddexp2_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logaddexp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logcumsumexp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logdet_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logical_not_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logical_not_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logit_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_logsumexp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lt_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lt_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lu_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lu_solve_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_lu_unpack_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_fill_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_fill_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_scatter_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_scatter_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_select_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_masked_select_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_matmul_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_matrix_exp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_binary_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_binary_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_reduction_no_dim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_reduction_no_dim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_reduction_with_dim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_max_reduction_with_dim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_maximum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_maximum_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mean_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_median_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_median_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_meshgrid_list_of_tensors_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_meshgrid_list_of_tensors_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_meshgrid_variadic_tensors_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_meshgrid_variadic_tensors_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_binary_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_binary_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_reduction_no_dim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_reduction_no_dim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_reduction_with_dim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_min_reduction_with_dim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_minimum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_minimum_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mode_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mode_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_movedim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_movedim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_msort_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_msort_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mul_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mul_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mv_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_nan_to_num_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nan_to_num_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nanmedian_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nanmedian_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nanquantile_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nansum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nansum_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_narrow_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_narrow_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ne_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ne_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_neg_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_neg_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nextafter_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_adaptive_avg_pool2d_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_avg_pool2d_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_conv_transpose2d_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_cosine_similarity_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_gelu_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_grid_sample_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_hardshrink_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_hardswish_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_hardtanh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_hardtanh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_area_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_bicubic_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_bilinear_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_linear_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_nearest_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_interpolate_trilinear_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_layer_norm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_leaky_relu_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_logsigmoid_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_mse_loss_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_nll_loss_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_normalize_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_one_hot_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_circular_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_circular_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_constant_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_constant_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_reflect_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_pad_replicate_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_relu6_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_relu6_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_relu_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_relu_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_softplus_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_nn_functional_unfold_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_norm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_norm_fro_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_norm_inf_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_norm_nuc_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ormqr_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'cuSOLVER not available'
test_multiple_devices_outer_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_outer_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_permute_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_permute_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_pinverse_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_polar_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_polygamma_polygamma_n_0_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_polygamma_polygamma_n_0_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_polygamma_polygamma_n_1_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_1_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_2_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_2_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_3_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_3_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_4_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_polygamma_polygamma_n_4_cuda_int64 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_multiple_devices_positive_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_positive_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_pow_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_pow_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_prod_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_prod_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_put_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_put_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_qr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_quantile_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rad2deg_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rad2deg_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ravel_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_ravel_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reciprocal_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reciprocal_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_remainder_autodiffed_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_remainder_autodiffed_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_remainder_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_remainder_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_renorm_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_repeat_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_repeat_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reshape_as_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reshape_as_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reshape_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_reshape_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resize__cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resize__cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resize_as__cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resize_as__cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resolve_conj_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resolve_conj_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resolve_neg_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_resolve_neg_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_roll_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_roll_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rot90_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rot90_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_round_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsqrt_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsqrt_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsub_rsub_scalar_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsub_rsub_scalar_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsub_rsub_tensor_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_rsub_rsub_tensor_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_scatter_add_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_scatter_add_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_scatter_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_scatter_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_select_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_select_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sgn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sgn_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sigmoid_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sigmoid_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sign_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sign_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_signbit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_signbit_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sin_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sin_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sinc_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sinc_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sinh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sinh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_softmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_softmax_with_dtype_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_softmax_with_dtype_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_solve_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sort_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sort_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_entr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_entr_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_erfcx_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_erfcx_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i0e_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i0e_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i1_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i1_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i1e_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_i1e_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_ndtr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_ndtr_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_ndtri_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_ndtri_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_xlog1py_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_xlog1py_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_zeta_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_zeta_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_zeta_grad_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_special_zeta_grad_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_list_args_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_list_args_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_with_sizes_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_split_with_sizes_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sqrt_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sqrt_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_square_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_square_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_squeeze_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_squeeze_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_stack_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_stack_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_std_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_std_mean_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sub_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sub_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_sum_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_svd_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_symeig_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_t_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_t_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_take_along_dim_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_take_along_dim_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_take_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_take_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tan_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tan_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tanh_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tanh_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tensor_split_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tensor_split_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tensordot_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tile_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tile_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_to_sparse_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_to_sparse_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_topk_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_topk_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trace_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trace_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_transpose_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_transpose_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trapezoid_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trapezoid_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trapz_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trapz_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_triangular_solve_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tril_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_tril_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_triu_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_triu_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_true_divide_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_true_divide_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_trunc_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_unfold_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_unfold_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_unsqueeze_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_unsqueeze_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_var_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_var_mean_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_vdot_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_view_as_complex_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_view_as_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_view_as_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_view_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_view_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_vsplit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_vsplit_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_vstack_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_vstack_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_where_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_where_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_xlogy_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_xlogy_cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_zero__cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_multiple_devices_zero__cuda_int64 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out___getitem___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___radd___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rdiv___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rmatmul___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rmod___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rmul___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rpow___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out___rsub___cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_abs_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_acos_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_acosh_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_add_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addbmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_addcdiv_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addcmul_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addmm_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addmm_decomposed_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addmv_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_addr_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_all_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_amax_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_amin_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_aminmax_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_angle_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_any_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_argmax_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_argmin_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_asin_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_asinh_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_atan2_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_atan_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_atanh_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_baddbmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_bitwise_left_shift_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_bitwise_right_shift_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_bmm_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_broadcast_to_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_cat_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_cdist_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_ceil_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_cholesky_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_cholesky_inverse_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_chunk_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_clamp_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_clamp_scalar_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_clone_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_complex_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_conj_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_conj_physical_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_contiguous_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_copysign_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_corrcoef_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_cos_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_cosh_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_count_nonzero_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_cov_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_cross_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_cummax_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_cummin_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_cumprod_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_cumsum_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_cumulative_trapezoid_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_deg2rad_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_diag_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_diag_embed_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_diagonal_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_diff_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_digamma_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_dist_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_div_floor_rounding_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_div_no_rounding_mode_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_div_trunc_rounding_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_dot_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_dsplit_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_dstack_cuda_float32 (__main__.TestCommonCUDA) ... skipped 'Skipped!'
test_out_eig_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_einsum_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_eq_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_erf_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_erfc_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_erfinv_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_exp2_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_exp_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_expand_as_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_expand_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_expm1_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_fft_fft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_fftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_hfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_ifft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_ifftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_ihfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_irfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_irfftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_rfft_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fft_rfftn_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_fill__cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_flip_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_fliplr_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_flipud_cuda_float32 (__main__.TestCommonCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_float_power_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_floor_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_floor_divide_cuda_float32 (__main__.TestCommonCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_methods_invocations.py:648: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)
  return self.op(*args, **kwargs)
/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_methods_invocations.py:648: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BinaryOps.cpp:584.)
  return self.op(*args, **kwargs)
ok
test_out_fmax_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_fmin_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_fmod_autodiffed_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_fmod_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_frac_cuda_float32 (__main__.TestCommonCUDA) ... ok
test_out_frexp_cuda_float32 (__main__.TestCommonCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_gather_cuda_float32 (__main__.TestCommonCUDA) ... "Cannot find Symbol"
test_ops failed! Received signal: SIGIOT
Running test_optim ... [2021-10-12 10:38:40.505113]
Executing ['/opt/conda/bin/python3.6', 'test_optim.py', '-v'] ... [2021-10-12 10:38:40.505177]
test_CosineAnnealingWarmRestarts_lr1 (__main__.TestLRScheduler) ... ok
test_CosineAnnealingWarmRestarts_lr2 (__main__.TestLRScheduler) ... ok
test_CosineAnnealingWarmRestarts_lr3 (__main__.TestLRScheduler) ... ok
test_CosineAnnealingWarmRestarts_lr_state_dict (__main__.TestLRScheduler) ... ok
test_chained_lr1 (__main__.TestLRScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
ok
test_chained_lr2 (__main__.TestLRScheduler) ... ok
test_chained_lr3 (__main__.TestLRScheduler) ... ok
test_chained_lr4 (__main__.TestLRScheduler) ... ok
test_closed_form_constantlr (__main__.TestLRScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
ok
test_closed_form_cos_anneal_lr (__main__.TestLRScheduler) ... ok
test_closed_form_exp_lr (__main__.TestLRScheduler) ... ok
test_closed_form_linearlr (__main__.TestLRScheduler) ... ok
test_closed_form_multi_step_lr (__main__.TestLRScheduler) ... ok
test_closed_form_step_lr (__main__.TestLRScheduler) ... ok
test_compound_cosanneal_and_exp_lr (__main__.TestLRScheduler) ... ok
test_compound_cosanneal_and_linearlr (__main__.TestLRScheduler) ... ok
test_compound_cosanneal_and_multistep_lr (__main__.TestLRScheduler) ... ok
test_compound_cosanneal_and_step_lr (__main__.TestLRScheduler) ... ok
test_compound_exp_and_linearlr (__main__.TestLRScheduler) ... ok
test_compound_exp_and_multistep_lr (__main__.TestLRScheduler) ... ok
test_compound_linearlr_and_multistep_lr (__main__.TestLRScheduler) ... ok
test_compound_reduce_lr_on_plateau1 (__main__.TestLRScheduler) ... ok
test_compound_reduce_lr_on_plateau2 (__main__.TestLRScheduler) ... ok
test_compound_reduce_lr_on_plateau3 (__main__.TestLRScheduler) ... ok
test_compound_reduce_lr_on_plateau4 (__main__.TestLRScheduler) ... ok
test_compound_reduce_lr_on_plateau5 (__main__.TestLRScheduler) ... ok
test_compound_step_and_constantlr (__main__.TestLRScheduler) ... ok
test_compound_step_and_exp_lr (__main__.TestLRScheduler) ... ok
test_compound_step_and_multistep_lr (__main__.TestLRScheduler) ... ok
test_constantlr (__main__.TestLRScheduler) ... ok
test_constantlr_is_constant_for_constant_epoch (__main__.TestLRScheduler) ... ok
test_constantlr_with_epoch (__main__.TestLRScheduler) ... ok
test_cos_anneal_lr (__main__.TestLRScheduler) ... ok
test_cosine_lr_state_dict (__main__.TestLRScheduler) ... ok
test_cosine_then_cyclic (__main__.TestLRScheduler) ... ok
test_cycle_lr_cycle_momentum_fail_with_momentumless_optimizer (__main__.TestLRScheduler) ... ok
test_cycle_lr_exp_range_mode (__main__.TestLRScheduler) ... ok
test_cycle_lr_exp_range_mode_one_lr (__main__.TestLRScheduler) ... ok
test_cycle_lr_exp_range_mode_step_size_up_down (__main__.TestLRScheduler) ... ok
test_cycle_lr_invalid_mode (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular2_mode (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular2_mode_one_lr (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular2_mode_step_size_up_down (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular_mode (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular_mode_one_lr (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular_mode_one_lr_no_momentum (__main__.TestLRScheduler) ... ok
test_cycle_lr_triangular_mode_step_size_up_down (__main__.TestLRScheduler) ... ok
test_cycle_lr_with_adam (__main__.TestLRScheduler) ... ok
test_cycle_lr_with_momentumless_optimizer (__main__.TestLRScheduler) ... ok
test_error_when_getlr_has_epoch (__main__.TestLRScheduler) ... ok
test_exp_lr (__main__.TestLRScheduler) ... ok
test_exp_step_lr_state_dict (__main__.TestLRScheduler) ... ok
test_exponential_lr_is_constant_for_constant_epoch (__main__.TestLRScheduler) ... ok
test_get_last_lr_constantlr (__main__.TestLRScheduler) ... ok
test_get_last_lr_linearlr (__main__.TestLRScheduler) ... ok
test_get_last_lr_multi_step_lr (__main__.TestLRScheduler) ... ok
test_get_last_lr_step_lr (__main__.TestLRScheduler) ... ok
test_lambda_lr (__main__.TestLRScheduler) ... ok
test_lambda_lr_state_dict_fn (__main__.TestLRScheduler) ... ok
test_lambda_lr_state_dict_obj (__main__.TestLRScheduler) ... ok
test_linear_linearlr_is_constant_for_constant_epoch (__main__.TestLRScheduler) ... ok
test_linearlr (__main__.TestLRScheduler) ... ok
test_linearlr_with_epoch (__main__.TestLRScheduler) ... ok
test_multi_step_lr (__main__.TestLRScheduler) ... ok
test_multi_step_lr_state_dict (__main__.TestLRScheduler) ... ok
test_multi_step_lr_with_epoch (__main__.TestLRScheduler) ... ok
test_multiplicative_lr (__main__.TestLRScheduler) ... ok
test_new_pattern_no_warning (__main__.TestLRScheduler) ... ok
test_new_pattern_no_warning_with_arg (__main__.TestLRScheduler) ... ok
test_new_pattern_no_warning_with_overridden_optim_step (__main__.TestLRScheduler) ... ok
test_no_cyclic_references (__main__.TestLRScheduler) ... ok
test_old_pattern_warning (__main__.TestLRScheduler) ... ok
test_old_pattern_warning_resuming (__main__.TestLRScheduler) ... ok
test_old_pattern_warning_resuming_with_arg (__main__.TestLRScheduler) ... ok
test_old_pattern_warning_with_arg (__main__.TestLRScheduler) ... ok
test_old_pattern_warning_with_overridden_optim_step (__main__.TestLRScheduler) ... ok
test_onecycle_lr_cannot_calculate_total_steps (__main__.TestLRScheduler) ... ok
test_onecycle_lr_cosine_annealing (__main__.TestLRScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
ok
test_onecycle_lr_invalid_anneal_strategy (__main__.TestLRScheduler) ... ok
test_onecycle_lr_invalid_pct_start (__main__.TestLRScheduler) ... ok
test_onecycle_lr_linear_annealing (__main__.TestLRScheduler) ... ok
test_onecycle_lr_linear_annealing_three_phases (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau1 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau2 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau3 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau4 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau5 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau6 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau7 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau8 (__main__.TestLRScheduler) ... ok
test_reduce_lr_on_plateau_state_dict (__main__.TestLRScheduler) ... ok
test_sequentiallr1 (__main__.TestLRScheduler) ... /opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
ok
test_sequentiallr2 (__main__.TestLRScheduler) ... ok
test_sequentiallr3 (__main__.TestLRScheduler) ... ok
test_step_lr (__main__.TestLRScheduler) ... ok
test_step_lr_is_constant_for_constant_epoch (__main__.TestLRScheduler) ... ok
test_step_lr_state_dict (__main__.TestLRScheduler) ... ok
test_swa_lr_state_dict (__main__.TestLRScheduler) ... ok
test_swalr_cosine_anneal_after_multiplicative (__main__.TestLRScheduler) ... ok
test_swalr_hypers (__main__.TestLRScheduler) ... ok
test_swalr_linear_anneal_after_multiplicative (__main__.TestLRScheduler) ... ok
test_swalr_no_anneal (__main__.TestLRScheduler) ... ok
test_adadelta (__main__.TestOptim) ... skipped "test doesn't currently work on the ROCm stack"
test_adagrad (__main__.TestOptim) ... ok
test_adagrad_sparse (__main__.TestOptim) ... ok
test_adam (__main__.TestOptim) ... ok
test_adamax (__main__.TestOptim) ... ok
test_adamw (__main__.TestOptim) ... ok
test_asgd (__main__.TestOptim) ... ok
test_complex_adam_variance (__main__.TestOptim) ... ok
test_duplicate_params_in_param_group (__main__.TestOptim) ... ok
test_invalid_param_type (__main__.TestOptim) ... ok
test_lbfgs (__main__.TestOptim) ... ok
test_lbfgs_return_type (__main__.TestOptim) ... ok
test_multi_tensor_optimizers (__main__.TestOptim) ... ok
test_nadam (__main__.TestOptim) ... /opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
ok
test_no_grad_for_all_params (__main__.TestOptim) ... /opt/conda/lib/python3.6/site-packages/torch/optim/adadelta.py:37: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(Adadelta, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/adamw.py:74: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(AdamW, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/adam.py:74: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(Adam, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/adagrad.py:60: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(Adagrad, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/adamax.py:61: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(Adamax, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/rmsprop.py:77: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(RMSprop, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/sgd.py:95: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(SGD, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/sparse_adam.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(SparseAdam, self).__init__(params, defaults)
/opt/conda/lib/python3.6/site-packages/torch/optim/asgd.py:34: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super(ASGD, self).__init__(params, defaults)
ok
test_radam (__main__.TestOptim) ... ok
test_rmsprop (__main__.TestOptim) ... ok
test_rprop (__main__.TestOptim) ... ok
test_sgd (__main__.TestOptim) ... ok
test_sgd_sparse (__main__.TestOptim) ... ok
test_sparse_adam (__main__.TestOptim) ... ok
test_averaged_model_all_devices (__main__.TestSWAUtils) ... ok
test_averaged_model_exponential (__main__.TestSWAUtils) ... ok
test_averaged_model_mixed_device (__main__.TestSWAUtils) ... ok
test_averaged_model_state_dict (__main__.TestSWAUtils) ... ok
test_bn_update_eval_momentum (__main__.TestSWAUtils) ... ok
test_update_bn_cnn (__main__.TestSWAUtils) ... ok
test_update_bn_dnn (__main__.TestSWAUtils) ... ok

----------------------------------------------------------------------
Ran 130 tests in 81.616s

OK (skipped=1)
Running test_overrides ... [2021-10-12 10:40:05.920916]
Executing ['/opt/conda/bin/python3.6', 'test_overrides.py', '-v'] ... [2021-10-12 10:40:05.921000]
test_broadcast_all (__main__.TestBroadcastAllOverride) ... ok
test_parameter_does_not_prevent_dispatch (__main__.TestDisabledTorchFunction) ... ok
test_wrapper (__main__.TestEinsumOverride) ... ok
test_gradcheck (__main__.TestGradCheckOverride) ... ok
test_newones (__main__.TestGradNewOnesOverride) ... ok
test_getitem (__main__.TestIndexing) ... ok
test_getitem_subclass (__main__.TestIndexing) ... ok
test_setitem (__main__.TestIndexing) ... ok
test_setitem_subclass (__main__.TestIndexing) ... ok
test_setitem_val (__main__.TestIndexing) ... ok
test_iterator (__main__.TestIterator) ... ok
test_max (__main__.TestNamedTuple) ... ok
test_pickle (__main__.TestPickle) ... ok
test_rnn (__main__.TestRNN) ... ok
test_Tensor___add__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___and__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___array__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___array_wrap__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___bool__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___complex__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___contains__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___cuda_array_interface_____get__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___deepcopy__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___div__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___eq__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___float__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___floordiv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___format__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ge__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___getitem__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___gt__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___hash__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___iadd__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___iand__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___idiv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ifloordiv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ilshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___imod__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___imul__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___index__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___int__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___invert__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ior__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ipow__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___irshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___isub__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ixor__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___le__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___len__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___long__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___lshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___lt__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___matmul__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___mod__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___mul__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ne__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___nonzero__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___or__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___radd__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rand__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rdiv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___reduce_ex__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___repr__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___reversed__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rfloordiv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rlshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rmatmul__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rmod__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rmul__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___ror__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rpow__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rrshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rshift__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rsub__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___rxor__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___setitem__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___setstate__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___sub__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___truediv__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor___xor__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__coalesced_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__dimI (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__dimV (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__indices (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__is_view (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__nnz (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__update_names (__main__.TestTorchFunctionOverride) ... ok
test_Tensor__values (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_abs (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_abs_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_absolute (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_absolute_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_acos (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_acos_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_acosh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_acosh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_add (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_add_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addbmm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addbmm_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addcdiv (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addcdiv_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addcmul (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addcmul_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addmm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addmm_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addmv (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addmv_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_addr_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_align_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_align_to (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_all (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_allclose (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_amax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_amin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_aminmax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_angle (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_any (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_apply_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arccos (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arccos_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arccosh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arccosh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arcsin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arcsin_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arcsinh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arcsinh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arctan (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arctan_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arctanh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_arctanh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_argmax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_argmin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_argsort (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_as_strided (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_as_strided_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_asin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_asin_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_asinh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_asinh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atan (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atan2 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atan2_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atan_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atanh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_atanh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_backward (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_baddbmm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_baddbmm_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bernoulli (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bernoulli_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bfloat16 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bincount (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_and (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_and_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_left_shift (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_left_shift_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_not (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_not_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_or (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_or_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_right_shift (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_right_shift_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_xor (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bitwise_xor_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bmm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_bool (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_broadcast_to (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_byte (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cauchy_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cdouble (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ceil (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ceil_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cfloat (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_char (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cholesky (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cholesky_inverse (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cholesky_solve (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_chunk (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp_max (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp_max_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp_min (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clamp_min_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clip (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clip_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_clone (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_coalesce (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_col_indices (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_conj (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_conj_physical (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_conj_physical_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_contiguous (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_copy_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_copysign (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_copysign_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_corrcoef (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cos (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cos_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cosh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cosh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_count_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cov (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cpu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cross (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_crow_indices (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cuda (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cummax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cummin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cumprod (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cumprod_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cumsum (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_cumsum_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_data_ptr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_deg2rad (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_deg2rad_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dense_dim (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dequantize (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_det (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_detach (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_detach_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_diag (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_diag_embed (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_diagflat (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_diagonal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_diff (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_digamma (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_digamma_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dim (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dist (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_div (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_div_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_divide (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_divide_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dot (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_double (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_dsplit (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_eig (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_element_size (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_eq (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_eq_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_equal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erf (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erf_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erfc (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erfc_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erfinv (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_erfinv_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_exp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_exp2 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_exp2_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_exp_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_expand (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_expand_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_expm1 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_expm1_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_exponential_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fill_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fill_diagonal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fix (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fix_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_flatten (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_flip (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fliplr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_flipud (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_float (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_float_power (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_float_power_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_floor (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_floor_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_floor_divide (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_floor_divide_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fmax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fmin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fmod (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_fmod_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_frac (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_frac_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_frexp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_gather (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_gcd (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_gcd_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ge (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ge_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_geometric_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_geqrf (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ger (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_get_device (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_grad___get__ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_greater (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_greater_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_greater_equal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_greater_equal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_gt (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_gt_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_half (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_hardshrink (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_has_names (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_heaviside (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_heaviside_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_histc (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_histogram (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_hsplit (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_hypot (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_hypot_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_i0 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_i0_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_igamma (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_igamma_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_igammac (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_igammac_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_add (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_add_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_copy (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_copy_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_fill (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_fill_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_put (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_put_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_index_select (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_indices (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_inner (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_int (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_int_repr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_inverse (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_coalesced (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_complex (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_conj (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_contiguous (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_distributed (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_floating_point (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_inference (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_neg (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_pinned (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_same_size (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_set_to (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_shared (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_is_signed (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isclose (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isfinite (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isinf (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isnan (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isneginf (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isposinf (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_isreal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_istft (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_item (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_kron (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_kthvalue (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lcm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lcm_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ldexp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ldexp_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_le (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_le_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lerp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lerp_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_less (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_less_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_less_equal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_less_equal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lgamma (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lgamma_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log10 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log10_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log1p (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log1p_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log2 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log2_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log_normal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_log_softmax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logaddexp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logaddexp2 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logcumsumexp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logdet (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_and (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_and_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_not (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_not_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_or (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_or_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_xor (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logical_xor_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logit (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logit_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_logsumexp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_long (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lstsq (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lt (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lt_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_lu_solve (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_map2_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_map_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_masked_fill (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_masked_fill_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_masked_scatter (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_masked_scatter_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_masked_select (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_matmul (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_matrix_exp (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_matrix_power (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_max (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_maximum (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mean (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_median (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_min (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_minimum (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mode (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_moveaxis (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_movedim (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_msort (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mul (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mul_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_multinomial (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_multiply (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_multiply_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mv (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mvlgamma (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_mvlgamma_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nan_to_num (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nan_to_num_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nanmedian (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nanquantile (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nansum (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_narrow (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_narrow_copy (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ndimension (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ne (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ne_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_neg (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_neg_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_negative (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_negative_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nelement (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nextafter (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nextafter_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_norm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_normal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_not_equal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_not_equal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_numel (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_numpy (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_orgqr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ormqr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_outer (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_permute (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_pin_memory (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_pinverse (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_polygamma (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_polygamma_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_positive (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_pow (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_pow_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_prelu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_prod (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_put (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_put_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_q_per_channel_axis (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_q_per_channel_scales (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_q_per_channel_zero_points (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_q_scale (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_q_zero_point (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_qr (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_qscheme (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_quantile (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rad2deg (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rad2deg_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_random_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_ravel (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_reciprocal (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_reciprocal_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_record_stream (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_refine_names (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_register_hook (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_relu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_relu_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_remainder (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_remainder_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rename (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rename_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_renorm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_renorm_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_repeat (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_repeat_interleave (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_requires_grad_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_reshape (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_reshape_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resize (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resize_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resize_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resize_as_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resolve_conj (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_resolve_neg (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_retain_grad (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_roll (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rot90 (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_round (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_round_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rsqrt (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_rsqrt_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_scatter (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_scatter_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_scatter_add (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_scatter_add_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_select (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_set_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sgn (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sgn_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_share_memory_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_short (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sigmoid (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sigmoid_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sign (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sign_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_signbit (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sin (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sin_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sinc (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sinc_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sinh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sinh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_size (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_slogdet (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_smm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_softmax (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_solve (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sort (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sparse_dim (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sparse_mask (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sparse_resize_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sparse_resize_and_clear_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_split (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_split_with_sizes (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sqrt (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sqrt_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_square (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_square_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_squeeze (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_squeeze_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sspaddmm (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_std (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_stft (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_storage (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_storage_offset (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_storage_type (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sub (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sub_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_subtract (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_subtract_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sum (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_sum_to_size (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_svd (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_swapaxes (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_swapaxes_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_swapdims (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_swapdims_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_symeig (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_t (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_t_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_take (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_take_along_dim (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tan (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tan_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tanh (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tanh_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tensor_split (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tile (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_to (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_to_dense (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_to_mkldnn (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_to_sparse (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tolist (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_topk (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_trace (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_transpose (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_transpose_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_triangular_solve (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tril (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_tril_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_triu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_triu_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_true_divide (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_true_divide_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_trunc (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_trunc_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_type (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_type_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unbind (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unfold (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_uniform_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unique (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unique_consecutive (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unsafe_chunk (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unsafe_split (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unsafe_split_with_sizes (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unsqueeze (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_unsqueeze_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_values (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_var (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_vdot (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_view (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_view_as (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_vsplit (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_where (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_xlogy (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_xlogy_ (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_xpu (__main__.TestTorchFunctionOverride) ... ok
test_Tensor_zero_ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_T___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__backward_hooks___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__base___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__cdata___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__grad___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__grad_fn___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase__version___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_data___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_device___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_dtype___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_grad_fn___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_imag___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_cuda___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_leaf___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_meta___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_mkldnn___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_mlc___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_ort___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_quantized___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_sparse___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_sparse_csr___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_vulkan___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_is_xpu___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_layout___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_name___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_names___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_ndim___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_output_nr___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_real___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_requires_grad___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_retains_grad___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_shape___get__ (__main__.TestTorchFunctionOverride) ... ok
test__TensorBase_volatile___get__ (__main__.TestTorchFunctionOverride) ... ok
test__assert_async (__main__.TestTorchFunctionOverride) ... ok
test__rowwise_prune (__main__.TestTorchFunctionOverride) ... ok
test_abs (__main__.TestTorchFunctionOverride) ... ok
test_absolute (__main__.TestTorchFunctionOverride) ... ok
test_acos (__main__.TestTorchFunctionOverride) ... ok
test_acosh (__main__.TestTorchFunctionOverride) ... ok
test_adaptive_avg_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_adaptive_max_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_add (__main__.TestTorchFunctionOverride) ... ok
test_addbmm (__main__.TestTorchFunctionOverride) ... ok
test_addcdiv (__main__.TestTorchFunctionOverride) ... ok
test_addcmul (__main__.TestTorchFunctionOverride) ... ok
test_addmm (__main__.TestTorchFunctionOverride) ... ok
test_addmv (__main__.TestTorchFunctionOverride) ... ok
test_addr (__main__.TestTorchFunctionOverride) ... ok
test_affine_grid_generator (__main__.TestTorchFunctionOverride) ... ok
test_all (__main__.TestTorchFunctionOverride) ... ok
test_allclose (__main__.TestTorchFunctionOverride) ... ok
test_alpha_dropout (__main__.TestTorchFunctionOverride) ... ok
test_amax (__main__.TestTorchFunctionOverride) ... ok
test_amin (__main__.TestTorchFunctionOverride) ... ok
test_aminmax (__main__.TestTorchFunctionOverride) ... ok
test_angle (__main__.TestTorchFunctionOverride) ... ok
test_any (__main__.TestTorchFunctionOverride) ... ok
test_arccos (__main__.TestTorchFunctionOverride) ... ok
test_arccosh (__main__.TestTorchFunctionOverride) ... ok
test_arcsin (__main__.TestTorchFunctionOverride) ... ok
test_arcsinh (__main__.TestTorchFunctionOverride) ... ok
test_arctan (__main__.TestTorchFunctionOverride) ... ok
test_arctanh (__main__.TestTorchFunctionOverride) ... ok
test_argmax (__main__.TestTorchFunctionOverride) ... ok
test_argmin (__main__.TestTorchFunctionOverride) ... ok
test_argsort (__main__.TestTorchFunctionOverride) ... ok
test_asin (__main__.TestTorchFunctionOverride) ... ok
test_asinh (__main__.TestTorchFunctionOverride) ... ok
test_atan (__main__.TestTorchFunctionOverride) ... ok
test_atan2 (__main__.TestTorchFunctionOverride) ... ok
test_atanh (__main__.TestTorchFunctionOverride) ... ok
test_avg_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_baddbmm (__main__.TestTorchFunctionOverride) ... ok
test_base (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_backward_elemt (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_backward_reduce (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_elemt (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_gather_stats (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_gather_stats_with_counts (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_stats (__main__.TestTorchFunctionOverride) ... ok
test_batch_norm_update_stats (__main__.TestTorchFunctionOverride) ... ok
test_bernoulli (__main__.TestTorchFunctionOverride) ... ok
test_bilinear (__main__.TestTorchFunctionOverride) ... ok
test_binary_cross_entropy_with_logits (__main__.TestTorchFunctionOverride) ... ok
test_bincount (__main__.TestTorchFunctionOverride) ... ok
test_binomial (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_and (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_left_shift (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_not (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_or (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_right_shift (__main__.TestTorchFunctionOverride) ... ok
test_bitwise_xor (__main__.TestTorchFunctionOverride) ... ok
test_bmm (__main__.TestTorchFunctionOverride) ... ok
test_broadcast_to (__main__.TestTorchFunctionOverride) ... ok
test_bucketize (__main__.TestTorchFunctionOverride) ... ok
test_cat (__main__.TestTorchFunctionOverride) ... ok
test_ceil (__main__.TestTorchFunctionOverride) ... ok
test_celu (__main__.TestTorchFunctionOverride) ... ok
test_channel_shuffle (__main__.TestTorchFunctionOverride) ... ok
test_cholesky (__main__.TestTorchFunctionOverride) ... ok
test_cholesky_inverse (__main__.TestTorchFunctionOverride) ... ok
test_cholesky_solve (__main__.TestTorchFunctionOverride) ... ok
test_choose_qparams_optimized (__main__.TestTorchFunctionOverride) ... ok
test_chunk (__main__.TestTorchFunctionOverride) ... ok
test_clamp (__main__.TestTorchFunctionOverride) ... ok
test_clamp_max (__main__.TestTorchFunctionOverride) ... ok
test_clamp_min (__main__.TestTorchFunctionOverride) ... ok
test_clip (__main__.TestTorchFunctionOverride) ... ok
test_clone (__main__.TestTorchFunctionOverride) ... ok
test_column_stack (__main__.TestTorchFunctionOverride) ... ok
test_combinations (__main__.TestTorchFunctionOverride) ... ok
test_complex (__main__.TestTorchFunctionOverride) ... ok
test_concat (__main__.TestTorchFunctionOverride) ... ok
test_conj (__main__.TestTorchFunctionOverride) ... ok
test_conj_physical (__main__.TestTorchFunctionOverride) ... ok
test_constant_pad_nd (__main__.TestTorchFunctionOverride) ... ok
test_conv1d (__main__.TestTorchFunctionOverride) ... ok
test_conv2d (__main__.TestTorchFunctionOverride) ... ok
test_conv3d (__main__.TestTorchFunctionOverride) ... ok
test_conv_tbc (__main__.TestTorchFunctionOverride) ... ok
test_conv_transpose1d (__main__.TestTorchFunctionOverride) ... ok
test_conv_transpose2d (__main__.TestTorchFunctionOverride) ... ok
test_conv_transpose3d (__main__.TestTorchFunctionOverride) ... ok
test_convolution (__main__.TestTorchFunctionOverride) ... ok
test_copysign (__main__.TestTorchFunctionOverride) ... ok
test_corrcoef (__main__.TestTorchFunctionOverride) ... ok
test_cos (__main__.TestTorchFunctionOverride) ... ok
test_cosh (__main__.TestTorchFunctionOverride) ... ok
test_cosine_embedding_loss (__main__.TestTorchFunctionOverride) ... ok
test_cosine_similarity (__main__.TestTorchFunctionOverride) ... ok
test_count_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_cov (__main__.TestTorchFunctionOverride) ... ok
test_cross (__main__.TestTorchFunctionOverride) ... ok
test_ctc_loss (__main__.TestTorchFunctionOverride) ... ok
test_cummax (__main__.TestTorchFunctionOverride) ... ok
test_cummin (__main__.TestTorchFunctionOverride) ... ok
test_cumprod (__main__.TestTorchFunctionOverride) ... ok
test_cumsum (__main__.TestTorchFunctionOverride) ... ok
test_cumulative_trapezoid (__main__.TestTorchFunctionOverride) ... ok
test_deg2rad (__main__.TestTorchFunctionOverride) ... ok
test_dequantize (__main__.TestTorchFunctionOverride) ... ok
test_det (__main__.TestTorchFunctionOverride) ... ok
test_detach (__main__.TestTorchFunctionOverride) ... ok
test_diag (__main__.TestTorchFunctionOverride) ... ok
test_diag_embed (__main__.TestTorchFunctionOverride) ... ok
test_diagflat (__main__.TestTorchFunctionOverride) ... ok
test_diagonal (__main__.TestTorchFunctionOverride) ... ok
test_diff (__main__.TestTorchFunctionOverride) ... ok
test_digamma (__main__.TestTorchFunctionOverride) ... ok
test_dist (__main__.TestTorchFunctionOverride) ... ok
test_div (__main__.TestTorchFunctionOverride) ... ok
test_divide (__main__.TestTorchFunctionOverride) ... ok
test_dot (__main__.TestTorchFunctionOverride) ... ok
test_dropout (__main__.TestTorchFunctionOverride) ... ok
test_dsmm (__main__.TestTorchFunctionOverride) ... ok
test_dsplit (__main__.TestTorchFunctionOverride) ... ok
test_dstack (__main__.TestTorchFunctionOverride) ... ok
test_eig (__main__.TestTorchFunctionOverride) ... ok
test_embedding (__main__.TestTorchFunctionOverride) ... ok
test_embedding_bag (__main__.TestTorchFunctionOverride) ... ok
test_empty_like (__main__.TestTorchFunctionOverride) ... ok
test_eq (__main__.TestTorchFunctionOverride) ... ok
test_equal (__main__.TestTorchFunctionOverride) ... ok
test_erf (__main__.TestTorchFunctionOverride) ... ok
test_erfc (__main__.TestTorchFunctionOverride) ... ok
test_erfinv (__main__.TestTorchFunctionOverride) ... ok
test_exp (__main__.TestTorchFunctionOverride) ... ok
test_exp2 (__main__.TestTorchFunctionOverride) ... ok
test_expm1 (__main__.TestTorchFunctionOverride) ... ok
test_fake_quantize_per_channel_affine (__main__.TestTorchFunctionOverride) ... ok
test_fake_quantize_per_tensor_affine (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_linear_fp16_weight (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_linear_fp16_weight_fp32_activation (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_linear_int8_weight (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_linear_int8_weight_fp32_activation (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_linear_quantize_weight (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_pack_gemm_matrix_fp16 (__main__.TestTorchFunctionOverride) ... ok
test_fbgemm_pack_quantized_matrix (__main__.TestTorchFunctionOverride) ... ok
test_feature_alpha_dropout (__main__.TestTorchFunctionOverride) ... ok
test_feature_dropout (__main__.TestTorchFunctionOverride) ... ok
test_fix (__main__.TestTorchFunctionOverride) ... ok
test_flatten (__main__.TestTorchFunctionOverride) ... ok
test_flip (__main__.TestTorchFunctionOverride) ... ok
test_fliplr (__main__.TestTorchFunctionOverride) ... ok
test_flipud (__main__.TestTorchFunctionOverride) ... ok
test_float_power (__main__.TestTorchFunctionOverride) ... ok
test_floor (__main__.TestTorchFunctionOverride) ... ok
test_floor_divide (__main__.TestTorchFunctionOverride) ... ok
test_fmax (__main__.TestTorchFunctionOverride) ... ok
test_fmin (__main__.TestTorchFunctionOverride) ... ok
test_fmod (__main__.TestTorchFunctionOverride) ... ok
test_frac (__main__.TestTorchFunctionOverride) ... ok
test_frexp (__main__.TestTorchFunctionOverride) ... ok
test_frobenius_norm (__main__.TestTorchFunctionOverride) ... ok
test_full_like (__main__.TestTorchFunctionOverride) ... ok
test_fused_moving_avg_obs_fake_quant (__main__.TestTorchFunctionOverride) ... ok
test_gather (__main__.TestTorchFunctionOverride) ... ok
test_gcd (__main__.TestTorchFunctionOverride) ... ok
test_ge (__main__.TestTorchFunctionOverride) ... ok
test_geqrf (__main__.TestTorchFunctionOverride) ... ok
test_ger (__main__.TestTorchFunctionOverride) ... ok
test_gradient (__main__.TestTorchFunctionOverride) ... ok
test_greater (__main__.TestTorchFunctionOverride) ... ok
test_greater_equal (__main__.TestTorchFunctionOverride) ... ok
test_grid_sampler (__main__.TestTorchFunctionOverride) ... ok
test_grid_sampler_2d (__main__.TestTorchFunctionOverride) ... ok
test_grid_sampler_3d (__main__.TestTorchFunctionOverride) ... ok
test_group_norm (__main__.TestTorchFunctionOverride) ... ok
test_gru (__main__.TestTorchFunctionOverride) ... ok
test_gru_cell (__main__.TestTorchFunctionOverride) ... ok
test_gt (__main__.TestTorchFunctionOverride) ... ok
test_hardshrink (__main__.TestTorchFunctionOverride) ... ok
test_heaviside (__main__.TestTorchFunctionOverride) ... ok
test_hinge_embedding_loss (__main__.TestTorchFunctionOverride) ... ok
test_histc (__main__.TestTorchFunctionOverride) ... ok
test_histogram (__main__.TestTorchFunctionOverride) ... ok
test_hsmm (__main__.TestTorchFunctionOverride) ... ok
test_hsplit (__main__.TestTorchFunctionOverride) ... ok
test_hstack (__main__.TestTorchFunctionOverride) ... ok
test_hypot (__main__.TestTorchFunctionOverride) ... ok
test_i0 (__main__.TestTorchFunctionOverride) ... ok
test_igamma (__main__.TestTorchFunctionOverride) ... ok
test_igammac (__main__.TestTorchFunctionOverride) ... ok
test_imag (__main__.TestTorchFunctionOverride) ... ok
test_index_add (__main__.TestTorchFunctionOverride) ... ok
test_index_copy (__main__.TestTorchFunctionOverride) ... ok
test_index_fill (__main__.TestTorchFunctionOverride) ... ok
test_index_put (__main__.TestTorchFunctionOverride) ... ok
test_index_select (__main__.TestTorchFunctionOverride) ... ok
test_inner (__main__.TestTorchFunctionOverride) ... ok
test_instance_norm (__main__.TestTorchFunctionOverride) ... ok
test_int_repr (__main__.TestTorchFunctionOverride) ... ok
test_inverse (__main__.TestTorchFunctionOverride) ... ok
test_is_complex (__main__.TestTorchFunctionOverride) ... ok
test_is_conj (__main__.TestTorchFunctionOverride) ... ok
test_is_distributed (__main__.TestTorchFunctionOverride) ... ok
test_is_floating_point (__main__.TestTorchFunctionOverride) ... ok
test_is_inference (__main__.TestTorchFunctionOverride) ... ok
test_is_neg (__main__.TestTorchFunctionOverride) ... ok
test_is_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_is_same_size (__main__.TestTorchFunctionOverride) ... ok
test_is_signed (__main__.TestTorchFunctionOverride) ... ok
test_isclose (__main__.TestTorchFunctionOverride) ... ok
test_isfinite (__main__.TestTorchFunctionOverride) ... ok
test_isin (__main__.TestTorchFunctionOverride) ... ok
test_isinf (__main__.TestTorchFunctionOverride) ... ok
test_isnan (__main__.TestTorchFunctionOverride) ... ok
test_isneginf (__main__.TestTorchFunctionOverride) ... ok
test_isposinf (__main__.TestTorchFunctionOverride) ... ok
test_isreal (__main__.TestTorchFunctionOverride) ... ok
test_kl_div (__main__.TestTorchFunctionOverride) ... ok
test_kron (__main__.TestTorchFunctionOverride) ... ok
test_kthvalue (__main__.TestTorchFunctionOverride) ... ok
test_layer_norm (__main__.TestTorchFunctionOverride) ... ok
test_lcm (__main__.TestTorchFunctionOverride) ... ok
test_ldexp (__main__.TestTorchFunctionOverride) ... ok
test_le (__main__.TestTorchFunctionOverride) ... ok
test_lerp (__main__.TestTorchFunctionOverride) ... ok
test_less (__main__.TestTorchFunctionOverride) ... ok
test_less_equal (__main__.TestTorchFunctionOverride) ... ok
test_lgamma (__main__.TestTorchFunctionOverride) ... ok
test_log (__main__.TestTorchFunctionOverride) ... ok
test_log10 (__main__.TestTorchFunctionOverride) ... ok
test_log1p (__main__.TestTorchFunctionOverride) ... ok
test_log2 (__main__.TestTorchFunctionOverride) ... ok
test_log_softmax (__main__.TestTorchFunctionOverride) ... ok
test_logaddexp (__main__.TestTorchFunctionOverride) ... ok
test_logaddexp2 (__main__.TestTorchFunctionOverride) ... ok
test_logcumsumexp (__main__.TestTorchFunctionOverride) ... ok
test_logdet (__main__.TestTorchFunctionOverride) ... ok
test_logical_and (__main__.TestTorchFunctionOverride) ... ok
test_logical_not (__main__.TestTorchFunctionOverride) ... ok
test_logical_or (__main__.TestTorchFunctionOverride) ... ok
test_logical_xor (__main__.TestTorchFunctionOverride) ... ok
test_logit (__main__.TestTorchFunctionOverride) ... ok
test_logsumexp (__main__.TestTorchFunctionOverride) ... ok
test_lstm (__main__.TestTorchFunctionOverride) ... ok
test_lstm_cell (__main__.TestTorchFunctionOverride) ... ok
test_lstsq (__main__.TestTorchFunctionOverride) ... ok
test_lt (__main__.TestTorchFunctionOverride) ... ok
test_lu_solve (__main__.TestTorchFunctionOverride) ... ok
test_lu_unpack (__main__.TestTorchFunctionOverride) ... ok
test_margin_ranking_loss (__main__.TestTorchFunctionOverride) ... ok
test_masked_fill (__main__.TestTorchFunctionOverride) ... ok
test_masked_scatter (__main__.TestTorchFunctionOverride) ... ok
test_masked_select (__main__.TestTorchFunctionOverride) ... ok
test_matmul (__main__.TestTorchFunctionOverride) ... ok
test_matrix_exp (__main__.TestTorchFunctionOverride) ... ok
test_matrix_power (__main__.TestTorchFunctionOverride) ... ok
test_matrix_rank (__main__.TestTorchFunctionOverride) ... ok
test_max (__main__.TestTorchFunctionOverride) ... ok
test_max_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_max_pool1d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_max_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_max_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_maximum (__main__.TestTorchFunctionOverride) ... ok
test_mean (__main__.TestTorchFunctionOverride) ... ok
test_mean_semantics (__main__.TestTorchFunctionOverride)
Test that a function with one argument can be overrided ... ok
test_median (__main__.TestTorchFunctionOverride) ... ok
test_min (__main__.TestTorchFunctionOverride) ... ok
test_minimum (__main__.TestTorchFunctionOverride) ... ok
test_miopen_batch_norm (__main__.TestTorchFunctionOverride) ... ok
test_miopen_convolution (__main__.TestTorchFunctionOverride) ... ok
test_miopen_convolution_transpose (__main__.TestTorchFunctionOverride) ... ok
test_miopen_depthwise_convolution (__main__.TestTorchFunctionOverride) ... ok
test_miopen_rnn (__main__.TestTorchFunctionOverride) ... ok
test_mm_semantics (__main__.TestTorchFunctionOverride)
Test that a function with multiple arguments can be overrided ... ok
test_mode (__main__.TestTorchFunctionOverride) ... ok
test_moveaxis (__main__.TestTorchFunctionOverride) ... ok
test_movedim (__main__.TestTorchFunctionOverride) ... ok
test_msort (__main__.TestTorchFunctionOverride) ... ok
test_mul (__main__.TestTorchFunctionOverride) ... ok
test_multinomial (__main__.TestTorchFunctionOverride) ... ok
test_multiply (__main__.TestTorchFunctionOverride) ... ok
test_mv (__main__.TestTorchFunctionOverride) ... ok
test_mvlgamma (__main__.TestTorchFunctionOverride) ... ok
test_nan_to_num (__main__.TestTorchFunctionOverride) ... ok
test_nanmedian (__main__.TestTorchFunctionOverride) ... ok
test_nanquantile (__main__.TestTorchFunctionOverride) ... ok
test_nansum (__main__.TestTorchFunctionOverride) ... ok
test_narrow (__main__.TestTorchFunctionOverride) ... ok
test_narrow_copy (__main__.TestTorchFunctionOverride) ... ok
test_native_batch_norm (__main__.TestTorchFunctionOverride) ... ok
test_native_group_norm (__main__.TestTorchFunctionOverride) ... ok
test_native_layer_norm (__main__.TestTorchFunctionOverride) ... ok
test_native_norm (__main__.TestTorchFunctionOverride) ... ok
test_ne (__main__.TestTorchFunctionOverride) ... ok
test_neg (__main__.TestTorchFunctionOverride) ... ok
test_negative (__main__.TestTorchFunctionOverride) ... ok
test_nextafter (__main__.TestTorchFunctionOverride) ... ok
test_nonzero (__main__.TestTorchFunctionOverride) ... ok
test_norm_except_dim (__main__.TestTorchFunctionOverride) ... ok
test_not_equal (__main__.TestTorchFunctionOverride) ... ok
test_nuclear_norm (__main__.TestTorchFunctionOverride) ... ok
test_numel (__main__.TestTorchFunctionOverride) ... ok
test_ones_like (__main__.TestTorchFunctionOverride) ... ok
test_orgqr (__main__.TestTorchFunctionOverride) ... ok
test_ormqr (__main__.TestTorchFunctionOverride) ... ok
test_outer (__main__.TestTorchFunctionOverride) ... ok
test_pairwise_distance (__main__.TestTorchFunctionOverride) ... ok
test_pdist (__main__.TestTorchFunctionOverride) ... ok
test_permute (__main__.TestTorchFunctionOverride) ... ok
test_pinverse (__main__.TestTorchFunctionOverride) ... ok
test_pixel_shuffle (__main__.TestTorchFunctionOverride) ... ok
test_pixel_unshuffle (__main__.TestTorchFunctionOverride) ... ok
test_poisson (__main__.TestTorchFunctionOverride) ... ok
test_poisson_nll_loss (__main__.TestTorchFunctionOverride) ... ok
test_polar (__main__.TestTorchFunctionOverride) ... ok
test_polygamma (__main__.TestTorchFunctionOverride) ... ok
test_positive (__main__.TestTorchFunctionOverride) ... ok
test_pow (__main__.TestTorchFunctionOverride) ... ok
test_precedence_semantics (__main__.TestTorchFunctionOverride)
Test semantics for __torch_function__ for functions that take ... ok
test_prelu (__main__.TestTorchFunctionOverride) ... ok
test_prod (__main__.TestTorchFunctionOverride) ... ok
test_put (__main__.TestTorchFunctionOverride) ... ok
test_q_per_channel_axis (__main__.TestTorchFunctionOverride) ... ok
test_q_per_channel_scales (__main__.TestTorchFunctionOverride) ... ok
test_q_per_channel_zero_points (__main__.TestTorchFunctionOverride) ... ok
test_q_scale (__main__.TestTorchFunctionOverride) ... ok
test_q_zero_point (__main__.TestTorchFunctionOverride) ... ok
test_qr (__main__.TestTorchFunctionOverride) ... ok
test_quantile (__main__.TestTorchFunctionOverride) ... ok
test_quantize_per_channel (__main__.TestTorchFunctionOverride) ... ok
test_quantize_per_tensor (__main__.TestTorchFunctionOverride) ... ok
test_quantized_batch_norm (__main__.TestTorchFunctionOverride) ... ok
test_quantized_gru_cell (__main__.TestTorchFunctionOverride) ... ok
test_quantized_lstm_cell (__main__.TestTorchFunctionOverride) ... ok
test_quantized_max_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_quantized_max_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_quantized_rnn_relu_cell (__main__.TestTorchFunctionOverride) ... ok
test_quantized_rnn_tanh_cell (__main__.TestTorchFunctionOverride) ... ok
test_rad2deg (__main__.TestTorchFunctionOverride) ... ok
test_rand_like (__main__.TestTorchFunctionOverride) ... ok
test_randint_like (__main__.TestTorchFunctionOverride) ... ok
test_randn_like (__main__.TestTorchFunctionOverride) ... ok
test_ravel (__main__.TestTorchFunctionOverride) ... ok
test_real (__main__.TestTorchFunctionOverride) ... ok
test_reciprocal (__main__.TestTorchFunctionOverride) ... ok
test_relu (__main__.TestTorchFunctionOverride) ... ok
test_remainder (__main__.TestTorchFunctionOverride) ... ok
test_renorm (__main__.TestTorchFunctionOverride) ... ok
test_repeat_interleave (__main__.TestTorchFunctionOverride) ... ok
test_reshape (__main__.TestTorchFunctionOverride) ... ok
test_resolve_conj (__main__.TestTorchFunctionOverride) ... ok
test_resolve_neg (__main__.TestTorchFunctionOverride) ... ok
test_rnn_relu (__main__.TestTorchFunctionOverride) ... ok
test_rnn_relu_cell (__main__.TestTorchFunctionOverride) ... ok
test_rnn_tanh (__main__.TestTorchFunctionOverride) ... ok
test_rnn_tanh_cell (__main__.TestTorchFunctionOverride) ... ok
test_roll (__main__.TestTorchFunctionOverride) ... ok
test_rot90 (__main__.TestTorchFunctionOverride) ... ok
test_round (__main__.TestTorchFunctionOverride) ... ok
test_row_stack (__main__.TestTorchFunctionOverride) ... ok
test_rrelu (__main__.TestTorchFunctionOverride) ... ok
test_rsqrt (__main__.TestTorchFunctionOverride) ... ok
test_rsub (__main__.TestTorchFunctionOverride) ... ok
test_saddmm (__main__.TestTorchFunctionOverride) ... ok
test_scatter (__main__.TestTorchFunctionOverride) ... ok
test_scatter_add (__main__.TestTorchFunctionOverride) ... ok
test_searchsorted (__main__.TestTorchFunctionOverride) ... ok
test_segment_reduce (__main__.TestTorchFunctionOverride) ... ok
test_select (__main__.TestTorchFunctionOverride) ... ok
test_selu (__main__.TestTorchFunctionOverride) ... ok
test_sgn (__main__.TestTorchFunctionOverride) ... ok
test_sigmoid (__main__.TestTorchFunctionOverride) ... ok
test_sign (__main__.TestTorchFunctionOverride) ... ok
test_signbit (__main__.TestTorchFunctionOverride) ... ok
test_sin (__main__.TestTorchFunctionOverride) ... ok
test_sinc (__main__.TestTorchFunctionOverride) ... ok
test_sinh (__main__.TestTorchFunctionOverride) ... ok
test_slogdet (__main__.TestTorchFunctionOverride) ... ok
test_smm (__main__.TestTorchFunctionOverride) ... ok
test_softmax (__main__.TestTorchFunctionOverride) ... ok
test_solve (__main__.TestTorchFunctionOverride) ... ok
test_sort (__main__.TestTorchFunctionOverride) ... ok
test_split_with_sizes (__main__.TestTorchFunctionOverride) ... ok
test_sqrt (__main__.TestTorchFunctionOverride) ... ok
test_square (__main__.TestTorchFunctionOverride) ... ok
test_squeeze (__main__.TestTorchFunctionOverride) ... ok
test_stack (__main__.TestTorchFunctionOverride) ... ok
test_std (__main__.TestTorchFunctionOverride) ... ok
test_std_mean (__main__.TestTorchFunctionOverride) ... ok
test_sub (__main__.TestTorchFunctionOverride) ... ok
test_subtract (__main__.TestTorchFunctionOverride) ... ok
test_sum (__main__.TestTorchFunctionOverride) ... ok
test_svd (__main__.TestTorchFunctionOverride) ... ok
test_swapaxes (__main__.TestTorchFunctionOverride) ... ok
test_swapdims (__main__.TestTorchFunctionOverride) ... ok
test_symeig (__main__.TestTorchFunctionOverride) ... ok
test_t (__main__.TestTorchFunctionOverride) ... ok
test_take (__main__.TestTorchFunctionOverride) ... ok
test_take_along_dim (__main__.TestTorchFunctionOverride) ... ok
test_tan (__main__.TestTorchFunctionOverride) ... ok
test_tanh (__main__.TestTorchFunctionOverride) ... ok
test_tensor_split (__main__.TestTorchFunctionOverride) ... ok
test_tensor_subclass_propagation (__main__.TestTorchFunctionOverride)
this test exercises the functionality described in ... ok
test_threshold (__main__.TestTorchFunctionOverride) ... ok
test_tile (__main__.TestTorchFunctionOverride) ... ok
test_topk (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_fft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_fft2 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_fftn (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_fftshift (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_hfft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_ifft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_ifft2 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_ifftn (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_ifftshift (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_ihfft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_irfft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_irfft2 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_irfftn (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_rfft (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_rfft2 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__fft_fft_rfftn (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_cholesky (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_cholesky_ex (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_cond (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_det (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_eig (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_eigh (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_eigvals (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_eigvalsh (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_householder_product (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_inv (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_inv_ex (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_lstsq (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_matmul (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_matrix_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_matrix_power (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_matrix_rank (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_multi_dot (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_pinv (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_qr (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_slogdet (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_solve (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_svd (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_svdvals (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_tensorinv (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_tensorsolve (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__linalg_linalg_vector_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_avg_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_avg_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_log_sigmoid (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_one_hot (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_softplus (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__nn_softshrink (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_digamma (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_entr (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_erf (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_erfc (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_erfcx (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_erfinv (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_exp2 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_expit (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_expm1 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_gammainc (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_gammaincc (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_gammaln (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_i0 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_i0e (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_i1 (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_i1e (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_log1p (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_log_softmax (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_logit (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_logsumexp (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_multigammaln (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_ndtr (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_ndtri (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_polygamma (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_psi (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_round (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_sinc (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_xlog1py (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_xlogy (__main__.TestTorchFunctionOverride) ... ok
test_torch__C__special_special_zeta (__main__.TestTorchFunctionOverride) ... ok
test_torch__lobpcg_lobpcg (__main__.TestTorchFunctionOverride) ... ok
test_torch__lowrank_pca_lowrank (__main__.TestTorchFunctionOverride) ... ok
test_torch__lowrank_svd_lowrank (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_atleast_1d (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_atleast_2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_atleast_3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_block_diag (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_broadcast_tensors (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_cartesian_prod (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_cdist (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_chain_matmul (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_einsum (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_istft (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_lu (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_meshgrid (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_split (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_stft (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_tensordot (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_unique (__main__.TestTorchFunctionOverride) ... ok
test_torch_functional_unique_consecutive (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional__pad (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional__threshold (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_avg_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_avg_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool1d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool2d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_adaptive_max_pool3d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_affine_grid (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_alpha_dropout (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_batch_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_bilinear (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_binary_cross_entropy (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_binary_cross_entropy_with_logits (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_celu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_cosine_embedding_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_cross_entropy (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_ctc_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_dropout (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_dropout2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_dropout3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_elu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_embedding (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_embedding_bag (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_feature_alpha_dropout (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_fold (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_fractional_max_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_fractional_max_pool2d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_fractional_max_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_fractional_max_pool3d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_gaussian_nll_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_gelu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_glu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_grid_sample (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_group_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_gumbel_softmax (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_hardshrink (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_hardtanh (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_hinge_embedding_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_huber_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_instance_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_interpolate (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_kl_div (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_l1_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_layer_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_leaky_relu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_linear (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_local_response_norm (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_log_softmax (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_lp_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_lp_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_margin_ranking_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool1d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool1d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool2d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_pool3d_with_indices (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_unpool1d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_unpool2d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_max_unpool3d (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_mish (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_mse_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_multi_head_attention_forward (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_multi_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_multilabel_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_multilabel_soft_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_nll_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_normalize (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_pairwise_distance (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_poisson_nll_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_prelu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_relu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_relu6 (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_rrelu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_selu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_silu (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_smooth_l1_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_soft_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_softmax (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_softmin (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_softsign (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_tanhshrink (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_triplet_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_triplet_margin_with_distance_loss (__main__.TestTorchFunctionOverride) ... ok
test_torch_nn_functional_unfold (__main__.TestTorchFunctionOverride) ... ok
test_trace (__main__.TestTorchFunctionOverride) ... ok
test_transpose (__main__.TestTorchFunctionOverride) ... ok
test_trapezoid (__main__.TestTorchFunctionOverride) ... ok
test_trapz (__main__.TestTorchFunctionOverride) ... ok
test_triangular_solve (__main__.TestTorchFunctionOverride) ... ok
test_tril (__main__.TestTorchFunctionOverride) ... ok
test_triplet_margin_loss (__main__.TestTorchFunctionOverride) ... ok
test_triu (__main__.TestTorchFunctionOverride) ... ok
test_true_divide (__main__.TestTorchFunctionOverride) ... ok
test_trunc (__main__.TestTorchFunctionOverride) ... ok
test_unbind (__main__.TestTorchFunctionOverride) ... ok
test_unsafe_chunk (__main__.TestTorchFunctionOverride) ... ok
test_unsafe_split (__main__.TestTorchFunctionOverride) ... ok
test_unsafe_split_with_sizes (__main__.TestTorchFunctionOverride) ... ok
test_unsqueeze (__main__.TestTorchFunctionOverride) ... ok
test_user_implementation_raises (__main__.TestTorchFunctionOverride)
Test that errors raised in user implementations propagate correctly ... ok
test_var (__main__.TestTorchFunctionOverride) ... ok
test_var_mean (__main__.TestTorchFunctionOverride) ... ok
test_vdot (__main__.TestTorchFunctionOverride) ... ok
test_view_as_complex (__main__.TestTorchFunctionOverride) ... ok
test_view_as_real (__main__.TestTorchFunctionOverride) ... ok
test_vsplit (__main__.TestTorchFunctionOverride) ... ok
test_vstack (__main__.TestTorchFunctionOverride) ... ok
test_where (__main__.TestTorchFunctionOverride) ... ok
test_xlogy (__main__.TestTorchFunctionOverride) ... ok
test_zeros_like (__main__.TestTorchFunctionOverride) ... ok
test_wrap_torch_function (__main__.TestWrapTorchFunction) ... ok

----------------------------------------------------------------------
Ran 1271 tests in 0.504s

OK
Running test_package ... [2021-10-12 10:40:08.872913]
Executing ['/opt/conda/bin/python3.6', 'test_package.py', '-v'] ... [2021-10-12 10:40:08.872986]
test_trace_dependencies (test_analyze.TestAnalyze) ... ok
test_allow_empty_with_error (test_dependency_api.TestDependencyAPI)
If an error occurs during packaging, it should not be shadowed by the allow_empty error. ... ok
test_broken_dependency (test_dependency_api.TestDependencyAPI)
A unpackageable dependency should raise a PackagingError. ... ok
test_deny (test_dependency_api.TestDependencyAPI) ... ok
test_deny_glob (test_dependency_api.TestDependencyAPI) ... ok
test_extern (test_dependency_api.TestDependencyAPI) ... ok
test_extern_glob (test_dependency_api.TestDependencyAPI) ... ok
test_extern_glob_allow_empty (test_dependency_api.TestDependencyAPI) ... ok
test_implicit_intern (test_dependency_api.TestDependencyAPI)
The save_module APIs should implicitly intern the module being saved. ... ok
test_intern_error (test_dependency_api.TestDependencyAPI)
Failure to handle all dependencies should lead to an error. ... ok
test_invalid_import (test_dependency_api.TestDependencyAPI)
An incorrectly-formed import should raise a PackagingError. ... ok
test_mock (test_dependency_api.TestDependencyAPI) ... skipped 'mock uses __getattr__ a 3.7 feature'
test_mock_glob (test_dependency_api.TestDependencyAPI) ... skipped 'mock uses __getattr__ a 3.7 feature'
test_mock_glob_allow_empty (test_dependency_api.TestDependencyAPI) ... ok
test_pickle_mocked (test_dependency_api.TestDependencyAPI) ... skipped 'mock uses __getattr__ a 3.7 feature'
test_repackage_mocked_module (test_dependency_api.TestDependencyAPI)
Re-packaging a package that contains a mocked module should work correctly. ... skipped 'mock uses __getattr__ a 3.7 feature'
test_extern_and_mock_hook (test_dependency_hooks.TestDependencyHooks) ... ok
test_multiple_extern_hooks (test_dependency_hooks.TestDependencyHooks) ... ok
test_multiple_mock_hooks (test_dependency_hooks.TestDependencyHooks) ... ok
test_remove_hooks (test_dependency_hooks.TestDependencyHooks) ... ok
test_single_hook (test_dependency_hooks.TestDependencyHooks) ... ok
test_contains (test_digraph.TestDiGraph) ... ok
test_contains_non_hashable (test_digraph.TestDiGraph) ... ok
test_edges (test_digraph.TestDiGraph) ... ok
test_iter (test_digraph.TestDiGraph) ... ok
test_node_attr_update (test_digraph.TestDiGraph) ... ok
test_node_attrs (test_digraph.TestDiGraph) ... ok
test_predecessor_not_in_graph (test_digraph.TestDiGraph) ... ok
test_predecessors (test_digraph.TestDiGraph) ... ok
test_successor_not_in_graph (test_digraph.TestDiGraph) ... ok
test_successors (test_digraph.TestDiGraph) ... ok
test_importer_access (test_directory_reader.DirectoryReaderTest) ... skipped 'ResourceReader API introduced in Python 3.7'
test_loading_has_record (test_directory_reader.DirectoryReaderTest) ... ok
test_loading_module (test_directory_reader.DirectoryReaderTest) ... ok
test_loading_pickle (test_directory_reader.DirectoryReaderTest) ... ok
test_package_resource_access (test_directory_reader.DirectoryReaderTest)
Packaged modules should be able to use the importlib.resources API to access ... skipped 'ResourceReader API introduced in Python 3.7'
test_resource_access_by_path (test_directory_reader.DirectoryReaderTest) ... skipped 'ResourceReader API introduced in Python 3.7'
test_resource_reader (test_directory_reader.DirectoryReaderTest)
Tests DirectoryReader as the base for get_resource_reader. ... skipped 'ResourceReader API introduced in Python 3.7'
test_scriptobject_failure_message (test_directory_reader.DirectoryReaderTest) ... ok
test_exclude (test_glob_group.TestGlobGroup) ... ok
test_exclude_from_all (test_glob_group.TestGlobGroup) ... ok
test_invalid_raw (test_glob_group.TestGlobGroup) ... ok
test_list_include_exclude (test_glob_group.TestGlobGroup) ... ok
test_one_star (test_glob_group.TestGlobGroup) ... ok
test_one_star_middle (test_glob_group.TestGlobGroup) ... ok
test_one_star_multiple_in_component (test_glob_group.TestGlobGroup) ... ok
test_one_star_partial (test_glob_group.TestGlobGroup) ... ok
test_one_star_partial_extension (test_glob_group.TestGlobGroup) ... ok
test_raw_two_star (test_glob_group.TestGlobGroup) ... ok
test_two_star (test_glob_group.TestGlobGroup) ... ok
test_two_star_end (test_glob_group.TestGlobGroup) ... ok
test_two_star_middle (test_glob_group.TestGlobGroup) ... ok
test_two_star_multiple (test_glob_group.TestGlobGroup) ... ok
test_ordered_importer_basic (test_importer.TestImporter) ... ok
test_ordered_importer_whichmodule (test_importer.TestImporter)
OrderedImporter's implementation of whichmodule should try each ... ok
test_package_importer_whichmodule_no_dunder_module (test_importer.TestImporter)
Exercise corner case where we try to pickle an object whose ... ok
test_single_ordered_importer (test_importer.TestImporter) ... ok
test_sys_importer (test_importer.TestImporter) ... ok
test_sys_importer_roundtrip (test_importer.TestImporter) ... ok
test_demangle_base (test_mangling.TestMangling) ... ok
test_demangler_multiple_manglers (test_mangling.TestMangling) ... ok
test_is_mangled (test_mangling.TestMangling) ... ok
test_mangle_empty_errors (test_mangling.TestMangling) ... ok
test_mangle_prefix (test_mangling.TestMangling) ... ok
test_mangler_is_consistent (test_mangling.TestMangling) ... ok
test_roundtrip_mangling (test_mangling.TestMangling) ... ok
test_unique_manglers (test_mangling.TestMangling) ... ok
test_unique_module_names (test_mangling.TestMangling) ... ok
test_dunder_package_present (test_misc.TestMisc) ... ok
test_dunder_package_works_from_package (test_misc.TestMisc) ... ok
test_exporter_content_lists (test_misc.TestMisc) ... ok
test_file_structure (test_misc.TestMisc) ... ok
test_file_structure_has_file (test_misc.TestMisc) ... ok
test_inspect_class (test_misc.TestMisc)
Should be able to retrieve source for a packaged class. ... ok
test_is_from_package (test_misc.TestMisc)
is_from_package should work for objects and modules ... ok
test_std_lib_sys_hackery_checks (test_misc.TestMisc) ... ok
test_model_save (test_model.ModelTest) ... ok
test_resnet (test_model.ModelTest) ... ok
test_script_resnet (test_model.ModelTest) ... ok
test_package_fx_custom_tracer (test_package_fx.TestPackageFX) ... ok
test_package_fx_package (test_package_fx.TestPackageFX) ... ok
test_package_fx_simple (test_package_fx.TestPackageFX) ... ok
test_package_fx_with_imports (test_package_fx.TestPackageFX) ... ok
test_package_then_fx (test_package_fx.TestPackageFX) ... ok
test_different_package_interface (test_package_script.TestPackageScript)
Test a case where the interface defined in the package is ... ok
test_different_package_script_class (test_package_script.TestPackageScript)
Test a case where the script class defined in the package is ... ok
test_load_shared_scriptmodules (test_package_script.TestPackageScript) ... ok
test_load_shared_tensors (test_package_script.TestPackageScript) ... ok
test_load_shared_tensors_repackaged (test_package_script.TestPackageScript) ... ok
test_mixing_packaged_and_inline_modules (test_package_script.TestPackageScript) ... ok
test_mixing_packaged_and_inline_modules_shared_code (test_package_script.TestPackageScript) ... ok
test_package_interface (test_package_script.TestPackageScript)
Packaging an interface class should work correctly. ... ok
test_package_script_class (test_package_script.TestPackageScript) ... ok
test_save_eager_mods_sharing_scriptmodule (test_package_script.TestPackageScript) ... ok
test_save_independent_scriptmodules (test_package_script.TestPackageScript) ... ok
test_save_repeat_scriptmodules (test_package_script.TestPackageScript) ... ok
test_save_scriptmodule (test_package_script.TestPackageScript) ... ok
test_save_scriptmodule_file (test_package_script.TestPackageScript) ... ok
test_save_scriptmodule_only_necessary_code (test_package_script.TestPackageScript) ... ok
test_save_scriptmodule_with_submods (test_package_script.TestPackageScript) ... ok
test_save_scriptmodules_in_container (test_package_script.TestPackageScript) ... ok
test_save_scriptmodules_submod_redefinition (test_package_script.TestPackageScript) ... ok
test_save_shared_tensors (test_package_script.TestPackageScript) ... ok
test_saving_and_scripting_packaged_mod (test_package_script.TestPackageScript) ... ok
test_scriptmodules_repeat_save (test_package_script.TestPackageScript) ... ok
test_tensor_sharing_pickle (test_package_script.TestPackageScript)
Test that saving a ScriptModule and a separately saving a tensor ... ok
test_importer_access (test_resources.TestResources) ... skipped 'ResourceReader API introduced in Python 3.7'
test_package_resource_access (test_resources.TestResources)
Packaged modules should be able to use the importlib.resources API to access ... skipped 'ResourceReader API introduced in Python 3.7'
test_resource_access_by_path (test_resources.TestResources) ... skipped 'ResourceReader API introduced in Python 3.7'
test_resource_reader (test_resources.TestResources)
Test compliance with the get_resource_reader importlib API. ... skipped 'ResourceReader API introduced in Python 3.7'
test_bad_dunder_imports (test_save_load.TestSaveLoad)
Test to ensure bad __imports__ don't cause PackageExporter to fail. ... ok
test_dunder_imports (test_save_load.TestSaveLoad) ... ok
test_exporting_mismatched_code (test_save_load.TestSaveLoad) ... ok
test_pickle (test_save_load.TestSaveLoad) ... ok
test_save_imported_module_fails (test_save_load.TestSaveLoad) ... ok
test_save_module (test_save_load.TestSaveLoad) ... ok
test_save_module_binary (test_save_load.TestSaveLoad) ... ok
test_saving_source (test_save_load.TestSaveLoad) ... ok
test_saving_string (test_save_load.TestSaveLoad) ... ok

----------------------------------------------------------------------
Ran 119 tests in 4.288s

OK (skipped=12)
Running test_profiler ... [2021-10-12 10:40:15.735354]
Executing ['/opt/conda/bin/python3.6', 'test_profiler.py', '-v'] ... [2021-10-12 10:40:15.735436]
test_export_stacks (__main__.TestProfiler) ... ok
test_flops (__main__.TestProfiler) ... ok
test_high_level_trace (__main__.TestProfiler)
Checks that python side high level events are recorded. ... ok
test_kineto (__main__.TestProfiler) ... ok
test_kineto_multigpu (__main__.TestProfiler) ... skipped 'Not supported on ROCm'
test_kineto_profiler_api (__main__.TestProfiler) ... ok
test_memory_profiler (__main__.TestProfiler) ... ok
test_module_hierarchy (__main__.TestProfiler) ... ok
test_profiler_metadata (__main__.TestProfiler) ... ok
test_profiler_tracing (__main__.TestProfiler) ... ok
test_source (__main__.TestProfiler)
Checks that source code attribution works for eager, TS and autograd mode ... ok
test_tensorboard_trace_handler (__main__.TestProfiler) ... ok
test_mem_leak (__main__.TestProfilerCUDA)
Checks that there's no memory leak when using profiler with CUDA ... ok

----------------------------------------------------------------------
Ran 13 tests in 10.407s

OK (skipped=1)
Running test_pruning_op ... [2021-10-12 10:40:30.520900]
Executing ['/opt/conda/bin/python3.6', 'test_pruning_op.py', '-v'] ... [2021-10-12 10:40:30.520984]
Running test_public_bindings ... [2021-10-12 10:40:32.790926]
Executing ['/opt/conda/bin/python3.6', 'test_public_bindings.py', '-v'] ... [2021-10-12 10:40:32.790999]
test_no_new_bindings (__main__.TestPublicBindings) ... ok

----------------------------------------------------------------------
Ran 1 test in 0.000s

OK
Running test_pytree ... [2021-10-12 10:40:35.055164]
Executing ['/opt/conda/bin/python3.6', 'test_pytree.py', '-v'] ... [2021-10-12 10:40:35.055248]
test_broadcast_to_and_flatten (__main__.TestPytree) ... ok
test_flatten_unflatten_dict (__main__.TestPytree) ... ok
test_flatten_unflatten_leaf (__main__.TestPytree) ... ok
test_flatten_unflatten_list (__main__.TestPytree) ... ok
test_flatten_unflatten_namedtuple (__main__.TestPytree) ... ok
test_flatten_unflatten_nested (__main__.TestPytree) ... ok
test_flatten_unflatten_torch_namedtuple_return_type (__main__.TestPytree) ... ok
test_flatten_unflatten_tuple (__main__.TestPytree) ... ok
test_treemap (__main__.TestPytree) ... ok
test_treespec_equality (__main__.TestPytree) ... ok
test_treespec_repr (__main__.TestPytree) ... ok

----------------------------------------------------------------------
Ran 11 tests in 0.029s

OK
Running test_quantization ... [2021-10-12 10:40:37.330911]
Executing ['/opt/conda/bin/python3.6', 'test_quantization.py', '-v'] ... [2021-10-12 10:40:37.330991]
test_function_import (quantization.ao_migration.test_quantize.TestAOMigrationQuantizePy) ... ok
test_package_import (quantization.ao_migration.test_quantize.TestAOMigrationQuantizePy) ... ok
test_conv_chain (quantization.eager.test_bias_correction_eager.TestBiasCorrection) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
ok
test_linear_chain (quantization.eager.test_bias_correction_eager.TestBiasCorrection) ... ok
test_compare_tensor_scalar (quantization.core.test_quantized_op.TestComparatorOps) ... ok
test_compare_tensor_tensor (quantization.core.test_quantized_op.TestComparatorOps) ... ok
test_conv_bn_folded_vs_unfolded (quantization.eager.test_quantize_eager_qat.TestConvBNQATModule) ... ok
test_conv_bn_relu (quantization.eager.test_quantize_eager_qat.TestConvBNQATModule) ... ok
test_erase_class_tensor_shapes (quantization.jit.test_deprecated_jit_quant.TestDeprecatedJitQuantized) ... ok
test_quantization_modules (quantization.jit.test_deprecated_jit_quant.TestDeprecatedJitQuantized) ... ok
test_rnn_cell_quantized (quantization.jit.test_deprecated_jit_quant.TestDeprecatedJitQuantized) ... /opt/conda/lib/python3.6/site-packages/torch/jit/quantized.py:477: UserWarning: quantize_rnn_cell_modules function has been deprecated. Please use torch.quantization.quantize_dynamic API instead.
  warnings.warn("quantize_rnn_cell_modules function has been deprecated. "
ok
test_rnn_quantized (quantization.jit.test_deprecated_jit_quant.TestDeprecatedJitQuantized) ... /opt/conda/lib/python3.6/site-packages/torch/jit/quantized.py:523: UserWarning: quantize_rnn_modules function has been deprecated. Please use torch.quantization.quantize_dynamic API instead.
  warnings.warn("quantize_rnn_modules function has been deprecated. "
ok
test_device_affinity (quantization.core.test_workflow_module.TestDistributed) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
ok
test_fake_quant_preserves_buffers (quantization.core.test_workflow_module.TestDistributed) ... ok
test_observers_preserve_buffers (quantization.core.test_workflow_module.TestDistributed) ... ok
test_qat_convbn_fused_syncbn_replacement (quantization.core.test_workflow_module.TestDistributed) ... ok
test_qat_data_parallel (quantization.core.test_workflow_module.TestDistributed) ... ok
test_syncbn_preserves_qconfig (quantization.core.test_workflow_module.TestDistributed) ... ok
test_qlinear (quantization.core.test_quantized_op.TestDynamicQuantizedLinear) ... [W TensorImpl.h:1378] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())
ok
test_qlinear_dynamic_fp16 (quantization.core.test_quantized_op.TestDynamicQuantizedLinear) ... ok
test_qlinear_legacy (quantization.core.test_quantized_op.TestDynamicQuantizedLinear) ... ok
test_cell_api (quantization.core.test_quantized_module.TestDynamicQuantizedModule) ... ok
test_gru_api (quantization.core.test_quantized_module.TestDynamicQuantizedModule) ... ok
test_linear_api (quantization.core.test_quantized_module.TestDynamicQuantizedModule) ... ok
test_lstm_api (quantization.core.test_quantized_module.TestDynamicQuantizedModule) ... ok
test_qlstmGRU (quantization.core.test_quantized_op.TestDynamicQuantizedRNNOp) ... ok
test_qrnncell (quantization.core.test_quantized_op.TestDynamicQuantizedRNNOp) ... ok
test_leaky_relu (quantization.eager.test_quantize_eager_ptq.TestEagerModeActivationOps) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/utils.py:158: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  "Returning default values."
ok
test_relu (quantization.eager.test_quantize_eager_ptq.TestEagerModeActivationOps) ... ok
test_compare_model_outputs_conv_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
ok
test_compare_model_outputs_functional_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:1109: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point 
  Returning default scale and zero point "
ok
test_compare_model_outputs_linear_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_outputs_linear_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_outputs_lstm_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_conv_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_functional_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_linear_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_linear_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_lstm_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_partial (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_model_stub_submodule_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_weights_conv_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_weights_linear_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_weights_linear_static (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_compare_weights_lstm_dynamic (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_output_logger (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_shadow_logger (quantization.eager.test_numeric_suite_eager.TestEagerModeNumericSuite) ... ok
test_converged (quantization.eager.test_equalize_eager.TestEqualizeEager)
Sanity checks on _equalize.converged working ... ok
test_cross_layer_equalization (quantization.eager.test_equalize_eager.TestEqualizeEager)
applies _equalize.cross_layer_equalization on two modules and checks ... ok
test_equalize (quantization.eager.test_equalize_eager.TestEqualizeEager)
First checks to see if _equalize.equalize can handle multiple ... ok
test_equalize_fused_convrelu (quantization.eager.test_equalize_eager.TestEqualizeEager)
Checks to see if eager mode equalization supports fused ... ok
test_equalize_fused_linearrelu (quantization.eager.test_equalize_eager.TestEqualizeEager)
Checks to see if eager mode equalization supports fused ... ok
test_input_weight_eq_observer (quantization.fx.test_equalize_fx.TestEqualizeFx) ... ok
test_input_weight_equalization_activation_values (quantization.fx.test_equalize_fx.TestEqualizeFx)
After applying the equalization functions check if the input ... ok
test_input_weight_equalization_branching (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that graphs containing branches are prepared correctly. ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/prepare.py:506: UserWarning: Cannot equalize linear1 because it is part of a branch.
  f"Cannot equalize {node} because it is part of a branch."
/opt/conda/lib/python3.6/site-packages/torch/quantization/fx/prepare.py:506: UserWarning: Cannot equalize linear2 because it is part of a branch.
  f"Cannot equalize {node} because it is part of a branch."
ok
test_input_weight_equalization_convert (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that the modified model for equalization (before quantization) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node linear1_packed_weight_0 target linear1_packed_weight_0 linear1_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node linear2_packed_weight_0 target linear2_packed_weight_0 linear2_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node linear_packed_weight_0 target linear_packed_weight_0 linear_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node conv1_packed_weight_0 target conv1_packed_weight_0 conv1_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node conv2_packed_weight_0 target conv2_packed_weight_0 conv2_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node conv_packed_weight_0 target conv_packed_weight_0 conv_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_input_weight_equalization_equalization_scales (quantization.fx.test_equalize_fx.TestEqualizeFx)
After applying the equalization functions, check if the equalization ... ok
test_input_weight_equalization_graphs (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that the modified model for equalization has the same graph ... /opt/conda/lib/python3.6/site-packages/torch/quantization/utils.py:150: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  "Returning default values."
/opt/conda/lib/python3.6/site-packages/torch/quantization/fx/_equalize.py:183: UserWarning: Must run observer before calling calculate_equalization_scale. Returning default equalization scale torch.tensor(1).
  "Returning default equalization scale torch.tensor(1)."
/opt/conda/lib/python3.6/site-packages/torch/quantization/fx/_equalize.py:98: UserWarning: Must call calculate_equalization_scale before calling calculate_scaled_minmax. Will not scale the next quantization observer.
  "Will not scale the next quantization observer."
ok
test_input_weight_equalization_prepare (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that graphs created after prepare_fx is as expected ... ok
test_input_weight_equalization_results (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that for small models, the results of quantized models that ... ok
test_input_weight_equalization_weights_bias (quantization.fx.test_equalize_fx.TestEqualizeFx)
After applying the equalization functions check if the weights and ... ok
test_selective_equalization (quantization.fx.test_equalize_fx.TestEqualizeFx)
Tests that we are able to run numeric suite on the equalized model ... ok
test_dict_return_type (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_matching_failure_node_count (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_matching_failure_node_type (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_methods (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... skipped 'Broken by https://github.com/pytorch/pytorch/pull/62608, need dtype inference support'
test_nodes_before_cat (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_nodes_with_equal_types_get_matched (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... skipped 'Broken by https://github.com/pytorch/pytorch/pull/62608, need dtype inference support'
test_op_relationship_mapping (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_results_order (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_simple_fun (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_0 target _packed_weight_0 _packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_simple_fusion (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_simple_mod (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_simple_mod_multi (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_simple_tensor_ops (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_user_defined_function (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcher) ... ok
test_mobilenet_v2 (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcherModels) ... ok
test_mobilenet_v2_qat (quantization.fx.test_numeric_suite_fx.TestFXGraphMatcherModels) ... ok
test_add_mul_inputs_activations (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_add_shadow_loggers_fun_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_1 target _packed_weight_1 _packed_weight_1 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_add_shadow_loggers_fun_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/utils.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  module.register_buffer(attr_name, torch.tensor(value, device=device))
ok
test_add_shadow_loggers_meth_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... skipped 'Broken by https://github.com/pytorch/pytorch/pull/62608, enable afterdtype inference is supported'
test_add_shadow_loggers_mod_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... skipped 'Broken by https://github.com/pytorch/pytorch/pull/62608, enable afterdtype inference is supported'
test_add_shadow_loggers_mod_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_add_shadow_loggers_multiple_dtype_casts (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extend_logger_results_with_comparison (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_conv_fun_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_2 target _packed_weight_2 _packed_weight_2 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_3 target _packed_weight_3 _packed_weight_3 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_4 target _packed_weight_4 _packed_weight_4 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node _packed_weight_5 target _packed_weight_5 _packed_weight_5 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_extract_weights_conv_fun_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_dynamic (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_fqn (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_linear_fun_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_linear_fun_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_mod_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_extract_weights_mod_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_int8_shadows_fp32_coverage (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_int8_shadows_fp32_simple (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_int8_shadows_int8_fun (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_int8_shadows_int8_mod (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_layer_names (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... skipped 'Broken by https://github.com/pytorch/pytorch/pull/62608, enable afterdtype inference is supported'
test_linear_fp16_activations (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_linear_fp16_shadow_activations (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_linear_fp16_vs_linear_fp16_shadow_activations (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_linear_fp16_weights (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_loggers_preserve_qat_numerics (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_logging_inputs (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_fqn (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_fun_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_fun_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_meth_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_mod_ptq (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_match_activations_mod_qat (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_op_io_dtype_coverage (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_op_with_either_fp32_or_int8_input (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... skipped 'TODO: broken by https://github.com/pytorch/pytorch/pull/61687, will enable later'
test_ops_with_same_fp32_and_int8_signature (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_shadow_activations_fqn (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_shadow_loggers_preserve_qat_numerics (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_user_defined_function (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
ok
test_user_module (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_user_module_scriptable (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIs) ... ok
test_compare_activations_conv (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_activations_linear (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_activations_lstm_dynamic (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_shadow_activations_conv (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_shadow_activations_linear (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_shadow_activations_lstm_dynamic (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_weights_conv (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_weights_linear (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_compare_weights_lstm_dynamic (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_mobilenet_v2 (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... skipped 'TODO: broken by https://github.com/pytorch/pytorch/pull/61687, will enable later'
test_resnet18 (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... skipped 'TODO: broken by https://github.com/pytorch/pytorch/pull/61687, will enable later'
test_sparsenn_compare_activations (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:1198: UserWarning: dtype combination: (torch.quint8, torch.qint8, None) is not supported by Embedding/EmbeddingBag, supported dtype combinations are: [(torch.float32, torch.quint8, None), (torch.float32, torch.quint4x2, None)]
  "supported dtype combinations are: {}".format(dtypes, supported_dtypes))
ok
test_sparsenn_shadow (quantization.fx.test_numeric_suite_fx.TestFXNumericSuiteCoreAPIsModels) ... ok
test_fq_module_per_channel (quantization.core.test_workflow_module.TestFakeQuantize) ... ok
test_fq_serializable_per_channel (quantization.core.test_workflow_module.TestFakeQuantize) ... ok
test_backward_per_channel (quantization.core.test_workflow_ops.TestFakeQuantizeOps)
Tests the backward method. ... ok
test_backward_per_channel_cachemask_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_backward_per_channel_cachemask_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_backward_per_tensor (quantization.core.test_workflow_ops.TestFakeQuantizeOps)
Tests the backward method. ... skipped 'temporarily disable the test'
test_backward_per_tensor_cachemask_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_backward_per_tensor_cachemask_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_fake_quant_control (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_fake_quant_preserves_qparam_shapes_for_activations (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_fixed_qparams_fq_module (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_backward_per_tensor_with_amp (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_channel (quantization.core.test_workflow_ops.TestFakeQuantizeOps)
Tests the forward path of the FakeQuantizePerTensorAffine op. ... ok
test_forward_per_channel_cachemask_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_channel_cachemask_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_channel_half_precision_numerics (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_tensor (quantization.core.test_workflow_ops.TestFakeQuantizeOps)
Tests the forward path of the FakeQuantizePerTensorAffine op. ... ok
test_forward_per_tensor_cachemask_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_tensor_cachemask_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_forward_per_tensor_half_precision_numerics (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_fq_module_per_tensor (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_fq_serializable_per_tensor (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_backward_per_channel_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_backward_per_channel_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_backward_per_tensor_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_backward_per_tensor_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_forward_per_channel_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_forward_per_channel_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_forward_per_tensor_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_learnable_forward_per_tensor_cuda (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_numerical_consistency_per_channel (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_numerical_consistency_per_tensor (quantization.core.test_workflow_ops.TestFakeQuantizeOps) ... ok
test_functional_module (quantization.eager.test_quantize_eager_ptq.TestFunctionalModule) ... ok
test_fuse_conv_bn_relu (quantization.fx.test_quantize_fx.TestFuseFx) ... ok
test_fuse_custom_config_dict_validity (quantization.fx.test_quantize_fx.TestFuseFx) ... ok
test_fuse_module_relu (quantization.fx.test_quantize_fx.TestFuseFx) ... ok
test_problematic_fuse_example (quantization.fx.test_quantize_fx.TestFuseFx) ... ok
test_qconfig_fused_module (quantization.fx.test_quantize_fx.TestFuseFx) ... ok
test_fused_backward_op_fake_quant_off (quantization.core.test_workflow_ops.TestFusedObsFakeQuant) ... ok
test_fused_obs_fake_quant_backward_op (quantization.core.test_workflow_ops.TestFusedObsFakeQuant) ... ok
test_fused_obs_fake_quant_moving_avg (quantization.core.test_workflow_ops.TestFusedObsFakeQuant) ... ok
test_fused_obs_fake_quant_moving_avg_per_channel (quantization.core.test_workflow_ops.TestFusedObsFakeQuant) ... ok
test_compare_fused_obs_fq_oss_module (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_default_fused_qat_config (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_fused_mod_per_channel (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_fused_mod_reduce_range (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_fused_obs_fq_module (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_fused_obs_fq_moving_avg_module (quantization.core.test_workflow_module.TestFusedObsFakeQuantModule) ... ok
test_forward_hooks_preserved (quantization.eager.test_fusion.TestFusion)
Test case that checks whether forward pre hooks of the first module and ... ok
test_fuse_module_eval (quantization.eager.test_fusion.TestFusion) ... ok
test_fuse_module_train (quantization.eager.test_fusion.TestFusion) ... ok
test_fusion_conv_with_bias (quantization.eager.test_fusion.TestFusion) ... ok
test_fusion_linear_bn_eval (quantization.eager.test_fusion.TestFusion) ... ok
test_fusion_sequential_model_eval (quantization.eager.test_fusion.TestFusion) ... ok
test_fusion_sequential_model_train (quantization.eager.test_fusion.TestFusion) ... ok
test_quantized_add_relu_fusion (quantization.jit.test_fusion_passes.TestFusionPasses) ... ok
test_histogram_observer (quantization.core.test_workflow_module.TestHistogramObserver) ... ok
test_histogram_observer_against_reference (quantization.core.test_workflow_module.TestHistogramObserver) ... ok
test_histogram_observer_one_sided (quantization.core.test_workflow_module.TestHistogramObserver) ... ok
test_histogram_observer_same_inputs (quantization.core.test_workflow_module.TestHistogramObserver) ... ok
test_observer_scriptable (quantization.core.test_workflow_module.TestHistogramObserver) ... ok
test_fake_quant_true_quant_compare (quantization.eager.test_model_numerics.TestModelNumericsEager) ... ok
test_float_quant_compare_per_channel (quantization.eager.test_model_numerics.TestModelNumericsEager) ... ok
test_float_quant_compare_per_tensor (quantization.eager.test_model_numerics.TestModelNumericsEager) ... ok
test_weight_only_activation_only_fakequant (quantization.eager.test_model_numerics.TestModelNumericsEager) ... ok
test_histogram_observer_consistent_buffer_shape (quantization.core.test_workflow_module.TestObserver) ... ok
test_histogram_observer_save_load_state_dict (quantization.core.test_workflow_module.TestObserver) ... ok
test_observer_qparams_respects_device_affinity (quantization.core.test_workflow_module.TestObserver) ... ok
test_observer_scriptable (quantization.core.test_workflow_module.TestObserver) ... ok
test_per_channel_observers (quantization.core.test_workflow_module.TestObserver) ... ok
test_per_tensor_observers (quantization.core.test_workflow_module.TestObserver) ... ok
test_save_load_state_dict_script (quantization.core.test_workflow_module.TestObserver) ... ok
test_state_dict_respects_device_affinity (quantization.core.test_workflow_module.TestObserver) ... ok
test_zero_numel (quantization.core.test_workflow_module.TestObserver) ... ok
test_constant_padNd (quantization.core.test_quantized_op.TestPadding) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:492: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
ok
test_reflection_pad1d (quantization.core.test_quantized_op.TestPadding) ... ok
test_reflection_pad2d (quantization.core.test_quantized_op.TestPadding) ... ok
test_forward_hooks_preserved (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test post-training dynamic quantization on preserving ... ok
test_linear_relu_fusion (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic) ... ok
test_nested1 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test quantization for nested model, top level 'fc3' and ... ok
test_nested2 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Another test case for quantized, we will quantize all submodules ... ok
test_nested3 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
More complicated nested test case with child qconfig overrides ... ok
test_per_channel_linear_quantize (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test quantization for per_channel dynamic quantization ... ok
test_quantized_rnn (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test dynamic quantization, scriptability and serialization for dynamic quantized lstm modules on int8 and fp16 ... ok
test_quantized_rnn_cell (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test dynamic quantization, scriptability and serialization for dynamic quantized rnn cell modules on int8 and fp16 ... ok
test_single_layer (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Dynamic Quantize SingleLayerLinearDynamicModel which has one Linear module, ... ok
test_two_layers (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
TwoLayerLinearModel has two Linear modules but we only quantize the second one ... ok
test_type_match_rule (quantization.eager.test_quantize_eager_ptq.TestPostTrainingDynamic)
Test quantization for nested model, top level 'fc3' and ... ok
test_activations (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_convtranspose_per_channel_fails_early (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_convtranspose_per_channel_qconfig_none (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_custom_module_class (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_dequant_stub (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_embedding_linear_dynamic (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_forward_hooks_preserved (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_manual (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
User inserts QuantStub and DeQuantStub in model code ... ok
test_nested1 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Test quantization for nested model, top level 'fc3' and ... ok
test_nested2 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... ok
test_nested3 (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
More complicated nested test case with child qconfig overrides ... ok
test_normalization (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic) ... /opt/conda/lib/python3.6/site-packages/torch/ao/quantization/quantize.py:251: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules
  warnings.warn("None of the submodule got qconfig applied. Make sure you "
ok
test_quantized_embedding (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Test the post-training quantization flow, serialization and scripting ... ok
test_quantized_embedding_bag (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Test the post-training quantization flow, serialization and scripting ... ok
test_resnet_base (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Test quantization for bottleneck topology used in resnet/resnext ... ok
test_save_load_state_dict (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Test PTQ flow of creating a model and quantizing it and saving the quantized state_dict ... ok
test_single_layer (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
Quantize SingleLayerLinearModel which has one Linear module, make sure it is swapped ... ok
test_skip_quant (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
The case when we want to skip quantizing some layers ... ok
test_two_layers (quantization.eager.test_quantize_eager_ptq.TestPostTrainingStatic)
TwoLayerLinearModel has two Linear modules but we only quantize the second one ... ok
test_fixed_qparam_ops (quantization.eager.test_quantize_eager_qat.TestQATActivationOps) ... ok
test_leaky_relu (quantization.eager.test_quantize_eager_qat.TestQATActivationOps) ... ok
test_relu (quantization.eager.test_quantize_eager_qat.TestQATActivationOps) ... ok
test_adaptive_avg_pool2d (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_avg_pool2d (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_hardtanh (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_mean (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_add (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_maxpool2d (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_relu (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_sigmoid (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_sigmoid_sweep (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_qnnpack_tanh (quantization.core.test_quantized_op.TestQNNPackOps) ... ok
test_add_scalar_uses_input_qparams (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining) ... ok
test_conv_linear (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining) ... ok
test_eval_only_fake_quant (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining)
Using FakeQuant in evaluation only mode, ... ok
test_forward_hooks_preserved (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining) ... ok
test_manual (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining) ... ok
test_mul_scalar_uses_input_qparams (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining) ... ok
test_train_save_load_eval (quantization.eager.test_quantize_eager_qat.TestQuantizationAwareTraining)
Test QAT flow of creating a model, doing QAT and saving the quantized state_dict ... ok
test_embedding_bag (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitOps) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:1012: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /var/lib/jenkins/pytorch/build/aten/src/ATen/core/TensorBody.h:420.)
  return self._grad
ok
test_embedding_bag_padding_idx_error (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitOps) ... ok
test_linear (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitOps) ... ok
test_convert_dynamic_fp16 (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_dynamic_multi_op (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... [W insert_quant_dequant.cpp:1323] Warning: debug option for add_scalar and mul_scalar is not supported, please don't use debug option for models that uses these ops. (function operator())
ok
test_dynamic_quant_multi_uses (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_dynamic_shared_weights (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_dynamic_weight_observer (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_dynamic_with_if (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_insert_quant_dequant_linear_dynamic (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_prepare_dynamic (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_prepare_dynamic_child_qconfig (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_quantize_dynamic_fp16 (quantization.jit.test_quantize_jit.TestQuantizeDynamicJitPasses) ... ok
test_assert_on_size_after_quant_layer (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_attention (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_conv_bn_relu (quantization.fx.test_quantize_fx.TestQuantizeFx) ... /opt/conda/lib/python3.6/site-packages/torch/nn/quantized/_reference/modules/conv.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/opt/conda/lib/python3.6/site-packages/torch/nn/quantized/_reference/modules/conv.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["zero_point"], dtype=torch.int, device=device))
ok
test_conv_linear_not_reference (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test quantizing conv and linear ... /opt/conda/lib/python3.6/site-packages/torch/nn/quantized/_reference/modules/linear.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/opt/conda/lib/python3.6/site-packages/torch/nn/quantized/_reference/modules/linear.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dtype=torch.int, device=device))
ok
test_conv_linear_reference (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test quantizing functional conv and linear with reference option ... ok
test_convert_custom_config_dict_validity (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_convtranspose_per_channel_fails_early (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_copy_node_has_shared_actpp_instance (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test the output of CopyNode to have the same ... ok
test_custom_module_class (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_deepcopy_preserve_attributes (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_default_quant_after_none_qconfig (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure default quant is inserted properly ... ok
test_dequantize (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test to make sure dequantize node are placed before ... ok
test_dict_output (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure quantization runs for models with dictionary output ... ok
test_dynamic_quant_fp16 (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_dynamic_quant_weight_observer (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test that weight observer is run in convert step ... ok
test_dynamic_with_fusion (quantization.fx.test_quantize_fx.TestQuantizeFx) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node mods2_packed_weight_0 target mods2_packed_weight_0 mods2_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_fold_quant_dequant (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test that the sequence of quant-dequant nodes in the ... ok
test_fp32_input_fp32_output (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_fp32_input_quantized_output (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_fp32_sum (quantization.fx.test_quantize_fx.TestQuantizeFx) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:1341: UserWarning: dtype combination: (torch.quint8, torch.qint8, None) is not supported by <built-in method sum of type object at 0x7f004903d420> supported dtype combinations are: [(torch.float16, torch.float16, None)]
  "supported dtype combinations are: {}".format(dtypes, self.op, default_op_supported_dtypes[self.op]))
ok
test_fusion_pattern_unquantized (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_getattr_with_nontensor_result (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_linear_qint8_activation (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test support for qint8 activation in reference pattern ... ok
test_lowering (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_mul_add_fp16_config (quantization.fx.test_quantize_fx.TestQuantizeFx) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:486: UserWarning: dtype combination: (torch.float32, torch.float16, None) is not supported by <built-in function mul> for is_reference=False. Supported non-reference dtype combinations are: [(torch.quint8, torch.qint8, None), (torch.float16, torch.float16, None)] 
  binary_op_supported_dtypes[self.binary_op]
/opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:486: UserWarning: dtype combination: (torch.float32, torch.float16, None) is not supported by <built-in function add> for is_reference=False. Supported non-reference dtype combinations are: [(torch.quint8, torch.qint8, None), (torch.float16, torch.float16, None)] 
  binary_op_supported_dtypes[self.binary_op]
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node mods1_0_packed_weight_0 target mods1_0_packed_weight_0 mods1_0_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node mods1_1_packed_weight_0 target mods1_1_packed_weight_0 mods1_1_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_no_obs_between_unmatched_node_and_copy_node (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_non_traceable_module (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_not_used (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test quantizing a not used value ... ok
test_output_lists_and_dicts (quantization.fx.test_quantize_fx.TestQuantizeFx)
Verify that specifying complicated output types does not crash. ... ok
test_packed_weight_fused_op (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_pattern_match (quantization.fx.test_quantize_fx.TestQuantizeFx)
test MatchAllNode with ... ok
test_prepare_custom_config_dict_validity (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_prepared_model_deepcopy (quantization.fx.test_quantize_fx.TestQuantizeFx)
Ensures that copy.deepcopy works correctly on a prepared model. ... ok
test_preserve_attributes (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_preserve_qconfig (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_preserve_tuple (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test tuple input type is preserved ... ok
test_qat_and_script (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qat_prepare_device_affinity (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_dict_validity (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_for_call_func (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_for_call_method (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_function (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_module_name_object_type_order (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_module_name_regex (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_module_type (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_none (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_precedence (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qconfig_qat_module_type (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qparams_buffers (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_qparams_fqn (quantization.fx.test_quantize_fx.TestQuantizeFx)
Test that the FQN of input_scale/zero_point is set ... ok
test_quant_output_always_observed (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_quantized_input_fp32_output (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_quantized_input_quantized_output (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_ref_conv_module (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure the numerics for models with ref conv module ... ok
test_ref_linear_module (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure the numerics for models with ref linear module ... ok
test_remove_qconfig (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_return_none (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_save_observer_state_dict (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_sequential (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_shape_followed_by_quantized_op (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure that shape does not dequantize ... ok
test_standalone_module_float_interface (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_standalone_module_quantized_interface (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_state_dict (quantization.fx.test_quantize_fx.TestQuantizeFx)
Make sure packed params appear in state_dict ... ok
test_sub_scalar (quantization.fx.test_quantize_fx.TestQuantizeFx) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:486: UserWarning: dtype combination: (torch.quint8, torch.qint8, None) is not supported by <built-in function sub> for is_reference=False. Supported non-reference dtype combinations are: [(torch.float16, torch.float16, None)] 
  binary_op_supported_dtypes[self.binary_op]
ok
test_trace_quantize_per_tensor (quantization.fx.test_quantize_fx.TestQuantizeFx) ... ok
test_prepare_serialize_switch_device_convert (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... ok
test_qat_functional_linear (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... ok
test_resnet18_ddp (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... skipped 'TODO: Test is always failing - https://github.com/pytorch/pytorch/issues/54979'
test_resnet_base (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... ok
test_static_gpu_convert_basic (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... ok
test_switch_device_prepare_convert (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... ok
test_torchvision (quantization.fx.test_quantize_fx.TestQuantizeFxModels) ... skipped 'skip for now since tbb failed'
test_add (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_add_relu (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_bmm (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_bmm_int_reference (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
int8 is not supported for bmm so we won't produce reference ... ok
test_boolean_tensor (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
Make sure we don't insert observer for boolean Tensors ... ok
test_cat (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
quantization of the output of cat will depend on the ... ok
test_chunk (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_clamp (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_conv_module (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_conv_transpose_1d (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_conv_transpose_2d (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_div (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_elu (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_embedding (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_embedding_bag (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_fixed_qparams_ops (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_fixed_qparams_ops_fp16 (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_float_functional (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_functional_conv (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
Test for function conv and functional conv + relu ... ok
test_functional_linear (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_gelu_normal (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_gelu_reference (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_general_shape_ops (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
A test that checks dequantize will be swapped for ... ok
test_general_value_ops (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
A test that checks correct patterns are produced for ... ok
test_getitem (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
Make sure we only insert observer for getitem if the following node is matched ... ok
test_hardswish (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_instance_norm (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_int8_input_no_unnecessary_fq (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_layer_norm (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:1391: UserWarning: Only reference patterns are currently supported for (torch.float16, torch.float16, None) dtype with <class 'torch.nn.modules.normalization.LayerNorm'> op
  "".format(dtype=dtypes, op=self.op))
/opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:1391: UserWarning: Only reference patterns are currently supported for (torch.float16, torch.float16, None) dtype with <function layer_norm at 0x7efa96c35620> op
  "".format(dtype=dtypes, op=self.op))
ok
test_leaky_relu (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_linear_dynamic_fp16 (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_linear_module (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_linear_static_fp16 (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_mish_reference (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_mul (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_mul_relu (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_multiple_qconfigs_for_single_value (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
Test multiple qconfigs for a single value ... ok
test_norm_weight_bias (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... /opt/conda/lib/python3.6/site-packages/torch/fx/graph.py:1125: UserWarning: Node mods1_packed_weight_0 target mods1_packed_weight_0 mods1_packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
ok
test_qbatch_norm (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_qbatch_norm_relu (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_quantized_add_qat (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_quantized_conv_relu (quantization.fx.test_quantize_fx.TestQuantizeFxOps)
tests for conv1d_relu/conv2d_relu/conv3d_relu ... ok
test_quantized_mul_qat (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_reshape_fp16 (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_rnn (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_rnn_cell (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_silu_reference (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_softmax_normal (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... <string>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
ok
test_softmax_reference (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... <string>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
<string>:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
ok
test_sub (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... ok
test_sum (quantization.fx.test_quantize_fx.TestQuantizeFxOps) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/fx/quantization_patterns.py:1391: UserWarning: Only reference patterns are currently supported for (torch.float16, torch.float16, None) dtype with <built-in method sum of type object at 0x7f004903d420> op
  "".format(dtype=dtypes, op=self.op))
ok
test_conv (quantization.jit.test_quantize_jit.TestQuantizeJit) ... [W BinaryOps.cpp:601] Warning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (function operator())
ok
test_conv_bn (quantization.jit.test_quantize_jit.TestQuantizeJit) ... ok
test_conv_transpose (quantization.jit.test_quantize_jit.TestQuantizeJit) ... [W insert_observers.cpp:1536] Warning: prim::Loop is not yet supported in quantization, please make sure nothing needs to be quantized in the loop (function operator())
ok
test_linear_dynamic_fp16 (quantization.jit.test_quantize_jit.TestQuantizeJit) ... [W quant_utils.h:203] Warning: FOUND weight out of range  (function HandleWeightsSaturation)
[W quant_utils.h:203] Warning: FOUND weight out of range  (function HandleWeightsSaturation)
ok
test_nested (quantization.jit.test_quantize_jit.TestQuantizeJit) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
/opt/conda/lib/python3.6/site-packages/torch/_tensor.py:1012: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /var/lib/jenkins/pytorch/build/aten/src/ATen/core/TensorBody.h:420.)
  return self._grad
ok
test_observer_with_ignored_function (quantization.jit.test_quantize_jit.TestQuantizeJit)
Test observers with ignored function and make sure it works in ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
ok
test_single_linear (quantization.jit.test_quantize_jit.TestQuantizeJit) ... ok
test_single_linear_dynamic (quantization.jit.test_quantize_jit.TestQuantizeJit) ... ok
test_skip_quant (quantization.jit.test_quantize_jit.TestQuantizeJit) ... ok
test_cat_linear (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_clamp (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_conv_with_benchmark_flag (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... [W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
ok
test_dequantize_tuple (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
Make sure dequantize can support Tuple of tensor ... ok
test_elu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_general_shape_ops (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
A test that checks dequantize will be swapped for ... ok
test_general_value_ops (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
A test that checks correct patterns are produced for ... ok
test_group_norm (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))
ok
test_hardswish (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_instance_norm (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_layer_norm (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_linear (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_qbatch_norm (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_qbatch_norm_relu_BNFuncInplaceRelu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_qbatch_norm_relu_BNFuncRelu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_qbatch_norm_relu_BNRelu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_add (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_add_alpha (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
Test quant fusion for multiple aten::add using same ... ok
test_quantized_add_relu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_add_relu_alpha (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
Test quant fusion for multiple aten::add using same ... ok
test_quantized_add_scalar (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_add_scalar_relu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_cat (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
quantization of the output of cat will be depend on the ... ok
test_quantized_conv (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_conv_relu (quantization.jit.test_quantize_jit.TestQuantizeJitOps)
tests for conv1d_relu/conv2d_relu/conv3d_relu ... ok
test_quantized_mul (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_mul_relu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_mul_scalar (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_quantized_mul_scalar_relu (quantization.jit.test_quantize_jit.TestQuantizeJitOps) ... ok
test_conv_trace (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_dedup_module_uses (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_finalize_debug (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_finalize_for_linear (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_foldbn_complex_cases (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_foldbn_in_submodule (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_foldbn_no_fusion (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
Test that we don't fuse the cases when module type does not match ... ok
test_foldbn_shared_classtype (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_foldbn_trivial (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_foldbn_trivial_nobias (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_fuse_linear (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_inplace_option (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_child_qconfig (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_for_general_ops (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
Make sure we skip observers for ops that doesn't require ... ok
test_insert_observers_for_if (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_for_if_consistent_observation (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
check quantization for if works as long as ... ok
test_insert_observers_for_nested_if (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_for_reused_weight (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_interface (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_interface_unshare_type (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_propagate_observed (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
Make sure we propagate observed property through general ops ... ok
test_insert_observers_propagate_observed_for_function (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_propagate_observed_in_submodule (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
Make sure we propagate observed property through general ops ... ok
test_insert_observers_shared_class_type (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_skip_values (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_observers_weight_dtype (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_quant_dequant (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_insert_quant_dequant_shared_class_type (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_interface_with_fork (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_module_list (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_quantize_fork_wait (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
Tests the case where fork and wait calls are in different subgraphs ... [W utils.py:156] Warning: must run observer before calling calculate_qparams. Returning default values. (function )
ok
test_replicate_dequant_same_value (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_replicate_dequantize (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_replicate_dequantize_in_block (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_replicate_quantize_for_if (quantization.jit.test_quantize_jit.TestQuantizeJitPasses)
We want to move quantize nodes for output of prim::If ... [W utils.py:156] Warning: must run observer before calling calculate_qparams. Returning default values. (function )
[W utils.py:156] Warning: must run observer before calling calculate_qparams. Returning default values. (function )
ok
test_swap_functional_linear (quantization.jit.test_quantize_jit.TestQuantizeJitPasses) ... ok
test_lower_graph_conv2d (quantization.eager.test_quantize_eager_ptq.TestQuantizeONNXExport) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/utils.py:158: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  "Returning default values."
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:250: UserWarning: `add_node_names' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `add_node_names` argument will be ignored.
  "`{}` argument will be ignored.".format(arg_name, arg_name))
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:250: UserWarning: `do_constant_folding' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `do_constant_folding` argument will be ignored.
  "`{}` argument will be ignored.".format(arg_name, arg_name))
ok
test_lower_graph_conv3d (quantization.eager.test_quantize_eager_ptq.TestQuantizeONNXExport) ... skipped 'onnx opset9 does not support quantize_per_tensor and caffe2     does not support conv3d'
test_lower_graph_linear (quantization.eager.test_quantize_eager_ptq.TestQuantizeONNXExport) ... ok
test_qconv1d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv1d_unpack (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv2d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv2d_unpack (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv3d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv3d_unpack (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv_transpose1d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv_transpose2d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_qconv_transpose3d (quantization.core.test_quantized_op.TestQuantizedConv) ... ok
test_embedding_2d_indices (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_2bit (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_2bit_unpack (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... [W init.h:137] Caffe2 GlobalInit should be run before any other API calls.
/opt/conda/lib/python3.6/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  from imp import reload
ok
test_embedding_bag_2d_indices (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_4bit (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_4bit_unpack (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_byte (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_bag_byte_unpack (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_embedding_byte (quantization.core.test_quantized_op.TestQuantizedEmbeddingOps) ... ok
test_conv1d_api (quantization.core.test_quantized_functional.TestQuantizedFunctionalOps) ... ok
test_conv2d_api (quantization.core.test_quantized_functional.TestQuantizedFunctionalOps) ... ok
test_conv3d_api (quantization.core.test_quantized_functional.TestQuantizedFunctionalOps) ... ok
test_relu_api (quantization.core.test_quantized_functional.TestQuantizedFunctionalOps) ... ok
test_qlinear (quantization.core.test_quantized_op.TestQuantizedLinear) ... ok
test_qlinear_unpack (quantization.core.test_quantized_op.TestQuantizedLinear) ... ok
test_adaptive_avg_pool (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_adaptive_avg_pool2d_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_adaptive_avg_pool3d_ndhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_add_scalar_relu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_advanced_indexing (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_avg_pool2d (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_avg_pool2d_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_avg_pool3d (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_avg_pool3d_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_batch_norm (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_batch_norm_relu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_cat (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_cat_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_channel_shuffle (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_custom_module_lstm (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_custom_module_multi_head_attention (quantization.core.test_quantized_op.TestQuantizedOps) ... /opt/conda/lib/python3.6/site-packages/torch/quantization/observer.py:1109: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point 
  Returning default scale and zero point "
ok
test_empty_batch (quantization.core.test_quantized_op.TestQuantizedOps) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3792: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
ok
test_equal (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_group_norm (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_hardswish (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_hardtanh (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_instance_norm (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_interpolate (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_interpolate3d (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_leaky_relu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_leaky_relu_observed_output (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_linear_bias_unpack (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_max_pool1d (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_max_pool2d (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_max_pool2d_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_mean (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_mul_scalar_relu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qadd_relu_different_qparams (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qadd_relu_same_qparams (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qcelu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qclamp (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qelu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qhardsigmoid (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qlayer_norm (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qmul_broadcast (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qmul_relu_different_qparams (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qmul_relu_same_qparams (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qrelu (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qrelu6 (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qtanh (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qthreshold (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_qtopk (quantization.core.test_quantized_op.TestQuantizedOps) ... /var/lib/jenkins/pytorch/test/quantization/core/test_quantized_op.py:1615: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indices = torch.tensor(torch.from_numpy(X)).long()
ok
test_qtopk_nhwc (quantization.core.test_quantized_op.TestQuantizedOps) ... /var/lib/jenkins/pytorch/test/quantization/core/test_quantized_op.py:1641: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indices = torch.tensor(torch.from_numpy(X)).long()
ok
test_quantized_mean_qnnpack (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_sigmoid (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_sigmoid_non_observed (quantization.core.test_quantized_op.TestQuantizedOps) ... ok
test_bfp16_quantize (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_choose_qparams (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_choose_qparams_optimized (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_clone (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_compare_per_channel_device_numerics (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... skipped 'CUDA is not available'
test_compare_per_tensor_device_numerics (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... skipped 'CUDA is not available'
test_cuda_quantization_does_not_pin_memory (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... skipped 'CUDA is not available'
test_fp16_saturate_op (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... /var/lib/jenkins/pytorch/test/quantization/core/test_quantized_tensor.py:1024: UserWarning: FOUND weight out of range  (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/quantized/cpu/quant_utils.h:203.)
  y = torch._saturate_weight_to_fp16(x)
ok
test_jit_serialization (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_per_channel_qtensor_creation_cpu (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_per_channel_qtensor_creation_cuda (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_per_channel_to_device (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_per_tensor_to_device (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_pickle_checkpoint_qtensor (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qscheme_pickle (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_copy (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_cpu (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_creation (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_cuda (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_dtypes (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_fill (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_float_assignment (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_index_select_cpu (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_index_select_cuda (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_legacy_new_failure (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_load_save (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_per_channel_load_save (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_per_channel_permute (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_permute (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_quant_dequant (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_quantize_per_channel (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_reshape (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_resize (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... /opt/conda/lib/python3.6/site-packages/torch/_tensor.py:492: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
ok
test_qtensor_sub_byte (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_unsqueeze (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_qtensor_view (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_quant_pin_memory (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... skipped 'CUDA is not available'
test_quantize_per_channel_float_qparams (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_quantize_per_channel_sub_byte (quantization.core.test_quantized_tensor.TestQuantizedTensor)
Tests the per channel quantization scheme for 4-bit qtensors. ... ok
test_repeat (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_torch_qtensor_deepcopy (quantization.core.test_quantized_tensor.TestQuantizedTensor) ... ok
test_observer_scriptable (quantization.core.test_workflow_module.TestRecordHistogramObserver) ... ok
test_record_observer (quantization.core.test_workflow_module.TestRecordHistogramObserver) ... ok
test_conv2d (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_graph (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_graph_v2 (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_graph_v3 (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_nobias (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_nobias_graph (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_nobias_graph_v2 (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_nobias_graph_v3 (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv2d_relu (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv3d (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_conv3d_relu (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_default_qat_qconfig (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_linear (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_linear_dynamic (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_linear_relu (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_lstm (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_per_channel_observer (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_per_tensor_observer (quantization.bc.test_backward_compatibility.TestSerialization) ... ok
test_batch_norm2d (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the batchnorm2d module. ... ok
test_batch_norm3d (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the batchnorm3d module. ... ok
test_conv1d_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_conv2d_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_conv3d_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_elu (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the ELU module. ... ok
test_embedding_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_embedding_bag_api (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Test execution and serialization for dynamic quantized embedding_bag modules on int8 ... ok
test_group_norm (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the groupnorm module. ... ok
test_instance_norm (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the instancenorm{n}d modules. ... ok
test_layer_norm (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the layernorm module. ... ok
test_leaky_relu (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_linear_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_pool_api (quantization.core.test_quantized_module.TestStaticQuantizedModule)
Tests the correctness of the pool module. ... ok
test_quant_dequant_api (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_relu (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok
test_sigmoid (quantization.core.test_quantized_module.TestStaticQuantizedModule) ... ok

----------------------------------------------------------------------
Ran 626 tests in 365.561s

OK (skipped=16)
skipping shadow loggers for node_b: _VariableFunctionsClass.mul, start_node_a: torch._ops.quantized.PyCapsule.mul, unknown dtype cast
skipping shadow loggers for node_b: torch.quantization.observer.MinMaxObserver, start_node_a: torch._ops.quantized.PyCapsule.mul, unknown dtype cast
skipping shadow loggers for node_b: _VariableFunctionsClass.add, start_node_a: torch._ops.quantized.PyCapsule.add_relu, unknown dtype cast
skipping shadow loggers for node_b: torch.quantization.observer.MinMaxObserver, start_node_a: torch._ops.quantized.PyCapsule.add_relu, unknown dtype cast
skipping shadow loggers for node_b: torch.nn.functional.linear, start_node_a: quantization.fx.test_numeric_suite_fx._wrapped_linear, unknown dtype cast
skipping shadow loggers for node_b: torch.quantization.observer.MinMaxObserver, start_node_a: quantization.fx.test_numeric_suite_fx._wrapped_linear, unknown dtype cast
skipping shadow loggers for node_b: torch.nn.modules.sparse.EmbeddingBag, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
skipping shadow loggers for node_b: torch.quantization.observer.MinMaxObserver, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
skipping shadow loggers for node_b: torch.nn.modules.sparse.EmbeddingBag, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
skipping shadow loggers for node_b: torch.nn.modules.sparse.EmbeddingBag, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
skipping shadow loggers for node_b: torch.quantization.observer.MinMaxObserver, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
skipping shadow loggers for node_b: torch.nn.modules.sparse.EmbeddingBag, start_node_a: torch.nn.modules.sparse.EmbeddingBag, unknown dtype cast
You can add @seed(39762171245839440607525939898097434553) to this test to reproduce this failure.
Running test_reductions ... [2021-10-12 10:46:49.204241]
Executing ['/opt/conda/bin/python3.6', 'test_reductions.py', '-v'] ... [2021-10-12 10:46:49.204322]
test_accreal_type_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_all_any_cuda (__main__.TestReductionsCUDA) ... ok
test_all_any_empty_cuda (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_bool (__main__.TestReductionsCUDA) ... test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 1], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 5, 1], which does not match the required output shape [5, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 5, 1], which does not match the required output shape [1, 1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 5, 1], which does not match the required output shape [1, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 3, 2], which does not match the required output shape [1, 3, 2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 3, 2], which does not match the required output shape [1, 1, 2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
test_reductions.py:1508: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 3, 2], which does not match the required output shape [1, 1, 3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.all(x, dim, out=out)
ok
test_all_any_vs_numpy_cuda_complex128 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_complex64 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_int16 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_int8 (__main__.TestReductionsCUDA) ... ok
test_all_any_vs_numpy_cuda_uint8 (__main__.TestReductionsCUDA) ... ok
test_all_any_with_dim_cuda (__main__.TestReductionsCUDA) ... ok
test_amax_cuda_bool (__main__.TestReductionsCUDA) ... ok
test_amax_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_amax_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_amax_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_amax_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_amin_amax_some_dims_cuda (__main__.TestReductionsCUDA) ... ok
test_amin_cuda_bool (__main__.TestReductionsCUDA) ... ok
test_amin_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_amin_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_amin_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_amin_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_aminmax_cuda_bfloat16 (__main__.TestReductionsCUDA) ... ok
test_aminmax_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_aminmax_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_argminmax_axis_with_dim_one_cuda (__main__.TestReductionsCUDA) ... ok
test_argminmax_large_axis_cuda (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_float16 (__main__.TestReductionsCUDA) ... test_reductions.py:1427: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x[mask] = torch.tensor(max_val + 1, dtype=dtype)
test_reductions.py:1430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x[mask] = torch.tensor(min_val - 1, dtype=dtype)
ok
test_argminmax_multiple_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_int16 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_int8 (__main__.TestReductionsCUDA) ... ok
test_argminmax_multiple_cuda_uint8 (__main__.TestReductionsCUDA) ... ok
test_bincount_cuda (__main__.TestReductionsCUDA) ... ok
test_bucketization_cuda (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_complex128 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_complex64 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_int16 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_int8 (__main__.TestReductionsCUDA) ... ok
test_count_nonzero_cuda_uint8 (__main__.TestReductionsCUDA) ... ok
test_cumprod_integer_upcast_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_cumsum_integer_upcast_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_dim_arg_reduction_scalar_cuda_bfloat16 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_int16 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_int8 (__main__.TestReductionsCUDA) ... ok
test_dim_arg_reduction_scalar_cuda_uint8 (__main__.TestReductionsCUDA) ... ok
test_dim_default_all_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_amax_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_amin_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_any_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_argmax_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_argmin_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_keepdim_all_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_default_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_default_keepdim_any_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_argmax_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_argmin_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_prod_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_nansum_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_prod_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_sum_cuda (__main__.TestReductionsCUDA)
Tests that the default dim reduces all dimensions. ... ok
test_dim_empty_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=[] is a no-op ... skipped 'Skipped!'
test_dim_empty_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=[] is a no-op ... skipped 'Skipped!'
test_dim_empty_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=[] is a no-op ... skipped 'Skipped!'
test_dim_empty_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=[], when keepdim=True, is a no-op ... skipped 'Skipped!'
test_dim_empty_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=[], when keepdim=True, is a no-op ... skipped 'Skipped!'
test_dim_empty_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=[], when keepdim=True, is a no-op ... skipped 'Skipped!'
test_dim_empty_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[], when keepdim=True, is a no-op ... skipped 'Skipped!'
test_dim_empty_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[], when keepdim=True, is a no-op ... skipped 'Skipped!'
test_dim_empty_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[] is a no-op ... skipped 'Skipped!'
test_dim_empty_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[] is a no-op ... skipped 'Skipped!'
test_dim_multi_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...] reduces dimensions i, j, .... ... ok
test_dim_multi_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...] reduces dimensions i, j, .... ... ok
test_dim_multi_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...] reduces dimensions i, j, .... ... ok
test_dim_multi_duplicate_amax_cuda (__main__.TestReductionsCUDA)
Tests that an error is raised if dim has duplicate entries. ... ok
test_dim_multi_duplicate_amin_cuda (__main__.TestReductionsCUDA)
Tests that an error is raised if dim has duplicate entries. ... ok
test_dim_multi_duplicate_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that an error is raised if dim has duplicate entries. ... ok
test_dim_multi_duplicate_nansum_cuda (__main__.TestReductionsCUDA)
Tests that an error is raised if dim has duplicate entries. ... ok
test_dim_multi_duplicate_sum_cuda (__main__.TestReductionsCUDA)
Tests that an error is raised if dim has duplicate entries. ... ok
test_dim_multi_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...], when keepdim=True, reduces dimensions i, j, .... to size 1. ... ok
test_dim_multi_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...], when keepdim=True, reduces dimensions i, j, .... to size 1. ... ok
test_dim_multi_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...], when keepdim=True, reduces dimensions i, j, .... to size 1. ... skipped 'Skipped!'
test_dim_multi_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...], when keepdim=True, reduces dimensions i, j, .... to size 1. ... ok
test_dim_multi_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...], when keepdim=True, reduces dimensions i, j, .... to size 1. ... ok
test_dim_multi_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...] reduces dimensions i, j, .... ... ok
test_dim_multi_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=[i, j, ...] reduces dimensions i, j, .... ... ok
test_dim_multi_unsorted_amax_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list. ... ok
test_dim_multi_unsorted_amin_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list. ... ok
test_dim_multi_unsorted_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list. ... ok
test_dim_multi_unsorted_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list when keepdim=True. ... ok
test_dim_multi_unsorted_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list when keepdim=True. ... ok
test_dim_multi_unsorted_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list when keepdim=True. ... skipped 'Skipped!'
test_dim_multi_unsorted_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list when keepdim=True. ... ok
test_dim_multi_unsorted_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list when keepdim=True. ... ok
test_dim_multi_unsorted_nansum_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list. ... ok
test_dim_multi_unsorted_sum_cuda (__main__.TestReductionsCUDA)
Tests that operator correctly handles unsorted dim list. ... ok
test_dim_multi_unsupported_all_cuda (__main__.TestReductionsCUDA)
Tests that ops claiming to not support multi dim actually don't. ... ok
test_dim_multi_unsupported_any_cuda (__main__.TestReductionsCUDA)
Tests that ops claiming to not support multi dim actually don't. ... ok
test_dim_multi_unsupported_argmax_cuda (__main__.TestReductionsCUDA)
Tests that ops claiming to not support multi dim actually don't. ... ok
test_dim_multi_unsupported_argmin_cuda (__main__.TestReductionsCUDA)
Tests that ops claiming to not support multi dim actually don't. ... ok
test_dim_multi_unsupported_prod_cuda (__main__.TestReductionsCUDA)
Tests that ops claiming to not support multi dim actually don't. ... ok
test_dim_ndim_limit_all_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_amax_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_amin_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_any_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_argmax_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_argmin_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_nansum_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_prod_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_sum_cuda (__main__.TestReductionsCUDA)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_none_all_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_any_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_argmax_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_argmin_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_keepdim_all_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_none_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_none_keepdim_any_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_argmax_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_argmin_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_prod_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_prod_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_offbounds_all_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_amax_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_amin_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_any_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_argmax_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_argmin_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_nansum_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_prod_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_sum_cuda (__main__.TestReductionsCUDA)
Tests that passing an off-bounds dim throws ... ok
test_dim_reduction_cuda_bfloat16 (__main__.TestReductionsCUDA) ... test_reductions.py:1753: UserWarning: An output with one or more elements was resized since it had shape [2, 3], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.sum(x, 0, out=y)
ok
test_dim_reduction_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_int16 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_int32 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_cuda_int8 (__main__.TestReductionsCUDA) ... ok
test_dim_reduction_less_than_64_cuda (__main__.TestReductionsCUDA) ... ok
test_dim_single_all_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_any_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_argmax_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_argmin_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_keepdim_all_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_amax_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_amin_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_any_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_argmax_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_argmin_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... skipped 'Skipped!'
test_dim_single_keepdim_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_prod_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_nansum_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_prod_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_sum_cuda (__main__.TestReductionsCUDA)
Tests that dim=i reduces dimension i. ... ok
test_empty_tensor_empty_slice_all_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_amax_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_amin_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_any_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_argmax_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_argmin_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_nansum_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_prod_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_sum_cuda (__main__.TestReductionsCUDA)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_nonempty_slice_all_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_amax_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_amin_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_any_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_argmax_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_argmin_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_count_nonzero_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_nansum_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_prod_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_sum_cuda (__main__.TestReductionsCUDA)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_histc_cuda (__main__.TestReductionsCUDA) ... ok
test_histogram_cuda_float32 (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_histogram_cuda_float64 (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_histogram_error_handling_cuda_float32 (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_histogram_error_handling_cuda_float64 (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_identity_all_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_complex128 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_complex64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_all_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_complex128 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_complex64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_any_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_complex128 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_complex64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_count_nonzero_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_nansum_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_complex128 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_complex64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_prod_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_bfloat16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_bool (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_complex128 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_complex64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_float16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_float32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_float64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_int16 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_int32 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_int64 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_int8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_identity_sum_cuda_uint8 (__main__.TestReductionsCUDA)
Tests that the identity value is an identity for the operator ... ok
test_logsumexp_cuda (__main__.TestReductionsCUDA) ... /opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
ok
test_logsumexp_dim_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_max_cuda_bool (__main__.TestReductionsCUDA) ... ok
test_max_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_max_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_max_cuda_int64 (__main__.TestReductionsCUDA) ... ok
test_max_elementwise_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_max_mixed_devices_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_max_with_inf_cuda_bfloat16 (__main__.TestReductionsCUDA) ... ok
test_max_with_inf_cuda_float16 (__main__.TestReductionsCUDA) ... ok
test_max_with_inf_cuda_float32 (__main__.TestReductionsCUDA) ... ok
test_max_with_inf_cuda_float64 (__main__.TestReductionsCUDA) ... ok
test_mean_dim_cuda (__main__.TestReductionsCUDA) ... skipped 'Only runs on cpu'
test_median_corner_cases_cuda (__main__.TestReductionsCUDA) ... ok
test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA) ... "Cannot find Symbol"
test_reductions failed! Received signal: SIGIOT
Running test_serialization ... [2021-10-12 10:47:37.617391]
Executing ['/opt/conda/bin/python3.6', 'test_serialization.py', '-v'] ... [2021-10-12 10:47:37.617454]
test_serialization_new_format_old_format_compat_cuda (__main__.TestBothSerializationCUDA) ... ok
test_load_error_msg (__main__.TestOldSerialization) ... ok
test_load_nonexistent_device (__main__.TestOldSerialization) ... skipped 'Testing torch.load on CPU-only machine'
test_load_python2_unicode_module (__main__.TestOldSerialization) ... ok
test_load_unicode_error_msg (__main__.TestOldSerialization) ... ok
test_serialization (__main__.TestOldSerialization) ... ok
test_serialization_backwards_compat (__main__.TestOldSerialization) ... ok
test_serialization_container (__main__.TestOldSerialization) ... ok
test_serialization_container_filelike (__main__.TestOldSerialization) ... ok
test_serialization_dill (__main__.TestOldSerialization) ... skipped '"dill" not found or not correct version'
test_serialization_dill_version_not_supported (__main__.TestOldSerialization) ... skipped '"dill" not found or is correct version'
test_serialization_fake_zip (__main__.TestOldSerialization) ... ok
test_serialization_filelike (__main__.TestOldSerialization) ... ok
test_serialization_filelike_api_requirements (__main__.TestOldSerialization) ... ok
test_serialization_filelike_missing_attrs (__main__.TestOldSerialization) ... ok
test_serialization_filelike_stress (__main__.TestOldSerialization) ... ok
test_serialization_filelike_uses_readinto (__main__.TestOldSerialization) ... ok
test_serialization_gzip (__main__.TestOldSerialization) ... ok
test_serialization_map_location (__main__.TestOldSerialization) ... ok
test_serialization_offset (__main__.TestOldSerialization) ... ok
test_serialization_offset_filelike (__main__.TestOldSerialization) ... ok
test_serialization_offset_gzip (__main__.TestOldSerialization) ... ok
test_serialization_save_warnings (__main__.TestOldSerialization) ... ok
test_serialization_sparse (__main__.TestOldSerialization) ... ok
test_serialization_sparse_invalid (__main__.TestOldSerialization) ... ok
test_serialization_storage_slice (__main__.TestOldSerialization) ... ok
test_serialization_zipfile_utils (__main__.TestOldSerialization) ... ok
test_serialize_device (__main__.TestOldSerialization) ... ok
test_load_error_msg (__main__.TestSerialization) ... ok
test_load_nonexistent_device (__main__.TestSerialization) ... skipped 'Testing torch.load on CPU-only machine'
test_load_python2_unicode_module (__main__.TestSerialization) ... ok
test_load_unicode_error_msg (__main__.TestSerialization) ... ok
test_meta_serialization (__main__.TestSerialization) ... ok
test_pathlike_serialization (__main__.TestSerialization) ... ok
test_serialization (__main__.TestSerialization) ... ok
test_serialization_2gb_file (__main__.TestSerialization) ... ok
test_serialization_backwards_compat (__main__.TestSerialization) ... ok
test_serialization_dill (__main__.TestSerialization) ... skipped '"dill" not found or not correct version'
test_serialization_dill_version_not_supported (__main__.TestSerialization) ... skipped '"dill" not found or is correct version'
test_serialization_fake_zip (__main__.TestSerialization) ... ok
test_serialization_filelike (__main__.TestSerialization) ... ok
test_serialization_filelike_api_requirements (__main__.TestSerialization) ... ok
test_serialization_filelike_missing_attrs (__main__.TestSerialization) ... ok
test_serialization_filelike_stress (__main__.TestSerialization) ... ok
test_serialization_filelike_uses_readinto (__main__.TestSerialization) ... ok
test_serialization_gzip (__main__.TestSerialization) ... ok
test_serialization_map_location (__main__.TestSerialization) ... ok
test_serialization_offset_gzip (__main__.TestSerialization) ... ok
test_serialization_save_warnings (__main__.TestSerialization) ... ok
test_serialization_sparse (__main__.TestSerialization) ... ok
test_serialization_sparse_invalid (__main__.TestSerialization) ... ok
test_serialization_storage_slice (__main__.TestSerialization) ... ok
test_serialization_zipfile (__main__.TestSerialization) ... ok
test_serialization_zipfile_actually_jit (__main__.TestSerialization) ... /opt/conda/lib/python3.6/site-packages/torch/serialization.py:604: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  " silence this warning)", UserWarning)
ok
test_serialization_zipfile_utils (__main__.TestSerialization) ... ok
test_serialize_device (__main__.TestSerialization) ... ok
test_tensor_subclass_getstate_overwrite (__main__.TestSerialization) ... ok
test_tensor_subclass_wrapper_serialization (__main__.TestSerialization) ... ok

----------------------------------------------------------------------
Ran 58 tests in 22.757s

OK (skipped=6)
Running test_set_default_mobile_cpu_allocator ... [2021-10-12 10:48:03.781016]
Executing ['/opt/conda/bin/python3.6', 'test_set_default_mobile_cpu_allocator.py', '-v'] ... [2021-10-12 10:48:03.781102]
test_exception (__main__.TestSetDefaultMobileCPUAllocator) ... ok
test_no_exception (__main__.TestSetDefaultMobileCPUAllocator) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.012s

OK
Running test_shape_ops ... [2021-10-12 10:48:06.108049]
Executing ['/opt/conda/bin/python3.6', 'test_shape_ops.py', '-v'] ... [2021-10-12 10:48:06.108117]
test_clamp_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_clamp_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_clamp_propagates_nans_cuda (__main__.TestShapeOpsCUDA) ... ok
test_clamp_raises_arg_errors_cuda (__main__.TestShapeOpsCUDA) ... ok
test_complex_rot90_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_complex_rot90_cuda_complex64 (__main__.TestShapeOpsCUDA) ... ok
test_diag_cuda_bool (__main__.TestShapeOpsCUDA) ... ok
test_diag_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_diagonal_cuda (__main__.TestShapeOpsCUDA) ... ok
test_diagonal_multidim_cuda_float32 (__main__.TestShapeOpsCUDA) ... skipped 'Only runs on cpu'
test_flip_cuda_bfloat16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_bool (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_complex64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_float16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_int16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_int32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_int8 (__main__.TestShapeOpsCUDA) ... ok
test_flip_cuda_uint8 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_bfloat16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_bool (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_complex64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_float16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_int16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_int32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_int8 (__main__.TestShapeOpsCUDA) ... ok
test_flip_errors_cuda_uint8 (__main__.TestShapeOpsCUDA) ... ok
test_flip_large_tensor_cuda (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_bfloat16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_bool (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_complex64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_float16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_int16 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_int32 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_int8 (__main__.TestShapeOpsCUDA) ... ok
test_flip_numpy_cuda_uint8 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_invalid_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_invalid_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_fliplr_invalid_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_invalid_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_invalid_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_flipud_invalid_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_invalid_cuda_complex128 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_invalid_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_movedim_invalid_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_astuple_out_cuda (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_bfloat16 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_bool (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_float16 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_int16 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_int32 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_int8 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_cuda_uint8 (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_discontiguous_cuda (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_no_warning_cuda (__main__.TestShapeOpsCUDA) ... ok
test_nonzero_non_diff_cuda (__main__.TestShapeOpsCUDA) ... ok
test_rot90_cuda (__main__.TestShapeOpsCUDA) ... ok
test_tolist_cuda (__main__.TestShapeOpsCUDA) ... skipped 'Only runs on cpu'
test_trace_cuda_float16 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_float32 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_float64 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_int16 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_int32 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_int64 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_int8 (__main__.TestShapeOpsCUDA) ... ok
test_trace_cuda_uint8 (__main__.TestShapeOpsCUDA) ... ok
test_unbind_cuda (__main__.TestShapeOpsCUDA) ... skipped 'Only runs on cpu'

----------------------------------------------------------------------
Ran 90 tests in 151.638s

OK (skipped=3)
Running test_show_pickle ... [2021-10-12 10:50:41.312637]
Executing ['/opt/conda/bin/python3.6', 'test_show_pickle.py', '-v'] ... [2021-10-12 10:50:41.312730]
test_scripted_model (__main__.TestShowPickle) ... ok

----------------------------------------------------------------------
Ran 1 test in 0.011s

OK
Running test_sort_and_select ... [2021-10-12 10:50:43.655977]
Executing ['/opt/conda/bin/python3.6', 'test_sort_and_select.py', '-v'] ... [2021-10-12 10:50:43.656054]
test_isin_cuda_float16 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_int64 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_int8 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_int64 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_int8 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_devices_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... ok
test_isin_different_dtypes_cuda (__main__.TestSortAndSelectCUDA) ... ok
test_kthvalue_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_kthvalue_overlap_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_kthvalue_scalar_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_bfloat16 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_float16 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_int64 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_int8 (__main__.TestSortAndSelectCUDA) ... ok
test_msort_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... ok
test_sort_1d_output_discontiguous_cuda_float32 (__main__.TestSortAndSelectCUDA) ... test_sort_and_select.py:210: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch.sort(tensor, out=(values, indices))
ok
test_sort_cuda (__main__.TestSortAndSelectCUDA) ... FAIL
test_sort_discontiguous_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_sort_discontiguous_slow_cuda_float32 (__main__.TestSortAndSelectCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_sort_large_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... skipped 'Insufficient cuda:0 memory'
test_stable_sort_against_numpy_cuda_bfloat16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_float16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_int64 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_int8 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_against_numpy_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_bfloat16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_float16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_float64 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_int64 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_int8 (__main__.TestSortAndSelectCUDA) ... ok
test_stable_sort_cuda_uint8 (__main__.TestSortAndSelectCUDA) ... ok
test_topk_1d_output_discontiguous_cuda_float32 (__main__.TestSortAndSelectCUDA) ... ok
test_topk_4d_cuda (__main__.TestSortAndSelectCUDA) ... ok
test_topk_arguments_cuda (__main__.TestSortAndSelectCUDA) ... ok
test_topk_bfloat16_cuda_bfloat16 (__main__.TestSortAndSelectCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_topk_cuda (__main__.TestSortAndSelectCUDA) ... ok
test_topk_integral_cuda_int16 (__main__.TestSortAndSelectCUDA) ... ok
test_topk_integral_cuda_int32 (__main__.TestSortAndSelectCUDA) ... ok
test_topk_integral_cuda_int64 (__main__.TestSortAndSelectCUDA) ... "Cannot find Symbol"
test_sort_and_select failed! Received signal: SIGIOT
Running test_sparse ... [2021-10-12 10:51:03.968965]
Executing ['/opt/conda/bin/python3.6', 'test_sparse.py', '-v'] ... [2021-10-12 10:51:03.969026]
test_Sparse_to_Sparse_copy__cuda_complex128 (__main__.TestSparseCUDA) ... test_sparse.py:683: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Copy.cpp:244.)
  x1 = x1.to(torch.float32)
/opt/conda/lib/python3.6/site-packages/torch/_tensor.py:1012: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /var/lib/jenkins/pytorch/build/aten/src/ATen/core/TensorBody.h:420.)
  return self._grad
ok
test_Sparse_to_Sparse_copy__cuda_float64 (__main__.TestSparseCUDA) ... ok
test_Sparse_to_Sparse_copy_multi_gpu_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_Sparse_to_Sparse_copy_multi_gpu_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_add_dense_sparse_mismatch_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_add_dense_sparse_mismatch_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_add_noncontiguous_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_add_noncontiguous_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_add_sub_nnz_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_add_sub_nnz_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_add_zeros_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_add_zeros_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_allow_tensor_metadata_change_cuda (__main__.TestSparseCUDA) ... ok
test_any_cuda (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_float32 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_int16 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_int32 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_int64 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_int8 (__main__.TestSparseCUDA) ... ok
test_asin_arcsin_cuda_uint8 (__main__.TestSparseCUDA) ... ok
test_assign_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_basic_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_basic_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_basic_ops_cuda_float64 (__main__.TestSparseCUDA) ... test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
test_sparse.py:1569: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expected = self.safeToDense(x1) // 37.5
ok
test_bmm_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'bmm sparse-dense requires CUDA 10.1 or greater'
test_bmm_cuda_version_error_cuda_float64 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_bmm_deterministic_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'bmm sparse-dense requires CUDA 10.1 or greater'
test_bmm_windows_error_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'this test ensures bmm sparse-dense CUDA gives an error when run on Windows with CUDA < 11.0'
test_cat_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_cat_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_change_tensor_metadata_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_change_tensor_metadata_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_clone_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_clone_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_coalesce_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_coalesce_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_coalesce_reference_cycle_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_coalesce_transpose_mm_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_constructor_device_legacy_cuda (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_contig_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_contig_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_contig_hybrid_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_contig_hybrid_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_cpu_sparse_dense_mul_cuda (__main__.TestSparseCUDA) ... ok
test_ctor_large_sizes_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_ctor_size_checks_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_ctor_size_checks_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_cuda_empty_cuda (__main__.TestSparseCUDA) ... ok
test_div_by_sparse_error_cuda (__main__.TestSparseCUDA) ... ok
test_div_rounding_mode_cuda_float32 (__main__.TestSparseCUDA) ... ok
test_div_rounding_mode_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_dsmm_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_dtypes_cuda (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_empty_full_cuda (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_empty_like_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_empty_like_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_factory_copy_cuda (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_cuda_complex128 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_cuda_complex64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_cuda_float16 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_cuda_float32 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_default_cuda (__main__.TestSparseCUDA) ... ok
test_factory_dense_dim_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_factory_dense_dim_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_factory_device_type_inference_cuda (__main__.TestSparseCUDA) ... ok
test_factory_empty_indices_cuda (__main__.TestSparseCUDA) ... ok
test_factory_nnz_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_factory_nnz_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_factory_nnz_zero_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_factory_nnz_zero_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_factory_size_check_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_factory_size_check_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_factory_type_inference_cuda_complex128 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_type_inference_cuda_complex64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_type_inference_cuda_float16 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_type_inference_cuda_float32 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_type_inference_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_factory_type_inference_cuda_int64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_floor_divide_by_sparse_error_cuda (__main__.TestSparseCUDA) ... test_sparse.py:2919: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  // torch.tensor(1., device=device).to_sparse())
ok
test_hsmm_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_index_select_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_index_select_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_is_nonzero_cuda (__main__.TestSparseCUDA) ... ok
test_is_sparse_cuda (__main__.TestSparseCUDA) ... ok
test_isnan_cuda (__main__.TestSparseCUDA) ... ok
test_legacy_constructor_cuda (__main__.TestSparseCUDA) ... ok
test_legacy_new_cuda (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_float32 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_int16 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_int32 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_int64 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_int8 (__main__.TestSparseCUDA) ... ok
test_log1p_cuda_uint8 (__main__.TestSparseCUDA) ... ok
test_mm_cuda_complex128 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_mm_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_mv_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_narrow_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_narrow_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_neg_negative_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_neg_negative_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_new_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_new_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_new_device_multi_gpu_cuda (__main__.TestSparseCUDA) ... ok
test_new_device_single_gpu_cuda (__main__.TestSparseCUDA) ... ok
test_norm_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_norm_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_pickle_cuda_float64 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_print_coalesced_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_print_uncoalesced_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_resize_as_cuda (__main__.TestSparseCUDA) ... ok
test_resize_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_resize_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_saddmm_cuda_complex128 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_saddmm_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_same_gpu_cuda (__main__.TestSparseCUDA) ... ok
test_scalar_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_scalar_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_select_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_select_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_shared_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_shared_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_softmax_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_spadd_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_add_coalesce_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_add_coalesce_cuda_complex64 (__main__.TestSparseCUDA) ... ok
test_sparse_add_coalesce_cuda_float32 (__main__.TestSparseCUDA) ... ok
test_sparse_add_coalesce_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_add_out_bfloat16_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_add_out_bfloat16_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_addmm_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_addmm_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_bool_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_bool_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_mask_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_mask_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_mask_hybrid_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_sparse_mask_hybrid_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_matmul_cuda_float64 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_sparse_mm_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_sum_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_sparse_to_numpy_cuda (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_sspaddmm_cuda_complex128 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_sspaddmm_cuda_float64 (__main__.TestSparseCUDA) ... skipped 'Only runs on cpu'
test_storage_not_null_cuda (__main__.TestSparseCUDA) ... ok
test_t_empty_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_t_empty_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_to_dense_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_to_dense_cuda_complex64 (__main__.TestSparseCUDA) ... ok
test_to_dense_cuda_float16 (__main__.TestSparseCUDA) ... ok
test_to_dense_cuda_float32 (__main__.TestSparseCUDA) ... ok
test_to_dense_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_to_dense_hybrid_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_to_dense_hybrid_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_to_sparse_cuda_complex128 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_to_sparse_cuda_complex64 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_to_sparse_cuda_float16 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_to_sparse_cuda_float64 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_to_sparse_cuda_int32 (__main__.TestSparseCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_transpose_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_transpose_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_unsqueeze_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_unsqueeze_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_zeros_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_zeros_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_zeros_like_cuda_complex128 (__main__.TestSparseCUDA) ... ok
test_zeros_like_cuda_float64 (__main__.TestSparseCUDA) ... ok
test_cuda_from_cpu (__main__.TestSparseOneOff) ... ok
test_cuda_sparse_cpu_dense_add (__main__.TestSparseOneOff) ... ok
test_sparse_consistency_asin_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_asin_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_asin_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_asin_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_asin_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_asin_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_conj_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_conj_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_conj_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_conj_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_conj_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_conj_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_sqrt_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_sqrt_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_sqrt_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_consistency_sqrt_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_sqrt_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_consistency_sqrt_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_asin_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_asin_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_asin_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_asin_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_asin_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_asin_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_conj_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_conj_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_conj_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_conj_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_conj_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_conj_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_sqrt_cuda_bfloat16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_sqrt_cuda_complex64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_sqrt_cuda_float16 (__main__.TestSparseUnaryUfuncsCUDA) ... skipped 'Skipped! Unsupported dtypes for Sparse'
test_sparse_zero_dims_sqrt_cuda_float32 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_sqrt_cuda_int64 (__main__.TestSparseUnaryUfuncsCUDA) ... ok
test_sparse_zero_dims_sqrt_cuda_uint8 (__main__.TestSparseUnaryUfuncsCUDA) ... ok

----------------------------------------------------------------------
Ran 207 tests in 61.082s

OK (skipped=46)
Running test_sparse_csr ... [2021-10-12 10:52:10.069060]
Executing ['/opt/conda/bin/python3.6', 'test_sparse_csr.py', '-v'] ... [2021-10-12 10:52:10.069141]
test_add_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_add_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_coo_csr_conversion_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_coo_to_csr_convert_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_csr_layout_cuda (__main__.TestSparseCSRCUDA) ... skipped 'Only runs on cpu'
test_csr_matvec_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_csr_matvec_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_factory_device_type_inference_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_factory_indices_invariants_check_cuda (__main__.TestSparseCSRCUDA) ... ok
test_factory_layout_invariants_check_cuda (__main__.TestSparseCSRCUDA) ... ok
test_factory_shape_invariants_check_cuda (__main__.TestSparseCSRCUDA) ... ok
test_factory_type_invariants_check_cuda (__main__.TestSparseCSRCUDA) ... ok
test_matmul_device_mismatch_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_mkl_matvec_warnings_cuda_float32 (__main__.TestSparseCSRCUDA) ... skipped 'Only runs on cpu'
test_mkl_matvec_warnings_cuda_float64 (__main__.TestSparseCSRCUDA) ... skipped 'Only runs on cpu'
test_mm_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_addmm_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_addmm_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_from_lists_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_constructor_shape_inference_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_convert_error_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_from_dense_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_print_cuda (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_bfloat16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_bool (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_complex128 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_complex64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_float16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_int16 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_int32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_int64 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_int8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_csr_to_dense_cuda_uint8 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_mm_cuda_float32 (__main__.TestSparseCSRCUDA) ... ok
test_sparse_mm_cuda_float64 (__main__.TestSparseCSRCUDA) ... ok
test_make_crow_indices (__main__.TestSparseCSRSampler) ... ok

----------------------------------------------------------------------
Ran 116 tests in 34.725s

OK (skipped=3)
Running test_spectral_ops ... [2021-10-12 10:52:49.515982]
Executing ['/opt/conda/bin/python3.6', 'test_spectral_ops.py', '-v'] ... [2021-10-12 10:52:49.516064]
test_batch_istft_cuda (__main__.TestFFTCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/functional.py:629: UserWarning: istft will require a complex-valued input tensor in a future PyTorch release. Matching the output from stft with return_complex=True.  (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/SpectralOps.cpp:811.)
  normalized, onesided, length, return_complex)
ok
test_complex_istft_real_equiv_cuda_complex128 (__main__.TestFFTCUDA) ... ok
test_complex_stft_definition_cuda_complex128 (__main__.TestFFTCUDA) ... ok
test_complex_stft_onesided_cuda (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_complex_stft_real_equiv_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_complex_stft_roundtrip_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_complex_stft_roundtrip_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_cufft_plan_cache_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_fftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_hfft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_hfft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_hfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_hfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ifftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ihfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_ihfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_irfftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_rfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_rfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_rfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_empty_fft_fft_rfftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft2_fftn_equivalence_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft2_fftn_equivalence_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft2_invalid_cuda (__main__.TestFFTCUDA) ... ok
test_fft2_numpy_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft2_numpy_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_fft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_fft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_fftn_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_fftn_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_hfft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_hfft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ifft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ifft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ifftn_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ifftn_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ihfft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_ihfft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_irfft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_irfft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_irfftn_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_irfftn_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_rfft_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_rfft_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_rfftn_cuda_bfloat16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_half_and_bfloat16_errors_fft_rfftn_cuda_float16 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_ifft_rfft_irfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_input_modification_cuda (__main__.TestFFTCUDA) ... ok
test_fft_invalid_dtypes_cuda (__main__.TestFFTCUDA) ... ok
test_fft_plan_repeatable_cuda (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_round_trip_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_round_trip_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_round_trip_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_round_trip_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_type_promotion_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_type_promotion_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_type_promotion_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_type_promotion_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fft_type_promotion_cuda_int8 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftfreq_numpy_cuda_float32 (__main__.TestFFTCUDA) ... ok
test_fftfreq_numpy_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_fftfreq_out_cuda_float32 (__main__.TestFFTCUDA) ... ok
test_fftfreq_out_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_fftn_invalid_fft_fftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_fftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_ifftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_ifftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_irfftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_irfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_invalid_fft_rfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_round_trip_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_round_trip_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_round_trip_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftn_round_trip_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_fftshift_frequencies_cuda_float32 (__main__.TestFFTCUDA) ... ok
test_fftshift_frequencies_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_fftshift_numpy_cuda_complex128 (__main__.TestFFTCUDA) ... ok
test_fftshift_numpy_cuda_complex64 (__main__.TestFFTCUDA) ... ok
test_fftshift_numpy_cuda_float32 (__main__.TestFFTCUDA) ... ok
test_fftshift_numpy_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_istft_linearity_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_istft_of_sine_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_istft_round_trip_simple_cases_cuda_float64 (__main__.TestFFTCUDA)
stft -> istft should recover the original signale ... skipped "test doesn't currently work on the ROCm stack"
test_istft_round_trip_various_params_cuda_float64 (__main__.TestFFTCUDA)
stft -> istft should recover the original signale ... skipped "test doesn't currently work on the ROCm stack"
test_istft_round_trip_with_padding_cuda_float64 (__main__.TestFFTCUDA)
long hop_length or not centered may cause length mismatch in the inversed signal ... skipped "test doesn't currently work on the ROCm stack"
test_istft_throws_cuda (__main__.TestFFTCUDA)
istft should throw exception for invalid parameters ... ok
test_reference_1d_fft_fft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_fft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_fft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_fft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_hfft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_hfft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_hfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_hfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ifft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ifft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ifft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ifft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ihfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_ihfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_irfft_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_irfft_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_irfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_irfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_rfft_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_1d_fft_rfft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_fftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_fftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_fftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_fftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_ifftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_ifftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_ifftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_ifftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_irfftn_cuda_complex128 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_irfftn_cuda_complex64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_irfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_irfftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_rfftn_cuda_float32 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_nd_fft_rfftn_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_stft_cuda_float64 (__main__.TestFFTCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_stft_requires_complex_cuda (__main__.TestFFTCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/functional.py:554: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/SpectralOps.cpp:659.)
  normalized, onesided, return_complex)
ok
test_stft_roundtrip_complex_window_cuda_complex128 (__main__.TestFFTCUDA) ... ok
test_stft_roundtrip_complex_window_cuda_float64 (__main__.TestFFTCUDA) ... ok
test_stft_window_device_cuda (__main__.TestFFTCUDA) ... ok

----------------------------------------------------------------------
Ran 146 tests in 17.322s

OK (skipped=123)
Running test_tensor_creation_ops ... [2021-10-12 10:53:11.077458]
Executing ['/opt/conda/bin/python3.6', 'test_tensor_creation_ops.py', '-v'] ... [2021-10-12 10:53:11.077536]
test_empty_like_cuda (__main__.TestLikeTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_full_like_inference_cuda (__main__.TestLikeTensorCreationCUDA) ... ok
test_ones_like_cuda (__main__.TestLikeTensorCreationCUDA) ... ok
test_ones_like_multiple_device_cuda (__main__.TestLikeTensorCreationCUDA) ... ok
test_zeros_like_cuda (__main__.TestLikeTensorCreationCUDA) ... ok
test_zeros_like_multiple_device_cuda (__main__.TestLikeTensorCreationCUDA) ... ok
test_normal_cuda_float32 (__main__.TestRandomTensorCreationCUDA) ... ok
test_normal_cuda_float64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_normal_std_error_cuda (__main__.TestRandomTensorCreationCUDA) ... ok
test_rand_cuda_complex128 (__main__.TestRandomTensorCreationCUDA) ... ok
test_rand_cuda_complex64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_rand_cuda_float32 (__main__.TestRandomTensorCreationCUDA) ... ok
test_rand_cuda_float64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randint_cuda (__main__.TestRandomTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_randint_inference_cuda (__main__.TestRandomTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_randn_cuda_bfloat16 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randn_cuda_complex128 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randn_cuda_complex64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randn_cuda_float16 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randn_cuda_float32 (__main__.TestRandomTensorCreationCUDA) ... ok
test_randn_cuda_float64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_random_neg_values_cuda (__main__.TestRandomTensorCreationCUDA) ... ok
test_randperm_cuda (__main__.TestRandomTensorCreationCUDA) ... ok
test_randperm_device_compatibility_cuda (__main__.TestRandomTensorCreationCUDA) ... ok
test_uniform_from_to_cuda_bfloat16 (__main__.TestRandomTensorCreationCUDA) ... ok
test_uniform_from_to_cuda_float16 (__main__.TestRandomTensorCreationCUDA) ... ok
test_uniform_from_to_cuda_float32 (__main__.TestRandomTensorCreationCUDA) ... ok
test_uniform_from_to_cuda_float64 (__main__.TestRandomTensorCreationCUDA) ... ok
test_arange_bfloat16_cuda (__main__.TestTensorCreationCUDA) ... ok
test_arange_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_arange_device_vs_cpu_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_arange_device_vs_cpu_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_arange_device_vs_cpu_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_arange_device_vs_cpu_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_arange_inference_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_as_strided_neg_cuda (__main__.TestTensorCreationCUDA) ... ok
test_as_tensor_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_block_diag_cuda (__main__.TestTensorCreationCUDA) ... ok
test_block_diag_scipy_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cartesian_prod_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat2_cuda_float16 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_cat2_cuda_float64 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_cat2_cuda_int32 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_cat_all_dtypes_and_devices_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_bad_input_sizes_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_cat_big_cuda (__main__.TestTensorCreationCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cat_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_different_devices_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_empty_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_empty_legacy_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_in_channels_last_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_mem_overlap_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_out_channels_last_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_out_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_out_memory_format_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_preserve_channels_last_cuda (__main__.TestTensorCreationCUDA) ... ok
test_cat_scalars_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_cat_stack_cross_devices_cuda (__main__.TestTensorCreationCUDA) ... ok
test_combinations_cuda (__main__.TestTensorCreationCUDA) ... ok
test_complex_type_conversions_cuda (__main__.TestTensorCreationCUDA) ... skipped 'real and imag not implemented for complex'
test_constructor_device_legacy_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_constructor_dtypes_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_ctor_with_numpy_array_cuda (__main__.TestTensorCreationCUDA) ... ok
test_device_rounding_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_device_rounding_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_device_rounding_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_diag_embed_cuda_float32 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_diagflat_cuda (__main__.TestTensorCreationCUDA) ... ok
test_dsplit_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_dsplit_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_dsplit_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_dstack_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_empty_full_cuda (__main__.TestTensorCreationCUDA) ... ok
test_empty_strided_cuda (__main__.TestTensorCreationCUDA) ... ok
test_empty_tensor_props_cuda (__main__.TestTensorCreationCUDA) ... ok
test_eye_cuda (__main__.TestTensorCreationCUDA) ... ok
test_fill_all_dtypes_and_devices_cuda (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_bool (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_finite_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_float_to_int_conversion_nonfinite_cuda_bool (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_float_to_int_conversion_nonfinite_cuda_int16 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_float_to_int_conversion_nonfinite_cuda_int32 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_float_to_int_conversion_nonfinite_cuda_int64 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_float_to_int_conversion_nonfinite_cuda_int8 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_float_to_int_conversion_nonfinite_cuda_uint8 (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_full_inference_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_full_inference_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_full_inference_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_full_out_cuda (__main__.TestTensorCreationCUDA) ... ok
test_hsplit_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_hsplit_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_hsplit_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_hstack_column_stack_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_large_linspace_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_large_linspace_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_like_fn_stride_proparation_vs_tensoriterator_unary_op_cuda (__main__.TestTensorCreationCUDA) ... ok
test_linlogspace_mem_overlap_cuda (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_deduction_cuda (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_device_vs_cpu_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_special_steps_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_steps_warning_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_vs_numpy_complex_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_vs_numpy_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_vs_numpy_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_vs_numpy_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_linspace_vs_numpy_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_base2_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_base2_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_base2_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_deduction_cuda (__main__.TestTensorCreationCUDA) ... ok
test_logspace_device_vs_cpu_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_device_vs_cpu_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_device_vs_cpu_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_special_steps_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_special_steps_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_special_steps_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_steps_warning_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_vs_numpy_complex_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_vs_numpy_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_logspace_vs_numpy_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_meshgrid_cuda (__main__.TestTensorCreationCUDA) ... ok
test_new_empty_strided_cuda (__main__.TestTensorCreationCUDA) ... ok
test_new_methods_requires_grad_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_new_tensor_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_offset_scalar_cast_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_ones_cuda (__main__.TestTensorCreationCUDA) ... ok
test_random_bool_cuda (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_random_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_random_default_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_bool_cuda (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_random_from_to_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_random_full_range_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_random_to_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... test_tensor_creation_ops.py:1744: UserWarning: to - 1 is out of bounds [-(2^8), 2^8]. Due to precision limitations c10::BFloat16 can support discrete uniform distribution only within this range. This warning will become an error in version 1.7 release, please fix the code in advance (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/DistributionTemplates.h:93.)
  t.random_(to_)
ok
test_random_to_cuda_float16 (__main__.TestTensorCreationCUDA) ... test_tensor_creation_ops.py:1744: UserWarning: to - 1 is out of bounds [-(2^11), 2^11]. Due to precision limitations c10::Half can support discrete uniform distribution only within this range. This warning will become an error in version 1.7 release, please fix the code in advance (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/DistributionTemplates.h:93.)
  t.random_(to_)
ok
test_random_to_cuda_float32 (__main__.TestTensorCreationCUDA) ... test_tensor_creation_ops.py:1744: UserWarning: to - 1 is out of bounds [-(2^24), 2^24]. Due to precision limitations float can support discrete uniform distribution only within this range. This warning will become an error in version 1.7 release, please fix the code in advance (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/DistributionTemplates.h:93.)
  t.random_(to_)
ok
test_random_to_cuda_float64 (__main__.TestTensorCreationCUDA) ... test_tensor_creation_ops.py:1744: UserWarning: to - 1 is out of bounds [-(2^53), 2^53]. Due to precision limitations double can support discrete uniform distribution only within this range. This warning will become an error in version 1.7 release, please fix the code in advance (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/DistributionTemplates.h:93.)
  t.random_(to_)
ok
test_random_to_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_random_to_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_random_to_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_random_to_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_random_to_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_range_cuda (__main__.TestTensorCreationCUDA) ... ok
test_range_factories_64bit_indexing_cuda (__main__.TestTensorCreationCUDA) ... ok
test_range_warning_cuda (__main__.TestTensorCreationCUDA) ... ok
test_repeat_interleave_cuda (__main__.TestTensorCreationCUDA) ... ok
test_roll_cuda (__main__.TestTensorCreationCUDA) ... ok
test_signal_window_functions_cuda_bfloat16 (__main__.TestTensorCreationCUDA) ... /opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
ok
test_signal_window_functions_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_signal_window_functions_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_signal_window_functions_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_signal_window_functions_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_simple_scalar_cast_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_stack_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_stack_out_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_strided_mismatched_stride_shape_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_ctor_device_inference_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_device_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_factories_empty_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_factory_copy_var_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_tensor_factory_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_tensor_factory_gpu_type_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_factory_gpu_type_inference_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_factory_type_inference_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_tensor_from_non_writable_numpy_cuda (__main__.TestTensorCreationCUDA) ... ok
test_tensor_from_sequence_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_torch_complex_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_bool (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_floating_dtype_error_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_out_dtype_error_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_out_dtype_error_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_same_dtype_error_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_torch_complex_same_dtype_error_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_torch_polar_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_torch_polar_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_trilu_indices_cuda (__main__.TestTensorCreationCUDA) ... skipped 'Only runs on cpu'
test_triu_tril_cuda (__main__.TestTensorCreationCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_unpack_double_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_unpack_double_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_vander_cuda (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_bool (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_vander_types_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_vsplit_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_vsplit_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_vsplit_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_complex128 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_complex64 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_float16 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_float32 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_float64 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_int16 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_int32 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_int64 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_int8 (__main__.TestTensorCreationCUDA) ... ok
test_vstack_row_stack_cuda_uint8 (__main__.TestTensorCreationCUDA) ... ok
test_zeros_cuda (__main__.TestTensorCreationCUDA) ... ok
test_zeros_dtype_out_match_cuda (__main__.TestTensorCreationCUDA) ... ok
test_zeros_out_cuda (__main__.TestTensorCreationCUDA) ... ok

----------------------------------------------------------------------
Ran 293 tests in 449.327s

OK (skipped=34)
Running test_tensorboard ... [2021-10-12 11:00:43.993268]
Executing ['/opt/conda/bin/python3.6', 'test_tensorboard.py', '-v'] ... [2021-10-12 11:00:43.993351]
test_embedding (__main__.TestTensorBoardEmbedding) ... ok
test_embedding_64 (__main__.TestTensorBoardEmbedding) ... ok
test_figure (__main__.TestTensorBoardFigure) ... ok
test_figure_list (__main__.TestTensorBoardFigure) ... ok
test_caffe2_np (__main__.TestTensorBoardNumpy) ... ok
test_caffe2_np_expect_fail (__main__.TestTensorBoardNumpy) ... ok
test_caffe2_simple_cnnmodel (__main__.TestTensorBoardNumpy) ... WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.
/opt/conda/lib/python3.6/site-packages/caffe2/python/brew.py:104: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()
  var_names, _, varkw, _= inspect.getargspec(func)
ok
test_caffe2_simple_model (__main__.TestTensorBoardNumpy) ... ok
test_pytorch_np_expect_fail (__main__.TestTensorBoardNumpy) ... ok
test_scalar (__main__.TestTensorBoardNumpy) ... ok
test_pytorch_autograd_np (__main__.TestTensorBoardPyTorchNumpy) ... ok
test_pytorch_histogram (__main__.TestTensorBoardPyTorchNumpy) ... ok
test_pytorch_histogram_raw (__main__.TestTensorBoardPyTorchNumpy) ... ok
test_pytorch_np (__main__.TestTensorBoardPyTorchNumpy) ... ok
test_pytorch_write (__main__.TestTensorBoardPyTorchNumpy) ... ok
test_mlp_graph (__main__.TestTensorBoardPytorchGraph) ... ok
test_pytorch_graph (__main__.TestTensorBoardPytorchGraph) ... test_tensorboard.py:561: DeprecationWarning: Please use assertEqual instead.
  self.assertEquals(len(expected_proto.node), len(actual_proto.node))
ok
test_pytorch_graph_dict_input (__main__.TestTensorBoardPytorchGraph) ... ok
test_torchvision_smoke (__main__.TestTensorBoardPytorchGraph) ... ok
test_wrong_input_size (__main__.TestTensorBoardPytorchGraph) ... ok
test_audio (__main__.TestTensorBoardSummary) ... ok
test_custom_scalars (__main__.TestTensorBoardSummary) ... ok
test_empty_input (__main__.TestTensorBoardSummary) ... ok
test_float32_image (__main__.TestTensorBoardSummary) ... ok
test_histogram_auto (__main__.TestTensorBoardSummary) ... ok
test_histogram_doane (__main__.TestTensorBoardSummary) ... ok
test_histogram_fd (__main__.TestTensorBoardSummary) ... ok
test_hparams_bool (__main__.TestTensorBoardSummary) ... ok
test_hparams_domain_discrete (__main__.TestTensorBoardSummary) ... ok
test_hparams_number (__main__.TestTensorBoardSummary) ... ok
test_hparams_smoke (__main__.TestTensorBoardSummary) ... ok
test_hparams_string (__main__.TestTensorBoardSummary) ... ok
test_hparams_wrong_parameter (__main__.TestTensorBoardSummary) ... WARNING:root:parameter: hparam_dict should be a dictionary, nothing logged.
WARNING:root:parameter: metric_dict should be a dictionary, nothing logged.
ok
test_image_with_3_channel_batched (__main__.TestTensorBoardSummary) ... ok
test_image_with_boxes (__main__.TestTensorBoardSummary) ... ok
test_image_with_one_channel (__main__.TestTensorBoardSummary) ... ok
test_image_with_one_channel_batched (__main__.TestTensorBoardSummary) ... ok
test_image_without_channel (__main__.TestTensorBoardSummary) ... ok
test_list_input (__main__.TestTensorBoardSummary) ... ok
test_mesh (__main__.TestTensorBoardSummary) ... ok
test_scalar_new_style (__main__.TestTensorBoardSummary) ... ok
test_text (__main__.TestTensorBoardSummary) ... ok
test_uint8_image (__main__.TestTensorBoardSummary) ... ok
test_video (__main__.TestTensorBoardSummary) ... ok
test_pathlib (__main__.TestTensorBoardSummaryWriter) ... ok
test_summary_writer_close (__main__.TestTensorBoardSummaryWriter) ... ok
test_summary_writer_ctx (__main__.TestTensorBoardSummaryWriter) ... ok
test_convert_to_HWC_dtype_remains_same (__main__.TestTensorBoardUtils) ... ok
test_numpy_vid_uint8 (__main__.TestTensorBoardUtils) ... ok
test_prepare_video (__main__.TestTensorBoardUtils) ... ok
test_to_HWC (__main__.TestTensorBoardUtils) ... ok
test_writer (__main__.TestTensorBoardWriter) ... ok

----------------------------------------------------------------------
Ran 52 tests in 24.148s

OK
warning: Embedding dir exists, did you set global_step for add_embedding()?
warning: Embedding dir exists, did you set global_step for add_embedding()?
Encountering a dict at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.
Error occurs, No graph saved
mat1 and mat2 shapes cannot be multiplied (1x9 and 3x5)
Error occurs, No graph saved
warning: audio amplitude out of range, auto clipped.
add_video needs package moviepy
Running test_tensorexpr ... [2021-10-12 11:01:11.817630]
Executing ['/opt/conda/bin/python3.6', 'test_tensorexpr.py', '-v'] ... [2021-10-12 11:01:11.817719]
test_add_const_rhs (__main__.TestTensorExprFuser) ... ok
test_add_sub (__main__.TestTensorExprFuser) ... ok
test_alias_analysis_input_and_module (__main__.TestTensorExprFuser) ... ok
test_alias_analysis_inputs (__main__.TestTensorExprFuser) ... ok
test_alias_analysis_module (__main__.TestTensorExprFuser) ... ok
test_all_combos (__main__.TestTensorExprFuser) ... ok
test_alpha (__main__.TestTensorExprFuser) ... ok
test_binary_ops (__main__.TestTensorExprFuser) ... ok
test_bitwise_ops (__main__.TestTensorExprFuser) ... ok
test_broadcast (__main__.TestTensorExprFuser) ... ok
test_broadcast3 (__main__.TestTensorExprFuser) ... ok
test_broadcast_2 (__main__.TestTensorExprFuser) ... ok
test_broadcast_big2 (__main__.TestTensorExprFuser) ... ok
test_cat (__main__.TestTensorExprFuser) ... ok
test_cat_empty_tensors (__main__.TestTensorExprFuser) ... ok
test_cat_negative_dim (__main__.TestTensorExprFuser) ... ok
test_cat_only (__main__.TestTensorExprFuser) ... skipped 'cat is broken with fusion group inlining disabled'
test_cat_promote_inputs (__main__.TestTensorExprFuser) ... ok
test_cat_with_constant_dim (__main__.TestTensorExprFuser) ... ok
test_char (__main__.TestTensorExprFuser) ... ok
test_chunk (__main__.TestTensorExprFuser) ... ok
test_clamp (__main__.TestTensorExprFuser) ... ok
test_constant (__main__.TestTensorExprFuser) ... ok
test_double (__main__.TestTensorExprFuser) ... ok
test_double_intrinsics (__main__.TestTensorExprFuser) ... ok
test_dynamic_shape (__main__.TestTensorExprFuser) ... skipped 'dynamic shapes are not quite there yet'
test_easy (__main__.TestTensorExprFuser) ... ok
test_eq (__main__.TestTensorExprFuser) ... ok
test_exp_pow (__main__.TestTensorExprFuser) ... ok
test_four_arg (__main__.TestTensorExprFuser) ... ok
test_ge (__main__.TestTensorExprFuser) ... ok
test_gt (__main__.TestTensorExprFuser) ... ok
test_guard_fails (__main__.TestTensorExprFuser) ... ok
test_half_bn_relu (__main__.TestTensorExprFuser) ... ok
test_half_gelu (__main__.TestTensorExprFuser) ... skipped 'float16 is not supported yet.'
test_int64_promotion (__main__.TestTensorExprFuser) ... ok
test_int_output (__main__.TestTensorExprFuser) ... ok
test_le (__main__.TestTensorExprFuser) ... ok
test_loop (__main__.TestTensorExprFuser) ... ok
test_lt (__main__.TestTensorExprFuser) ... ok
test_mask (__main__.TestTensorExprFuser) ... ok
test_min_max (__main__.TestTensorExprFuser) ... ok
test_min_max_reduction (__main__.TestTensorExprFuser) ... ok
test_min_max_reduction2 (__main__.TestTensorExprFuser) ... ok
test_min_max_reduction_dim1 (__main__.TestTensorExprFuser) ... ok
test_min_max_reduction_dim1_2 (__main__.TestTensorExprFuser) ... ok
test_multi_rand (__main__.TestTensorExprFuser) ... ok
test_multioutput (__main__.TestTensorExprFuser) ... ok
test_multiple_outputs (__main__.TestTensorExprFuser) ... ok
test_nans (__main__.TestTensorExprFuser) ... ok
test_ne (__main__.TestTensorExprFuser) ... ok
test_promotion (__main__.TestTensorExprFuser) ... ok
test_rand_like (__main__.TestTensorExprFuser) ... ok
test_rank_two (__main__.TestTensorExprFuser) ... ok
test_relu (__main__.TestTensorExprFuser) ... ok
test_remainder (__main__.TestTensorExprFuser) ... ok
test_reps (__main__.TestTensorExprFuser) ... ok
test_scalar (__main__.TestTensorExprFuser) ... ok
test_short (__main__.TestTensorExprFuser) ... ok
test_simple_add (__main__.TestTensorExprFuser) ... ok
test_slice (__main__.TestTensorExprFuser) ... ok
test_sliced_stride (__main__.TestTensorExprFuser) ... ok
test_softmax_cpu (__main__.TestTensorExprFuser) ... ok
test_softmax_cuda (__main__.TestTensorExprFuser) ... skipped 'global allocs are not supported yet.'
test_strided_output_preserved (__main__.TestTensorExprFuser) ... ok
test_three_arg (__main__.TestTensorExprFuser) ... ok
test_three_arg2 (__main__.TestTensorExprFuser) ... ok
test_transpose (__main__.TestTensorExprFuser) ... ok
test_unary_ops (__main__.TestTensorExprFuser) ... ok
test_unsqueeze (__main__.TestTensorExprFuser) ... ok
test_where (__main__.TestTensorExprFuser) ... ok

----------------------------------------------------------------------
Ran 71 tests in 22.758s

OK (skipped=4)
graph(%a : Half(16, 16, strides=[16, 1], requires_grad=0, device=cuda:0),
      %b : Half(16, strides=[1], requires_grad=0, device=cuda:0),
      %c : Half(16, strides=[1], requires_grad=0, device=cuda:0)):
  %3 : NoneType = prim::Constant()
  %4 : NoneType = prim::Constant()
  %5 : bool = prim::Constant[value=0]() # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2283:0
  %6 : float = prim::Constant[value=0.10000000000000001]() # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2283:0
  %7 : float = prim::Constant[value=1.0000000000000001e-05]() # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2283:0
  %8 : bool = prim::Constant[value=1]() # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2283:0
  %y : Half(16, 16, strides=[16, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%a, %3, %4, %b, %c, %5, %6, %7, %8) # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2283:0
  %10 : Half(16, 16, strides=[16, 1], requires_grad=0, device=cuda:0) = aten::relu(%y) # test_tensorexpr.py:1246:0
  return (%10)

Running test_tensorexpr_pybind ... [2021-10-12 11:01:38.604583]
Executing ['/opt/conda/bin/python3.6', 'test_tensorexpr_pybind.py', '-v'] ... [2021-10-12 11:01:38.604668]
test_alloc_in_loop (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_call_raw (__main__.TestTensorExprPyBind) ... ok
test_dtype_error (__main__.TestTensorExprPyBind) ... ok
test_dynamic_shape (__main__.TestTensorExprPyBind) ... ok
test_external_calls (__main__.TestTensorExprPyBind) ... ok
test_kernel_shape_prop (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_shape_prop_module (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_custom_lowering (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_expand (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_permute (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_scalar_inputs (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_t (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_tensor_inputs (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_kernel_with_transpose (__main__.TestTensorExprPyBind) ... skipped 'LLVM backend not enabled'
test_simple_sum (__main__.TestTensorExprPyBind) ... ok

----------------------------------------------------------------------
Ran 15 tests in 0.018s

OK (skipped=10)
Running test_testing ... [2021-10-12 11:01:41.968997]
Executing ['/opt/conda/bin/python3.6', 'test_testing.py', '-v'] ... [2021-10-12 11:01:41.969069]
test_bool (__main__.TestAssertClose) ... ok
test_default_tolerance_selection_mismatching_dtypes (__main__.TestAssertClose) ... ok
test_docstring_examples (__main__.TestAssertClose) ... ok
test_matching (__main__.TestAssertClose) ... ok
test_matching_atol (__main__.TestAssertClose) ... ok
test_matching_conjugate_bit (__main__.TestAssertClose) ... ok
test_matching_nan (__main__.TestAssertClose) ... ok
test_matching_nan_with_equal_nan (__main__.TestAssertClose) ... ok
test_matching_rtol (__main__.TestAssertClose) ... ok
test_mismatching_dtype (__main__.TestAssertClose) ... ok
test_mismatching_dtype_no_check (__main__.TestAssertClose) ... ok
test_mismatching_layout (__main__.TestAssertClose) ... ok
test_mismatching_shape (__main__.TestAssertClose) ... ok
test_mismatching_stride (__main__.TestAssertClose) ... ok
test_mismatching_stride_no_check (__main__.TestAssertClose) ... ok
test_mismatching_types (__main__.TestAssertClose) ... ok
test_mismatching_types_subclasses (__main__.TestAssertClose) ... ok
test_mismatching_types_type_equality (__main__.TestAssertClose) ... ok
test_mismatching_values (__main__.TestAssertClose) ... ok
test_mismatching_values_atol (__main__.TestAssertClose) ... ok
test_mismatching_values_rtol (__main__.TestAssertClose) ... ok
test_numpy (__main__.TestAssertClose) ... ok
test_only_atol (__main__.TestAssertClose) ... ok
test_only_rtol (__main__.TestAssertClose) ... ok
test_scalar (__main__.TestAssertClose) ... ok
test_unknown_layout (__main__.TestAssertClose) ... ok
test_unknown_type (__main__.TestAssertClose) ... ok
test_mapping_mismatching_keys (__main__.TestAssertCloseContainer) ... ok
test_mapping_mismatching_values_msg (__main__.TestAssertCloseContainer) ... ok
test_sequence_mismatching_len (__main__.TestAssertCloseContainer) ... ok
test_sequence_mismatching_values_msg (__main__.TestAssertCloseContainer) ... ok
test_abs_diff (__main__.TestAssertCloseErrorMessage) ... ok
test_abs_diff_scalar (__main__.TestAssertCloseErrorMessage) ... ok
test_atol (__main__.TestAssertCloseErrorMessage) ... ok
test_identifier_scalars (__main__.TestAssertCloseErrorMessage) ... ok
test_identifier_tensor_likes (__main__.TestAssertCloseErrorMessage) ... ok
test_mismatched_elements (__main__.TestAssertCloseErrorMessage) ... ok
test_msg_callable (__main__.TestAssertCloseErrorMessage) ... ok
test_msg_callable_diagnostics (__main__.TestAssertCloseErrorMessage) ... ok
test_msg_callable_inputs (__main__.TestAssertCloseErrorMessage) ... ok
test_msg_str (__main__.TestAssertCloseErrorMessage) ... ok
test_not_close (__main__.TestAssertCloseErrorMessage) ... ok
test_not_equal (__main__.TestAssertCloseErrorMessage) ... ok
test_rel_diff (__main__.TestAssertCloseErrorMessage) ... ok
test_rel_diff_scalar (__main__.TestAssertCloseErrorMessage) ... ok
test_rtol (__main__.TestAssertCloseErrorMessage) ... ok
test_zero_div_zero (__main__.TestAssertCloseErrorMessage) ... ok
test_mismatching_device_cuda (__main__.TestAssertCloseMultiDeviceCUDA) ... ok
test_mismatching_device_no_check_cuda (__main__.TestAssertCloseMultiDeviceCUDA) ... ok
test_matching_per_channel (__main__.TestAssertCloseQuantized) ... ok
test_matching_per_tensor (__main__.TestAssertCloseQuantized) ... ok
test_mismatching_is_quantized (__main__.TestAssertCloseQuantized) ... ok
test_mismatching_qscheme (__main__.TestAssertCloseQuantized) ... ok
test_matching_coalesced (__main__.TestAssertCloseSparseCOO) ... ok
test_matching_uncoalesced (__main__.TestAssertCloseSparseCOO) ... ok
test_mismatching_indices_msg (__main__.TestAssertCloseSparseCOO) ... ok
test_mismatching_is_coalesced (__main__.TestAssertCloseSparseCOO) ... ok
test_mismatching_is_coalesced_no_check (__main__.TestAssertCloseSparseCOO) ... ok
test_mismatching_nnz (__main__.TestAssertCloseSparseCOO) ... ok
test_mismatching_values_msg (__main__.TestAssertCloseSparseCOO) ... ok
test_matching (__main__.TestAssertCloseSparseCSR) ... ok
test_mismatching_col_indices_msg (__main__.TestAssertCloseSparseCSR) ... ok
test_mismatching_crow_indices_msg (__main__.TestAssertCloseSparseCSR) ... ok
test_mismatching_values_msg (__main__.TestAssertCloseSparseCSR) ... ok
test_filtering_env_var (__main__.TestFrameworkUtils) ... skipped "test doesn't currently work on the ROCm stack"
test__comparescalars_debug_msg_cuda (__main__.TestTestingCUDA) ... ok
test__comparetensors_debug_msg_cuda (__main__.TestTestingCUDA) ... ok
test__comparetensors_legacy_cuda (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_bool (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_complex128 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_complex64 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_float16 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_float32 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_float64 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_int16 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_int32 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_int64 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_int8 (__main__.TestTestingCUDA) ... ok
test_assertEqual_numpy_cuda_uint8 (__main__.TestTestingCUDA) ... ok
test_assert_messages_cuda (__main__.TestTestingCUDA) ... ok
test_cuda_assert_should_not_stop_common_distributed_test_suite_cuda (__main__.TestTestingCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cuda_assert_should_stop_common_device_type_test_suite_cuda (__main__.TestTestingCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cuda_assert_should_stop_common_utils_test_suite_cuda (__main__.TestTestingCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_get_supported_dtypes_cuda (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_bool (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_float16 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_float32 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_float64 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_int16 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_int32 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_int64 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_int8 (__main__.TestTestingCUDA) ... ok
test_isclose_atol_rtol_greater_than_zero_cuda_uint8 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_bool_cuda (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_complex_cuda_complex128 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_complex_cuda_complex64 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_float_cuda_float16 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_float_cuda_float32 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_float_cuda_float64 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_integer_cuda_int16 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_integer_cuda_int32 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_integer_cuda_int64 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_integer_cuda_int8 (__main__.TestTestingCUDA) ... ok
test_isclose_comparetensors_integer_cuda_uint8 (__main__.TestTestingCUDA) ... ok
test_isclose_equality_shortcut_cuda (__main__.TestTestingCUDA) ... ok
test_isclose_nan_equality_shortcut_cuda_complex128 (__main__.TestTestingCUDA) ... ok
test_isclose_nan_equality_shortcut_cuda_complex64 (__main__.TestTestingCUDA) ... ok
test_isclose_nan_equality_shortcut_cuda_float16 (__main__.TestTestingCUDA) ... ok
test_isclose_nan_equality_shortcut_cuda_float32 (__main__.TestTestingCUDA) ... ok
test_isclose_nan_equality_shortcut_cuda_float64 (__main__.TestTestingCUDA) ... ok
test_make_tensor_cuda_bool (__main__.TestTestingCUDA) ... ok
test_make_tensor_cuda_complex64 (__main__.TestTestingCUDA) ... ok
test_make_tensor_cuda_float32 (__main__.TestTestingCUDA) ... ok
test_make_tensor_cuda_int64 (__main__.TestTestingCUDA) ... ok

----------------------------------------------------------------------
Ran 114 tests in 12.269s

OK (skipped=4)
Running test_torch ... [2021-10-12 11:01:57.840990]
Executing ['/opt/conda/bin/python3.6', 'test_torch.py', '-v'] ... [2021-10-12 11:01:57.841073]
test_basic_vitals (__main__.TestBasicVitalSigns) ... ok
test_basic_vitals_read_write (__main__.TestBasicVitalSigns) ... ok
test_dataloader_vitals (__main__.TestBasicVitalSigns) ... ok
test_advancedindex_mixed_cpu_devices_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_clamp_cuda_float32 (__main__.TestDevicePrecisionCUDA) ... ok
test_clamp_cuda_float64 (__main__.TestDevicePrecisionCUDA) ... ok
test_clamp_cuda_int64 (__main__.TestDevicePrecisionCUDA) ... ok
test_copy_broadcast_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_copy_noncontig_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_device_serialization_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_float16 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_float32 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_float64 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_int16 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_int32 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_int64 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_int8 (__main__.TestDevicePrecisionCUDA) ... ok
test_from_sequence_cuda_uint8 (__main__.TestDevicePrecisionCUDA) ... ok
test_index_add_bfloat16_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_multidevice_serialization_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_type_conversions_same_device_cuda (__main__.TestDevicePrecisionCUDA) ... ok
test_RNGState (__main__.TestTorch) ... ok
test_RNGStateAliasing (__main__.TestTorch) ... ok
test_RNG_after_pickle (__main__.TestTorch) ... ok
test_Size (__main__.TestTorch) ... ok
test_Size_iter (__main__.TestTorch) ... ok
test_Size_scalar (__main__.TestTorch) ... ok
test_add_meta_scalar (__main__.TestTorch) ... ok
test_allow_tensor_metadata_change (__main__.TestTorch) ... ok
test_apply (__main__.TestTorch) ... ok
test_as_subclass (__main__.TestTorch) ... ok
test_assertEqual (__main__.TestTorch) ... ok
test_assert_async (__main__.TestTorch) ... ok
test_backward_hooks_traverse (__main__.TestTorch) ... ok
test_batch_norm_cpu_inference (__main__.TestTorch) ... ok
test_bmm_multithreaded (__main__.TestTorch) ... ok
test_boxMullerState (__main__.TestTorch) ... ok
test_c10_layer_norm (__main__.TestTorch) ... test_torch.py:2456: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.ops._caffe2.LayerNorm(torch.tensor(X), torch.tensor(
test_torch.py:2457: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  weight), torch.tensor(bias), 1, epsilon, True)
ok
test_cat_neg_dim (__main__.TestTorch) ... ok
test_chunk_neg_dim (__main__.TestTorch) ... ok
test_contains (__main__.TestTorch) ... ok
test_conv2 (__main__.TestTorch) ... skipped 'Not implemented yet'
test_conv3 (__main__.TestTorch) ... skipped 'Not implemented yet'
test_conv3_conv2_eq_valid (__main__.TestTorch) ... skipped 'Not implemented yet'
test_copy_broadcast (__main__.TestTorch) ... ok
test_copy_dtypes (__main__.TestTorch) ... ok
test_copy_many_to_one (__main__.TestTorch) ... ok
test_copy_transpose (__main__.TestTorch) ... ok
test_copy_transpose_2d_broadcast (__main__.TestTorch) ... ok
test_cuda_not_built (__main__.TestTorch) ... skipped "CUDA is built, can't test CUDA not built error"
test_cummax_neg_dim (__main__.TestTorch) ... ok
test_cummin_neg_dim (__main__.TestTorch) ... ok
test_cumprod_neg_dim (__main__.TestTorch) ... ok
test_cumsum_neg_dim (__main__.TestTorch) ... ok
test_cxx_flags (__main__.TestTorch) ... ok
test_dead_weak_ref (__main__.TestTorch) ... [TORCH_VITAL] Dataloader.basic_unit_test		 TEST_VALUE_STRING
[TORCH_VITAL] CUDA.used		 true
[TORCH_VITAL] Dataloader.basic_unit_test		 TEST_VALUE_STRING
[TORCH_VITAL] CUDA.used		 true
[TORCH_VITAL] Dataloader.enabled		 True
[TORCH_VITAL] Dataloader.basic_unit_test		 TEST_VALUE_STRING
[TORCH_VITAL] CUDA.used		 true
[W python_variable.cpp:78] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function concrete_decref_fn)
ok
test_deepcopy_gradient (__main__.TestTorch) ... ok
test_deepcopy_parameter (__main__.TestTorch) ... ok
test_detach_meta (__main__.TestTorch) ... ok
test_deterministic_flag (__main__.TestTorch) ... ok
test_device (__main__.TestTorch) ... ok
test_dir (__main__.TestTorch) ... ok
test_doc (__main__.TestTorch) ... ok
test_doc_template (__main__.TestTorch) ... ok
test_dot_data_use (__main__.TestTorch) ... ok
test_dtype_is_signed (__main__.TestTorch) ... ok
test_dtypes (__main__.TestTorch) ... ok
test_element_size (__main__.TestTorch) ... ok
test_empty_meta (__main__.TestTorch) ... ok
test_empty_storage_view (__main__.TestTorch) ... ok
test_equal (__main__.TestTorch) ... ok
test_error_msg_type_translation (__main__.TestTorch) ... ok
test_fconv3_fconv2_eq (__main__.TestTorch) ... skipped 'Not implemented yet'
test_fill_diagonal (__main__.TestTorch) ... ok
test_from_buffer (__main__.TestTorch) ... ok
test_from_file (__main__.TestTorch) ... ok
test_gather (__main__.TestTorch) ... ok
test_gather_neg_dim (__main__.TestTorch) ... ok
test_generator_cpu (__main__.TestTorch) ... ok
test_half_tensor (__main__.TestTorch) ... ok
test_has_internal_overlap (__main__.TestTorch) ... ok
test_has_storage (__main__.TestTorch) ... ok
test_index_add (__main__.TestTorch) ... ok
test_index_add_all_dtypes (__main__.TestTorch) ... ok
test_index_add_neg_dim (__main__.TestTorch) ... ok
test_index_copy_neg_dim (__main__.TestTorch) ... ok
test_index_fill_neg_dim (__main__.TestTorch) ... ok
test_index_select_neg_dim (__main__.TestTorch) ... ok
test_invalid_generator_raises (__main__.TestTorch) ... ok
test_is_nonzero (__main__.TestTorch) ... ok
test_is_same_size (__main__.TestTorch) ... ok
test_iter (__main__.TestTorch) ... ok
test_kthvalue_neg_dim (__main__.TestTorch) ... ok
test_logcumsumexp_neg_dim (__main__.TestTorch) ... ok
test_manual_seed (__main__.TestTorch) ... ok
test_map (__main__.TestTorch) ... ok
test_map2 (__main__.TestTorch) ... ok
test_max_neg_dim (__main__.TestTorch) ... ok
test_mean_neg_dim (__main__.TestTorch) ... ok
test_median_neg_dim (__main__.TestTorch) ... ok
test_memory_format (__main__.TestTorch) ... ok
test_memory_format_contiguous_returns_same_tensor_if_already_satisfies (__main__.TestTorch) ... ok
test_memory_format_empty (__main__.TestTorch) ... ok
test_min_neg_dim (__main__.TestTorch) ... ok
test_mode_neg_dim (__main__.TestTorch) ... ok
test_multinomial_invalid_probs (__main__.TestTorch) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_nanmedian_neg_dim (__main__.TestTorch) ... ok
test_narrow_neg_dim (__main__.TestTorch) ... ok
test_ndim (__main__.TestTorch) ... ok
test_new (__main__.TestTorch) ... ok
test_newaxis_numpy_comparison (__main__.TestTorch) ... ok
test_newindex (__main__.TestTorch) ... ok
test_norm_neg_dim (__main__.TestTorch) ... ok
test_normal_shape (__main__.TestTorch) ... ok
test_numel (__main__.TestTorch) ... ok
test_ort_error (__main__.TestTorch) ... ok
test_parallel_info (__main__.TestTorch) ... ok
test_parsing_double (__main__.TestTorch) ... ok
test_parsing_int64 (__main__.TestTorch) ... ok
test_parsing_intlist (__main__.TestTorch) ... ok
test_permute (__main__.TestTorch) ... ok
test_pickle (__main__.TestTorch) ... ok
test_pickle_dtype (__main__.TestTorch) ... ok
test_pickle_function (__main__.TestTorch) ... ok
test_pickle_parameter (__main__.TestTorch) ... ok
test_pickle_parameter_no_requires_grad (__main__.TestTorch) ... ok
test_pickle_size (__main__.TestTorch) ... ok
test_pin_memory (__main__.TestTorch) ... ok
test_print (__main__.TestTorch) ... ok
test_prod_neg_dim (__main__.TestTorch) ... ok
test_pyobj_preserved (__main__.TestTorch) ... ok
test_qengine (__main__.TestTorch) ... ok
test_renorm_neg_dim (__main__.TestTorch) ... ok
test_resurrected_weak_ref (__main__.TestTorch) ... ok
test_reversed (__main__.TestTorch) ... ok
test_scatter (__main__.TestTorch) ... ok
test_scatterAdd (__main__.TestTorch) ... ok
test_scatterFill (__main__.TestTorch) ... ok
test_scatterReduce (__main__.TestTorch) ... ok
test_scatter_add_mult_index (__main__.TestTorch) ... ok
test_scatter_neg_dim (__main__.TestTorch) ... ok
test_select_neg_dim (__main__.TestTorch) ... ok
test_set_flush_denormal (__main__.TestTorch) ... skipped 'flush_denormal not supported'
test_show_config (__main__.TestTorch) ... ok
test_size_neg_dim (__main__.TestTorch) ... ok
test_sizeof (__main__.TestTorch) ... ok
test_slice (__main__.TestTorch) ... ok
test_slow_test (__main__.TestTorch) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_sobolengine_bounds (__main__.TestTorch) ... ok
test_sobolengine_bounds_scrambled (__main__.TestTorch) ... ok
test_sobolengine_continuing (__main__.TestTorch) ... ok
test_sobolengine_continuing_scrambled (__main__.TestTorch) ... ok
test_sobolengine_distribution (__main__.TestTorch) ... ok
test_sobolengine_distribution_scrambled (__main__.TestTorch) ... ok
test_sobolengine_draw (__main__.TestTorch) ... ok
test_sobolengine_draw_base2 (__main__.TestTorch) ... ok
test_sobolengine_draw_base2_scrambled (__main__.TestTorch) ... ok
test_sobolengine_draw_scrambled (__main__.TestTorch) ... ok
test_sobolengine_fast_forward (__main__.TestTorch) ... ok
test_sobolengine_fast_forward_scrambled (__main__.TestTorch) ... ok
test_sobolengine_first_point (__main__.TestTorch) ... ok
test_sobolengine_high_dim (__main__.TestTorch) ... ok
test_sobolengine_raise (__main__.TestTorch) ... ok
test_sobolengine_reset (__main__.TestTorch) ... ok
test_sobolengine_reset_scrambled (__main__.TestTorch) ... ok
test_sort_neg_dim (__main__.TestTorch) ... ok
test_split_neg_dim (__main__.TestTorch) ... ok
test_squeeze_neg_dim (__main__.TestTorch) ... ok
test_std_neg_dim (__main__.TestTorch) ... ok
test_storage_casts (__main__.TestTorch) ... ok
test_structseq_repr (__main__.TestTorch) ... ok
test_subclass_preserved (__main__.TestTorch) ... ok
test_subclass_tensors (__main__.TestTorch) ... ok
test_sum_neg_dim (__main__.TestTorch) ... ok
test_t_not_2d_error (__main__.TestTorch) ... ok
test_tensor_base_init (__main__.TestTorch) ... ok
test_tensor_base_new (__main__.TestTorch) ... ok
test_tensor_ctor_scalar (__main__.TestTorch) ... ok
test_tensor_cycle_via_dict (__main__.TestTorch) ... ok
test_tensor_cycle_via_slots (__main__.TestTorch) ... ok
test_tensor_dict_dealloc (__main__.TestTorch) ... ok
test_tensor_finalizer_dealloc (__main__.TestTorch) ... ok
test_tensor_set (__main__.TestTorch) ... ok
test_tensor_set_errors (__main__.TestTorch) ... ok
test_tensor_slot_dealloc (__main__.TestTorch) ... ok
test_tensor_weakref_dealloc (__main__.TestTorch) ... ok
test_tensoriterator_output_setup (__main__.TestTorch) ... ok
test_to (__main__.TestTorch) ... ok
test_to_with_tensor (__main__.TestTorch) ... ok
test_topk_neg_dim (__main__.TestTorch) ... ok
test_torch_from_file (__main__.TestTorch) ... ok
test_transpose_neg_dim (__main__.TestTorch) ... ok
test_type (__main__.TestTorch) ... ok
test_type_conversion_via_dtype_name (__main__.TestTorch) ... ok
test_unbind_neg_dim (__main__.TestTorch) ... ok
test_unflatten (__main__.TestTorch) ... ok
test_unfold_neg_dim (__main__.TestTorch) ... ok
test_unsqueeze_neg_dim (__main__.TestTorch) ... ok
test_upsample_nearest1d_meta (__main__.TestTorch) ... ok
test_upsample_nearest2d_meta (__main__.TestTorch) ... ok
test_var_neg_dim (__main__.TestTorch) ... ok
test_where_bool_tensor (__main__.TestTorch) ... ok
test_where_invalid_device (__main__.TestTorch) ... ok
test_where_tensor (__main__.TestTorch) ... ok
test_xcorr3_xcorr2_eq (__main__.TestTorch) ... skipped 'Not implemented yet'
test_xcorr3_xcorr2_eq_full (__main__.TestTorch) ... skipped 'Not implemented yet'
test_addcdiv_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcdiv_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_addcmul_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_edge_cases_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_bernoulli_edge_cases_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_bernoulli_edge_cases_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_bernoulli_mem_overlap_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_p_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_p_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_p_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_bool (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bernoulli_self_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_bool_tensor_value_change_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_broadcast_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cauchy_kstest_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... /opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192
  return f(*args, **kwds)
/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
ok
test_cauchy_kstest_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cauchy_kstest_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cauchy_kstest_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cauchy_no_inf_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cauchy_no_inf_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cdist_cuda_backward_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_empty_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_large_batch_cuda (__main__.TestTorchDeviceTypeCUDA) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_cdist_large_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_non_contiguous_batch_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_non_contiguous_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_norm_batch_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cdist_norm_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_clone_all_dtypes_and_devices_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_clone_not_memory_dense_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_clone_zero_stride_dim_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_complex_unsupported_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_complex_unsupported_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_constants_cuda (__main__.TestTorchDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_conv_transposed_backward_agnostic_to_memory_format_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_conv_transposed_large_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_copy_all_dtypes_and_devices_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_copy_mem_overlap_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_corrcoef_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... test_torch.py:4447: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Correlation.cpp:99.)
  res = torch.corrcoef(x)
/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis)
/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide
  ret, rcount, out=ret, casting='unsafe', subok=False)
/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2526: RuntimeWarning: Degrees of freedom <= 0 for slice
  c = cov(x, y, rowvar)
/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: divide by zero encountered in true_divide
  c *= np.true_divide(1, fact)
/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in multiply
  c *= np.true_divide(1, fact)
ok
test_corrcoef_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_corrcoef_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cov_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... test_torch.py:4454: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Correlation.cpp:99.)
  res = torch.cov(t, correction=correction, fweights=fweights, aweights=aweights)
test_torch.py:4458: RuntimeWarning: Degrees of freedom <= 0 for slice
  ref = np.cov(t, ddof=correction, fweights=fweights, aweights=aweights)
/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:425: RuntimeWarning: invalid value encountered in multiply
  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl
ok
test_cov_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cov_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cov_error_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cpp_warnings_have_python_context_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cublas_config_nondeterministic_alert_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cummax_cummin_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cummax_discontiguous_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cummin_discontiguous_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cumprod_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_cumsum_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_deepcopy_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_deepcopy_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_deepcopy_scalar_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_deepcopy_scalar_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_device_guard_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_bool (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_bool (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_diff_noncontig_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dim_function_empty_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_discontiguous_out_cumsum_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dist_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_bool (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_complex128 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_int16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_dlpack_conversion_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_embedding_scalar_weight_error_cuda (__main__.TestTorchDeviceTypeCUDA) ... ok
test_error_gradient_cuda_complex64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_error_gradient_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_error_gradient_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_errors_index_copy_cuda (__main__.TestTorchDeviceTypeCUDA) ... skipped 'Only runs on cpu'
test_exponential_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_kstest_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_kstest_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_kstest_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_kstest_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_no_zero_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_exponential_no_zero_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) ... ok
test_gather_backward_deterministic_path_cuda (__main__.TestTorchDeviceTypeCUDA) ... "Cannot find Symbol"
test_torch failed! Received signal: SIGIOT
Running test_type_info ... [2021-10-12 11:05:41.281595]
Executing ['/opt/conda/bin/python3.6', 'test_type_info.py', '-v'] ... [2021-10-12 11:05:41.281654]
test_finfo (__main__.TestDTypeInfo) ... ok
test_iinfo (__main__.TestDTypeInfo) ... ok
test_invalid_input (__main__.TestDTypeInfo) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.003s

OK
Running test_type_promotion ... [2021-10-12 11:05:43.580543]
Executing ['/opt/conda/bin/python3.6', 'test_type_promotion.py', '-v'] ... [2021-10-12 11:05:43.580617]
test_add_wrapped_cuda (__main__.TestTypePromotionCUDA) ... ok
test_alpha_mismatch_cuda (__main__.TestTypePromotionCUDA) ... ok
test_alternate_result_cuda (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_bool_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float16_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float32_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_float64_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int16_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int32_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int64_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_int8_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_bool (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_float16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_float32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_float64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_int16 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_int32 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_int64 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_int8 (__main__.TestTypePromotionCUDA) ... ok
test_atan2_type_promotion_cuda_uint8_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_bfloat16_cuda (__main__.TestTypePromotionCUDA) ... ok
test_booleans_cuda (__main__.TestTypePromotionCUDA) ... test_type_promotion.py:317: UserWarning: This overload of add is deprecated:
	add(Tensor input, Number alpha, Tensor other, *, Tensor out)
Consider using one of the following signatures instead:
	add(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)
  torch.tensor(True, device=device), True),
ok
test_can_cast_cuda (__main__.TestTypePromotionCUDA) ... ok
test_cat_different_dtypes_cuda (__main__.TestTypePromotionCUDA) ... ok
test_cat_out_different_dtypes_cuda (__main__.TestTypePromotionCUDA) ... ok
test_comparison_ops_with_type_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_complex_assertraises_cuda (__main__.TestTypePromotionCUDA) ... ok
test_complex_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_complex_scalar_mult_tensor_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_computation_ignores_out_cuda (__main__.TestTypePromotionCUDA) ... ok
test_create_bool_tensors_cuda (__main__.TestTypePromotionCUDA) ... test_type_promotion.py:331: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/RangeFactories.hip:66.)
  self.assertEqual(torch.linspace(False, True, device=device), torch.linspace(0, 1, device=device))
test_type_promotion.py:332: UserWarning: Not providing a value for logspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/RangeFactories.hip:125.)
  self.assertEqual(torch.logspace(False, True, device=device), torch.logspace(0, 1, device=device))
ok
test_div_promotion_cuda_bool (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_cuda_int16 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_cuda_int32 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_cuda_int64 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_cuda_int8 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_cuda_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_bool (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_float32 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_float64 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_int16 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_int32 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_int64 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_int8 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_inplace_cuda_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_bool (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_float32 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_float64 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_int16 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_int32 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_int64 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_int8 (__main__.TestTypePromotionCUDA) ... ok
test_div_promotion_out_cuda_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_float_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_from_issue_cuda (__main__.TestTypePromotionCUDA) ... ok
test_half_cuda (__main__.TestTypePromotionCUDA) ... ok
test_indexing_cuda (__main__.TestTypePromotionCUDA) ... ok
test_indexing_fail_cuda (__main__.TestTypePromotionCUDA) ... ok
test_inplace_cuda (__main__.TestTypePromotionCUDA) ... ok
test_int_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_int_to_float_cuda (__main__.TestTypePromotionCUDA) ... ok
test_integer_addcdiv_deprecated_cuda_int16 (__main__.TestTypePromotionCUDA) ... ok
test_integer_addcdiv_deprecated_cuda_int32 (__main__.TestTypePromotionCUDA) ... ok
test_integer_addcdiv_deprecated_cuda_int64 (__main__.TestTypePromotionCUDA) ... ok
test_integer_addcdiv_deprecated_cuda_int8 (__main__.TestTypePromotionCUDA) ... ok
test_integer_addcdiv_deprecated_cuda_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_lt_with_type_promotion_cuda (__main__.TestTypePromotionCUDA) ... ok
test_many_promotions_cuda (__main__.TestTypePromotionCUDA) ... ok
test_mixed_type_backward_cuda (__main__.TestTypePromotionCUDA) ... ok
test_non_promoting_ops_cuda (__main__.TestTypePromotionCUDA) ... ok
test_numpy_array_binary_ufunc_promotion_cuda_bool_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_bool_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex128_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_complex64_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float16_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float32_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_float64_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int16_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int32_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int64_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_int8_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_bool (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_complex128 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_complex64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_float16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_float32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_float64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_int16 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_int32 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_int64 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_int8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_numpy_array_binary_ufunc_promotion_cuda_uint8_uint8 (__main__.TestTypePromotionCUDA) ... skipped 'Only runs on cpu'
test_promote_self_cuda (__main__.TestTypePromotionCUDA) ... ok
test_promote_types_cuda (__main__.TestTypePromotionCUDA) ... ok
test_result_type_cuda_bfloat16_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bfloat16_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_bool_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex128_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_complex64_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float16_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float32_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_float64_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int16_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int32_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int64_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_int8_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_bfloat16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_bool (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_complex128 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_complex64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_float16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_float32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_float64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_int16 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_int32 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_int64 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_int8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_cuda_uint8_uint8 (__main__.TestTypePromotionCUDA)
Test result_type for tensor vs tensor and scalar vs scalar. ... ok
test_result_type_tensor_vs_scalar_cuda (__main__.TestTypePromotionCUDA) ... ok
test_sparse_add_cuda (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_cuda (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_promotion_cuda_bool (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_promotion_cuda_int16 (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_promotion_cuda_int32 (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_promotion_cuda_int64 (__main__.TestTypePromotionCUDA) ... ok
test_sparse_div_promotion_cuda_uint8 (__main__.TestTypePromotionCUDA) ... ok
test_sparse_mul_cuda (__main__.TestTypePromotionCUDA) ... ok
test_sparse_sub_cuda (__main__.TestTypePromotionCUDA) ... ok
test_transpose_cuda (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex128_complex128 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex128_complex64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex128_float32 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex128_float64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex128_int64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex64_complex128 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex64_complex64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex64_float32 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex64_float64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_complex64_int64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float32_complex128 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float32_complex64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float32_float32 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float32_float64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float32_int64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float64_complex128 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float64_complex64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float64_float32 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float64_float64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_float64_int64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_int64_complex128 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_int64_complex64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_int64_float32 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_int64_float64 (__main__.TestTypePromotionCUDA) ... ok
test_unary_op_out_casting_cuda_int64_int64 (__main__.TestTypePromotionCUDA) ... ok
test_unsigned_cuda (__main__.TestTypePromotionCUDA) ... ok

----------------------------------------------------------------------
Ran 438 tests in 57.454s

OK (skipped=121)
Running test_unary_ufuncs ... [2021-10-12 11:06:44.620791]
Executing ['/opt/conda/bin/python3.6', 'test_unary_ufuncs.py', '-v'] ... [2021-10-12 11:06:44.620876]
test_abs_angle_complex_to_float_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... test_unary_ufuncs.py:710: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Copy.cpp:244.)
  float_out = torch.empty_like(t).float()
test_unary_ufuncs.py:717: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [60].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/Resize.cpp:23.)
  torch_fn(t, out=float_out)
ok
test_abs_angle_complex_to_float_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_abs_big_number_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_abs_signed_zero_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_abs_signed_zero_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_abs_zero_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_abs_zero_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_batch_vs_slicing_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_batch_vs_slicing_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_batch_vs_slicing_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_complex_edge_values_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_contig_size1_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_contig_size1_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_contig_size1_large_dim_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_contig_size1_large_dim_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_large_dim_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_size1_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_contig_vs_every_other_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_contig_vs_every_other_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_every_other_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_contig_vs_transposed_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_contig_vs_transposed_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_contig_vs_transposed_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_digamma_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_digamma_special_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_digamma_special_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_exp_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:1367: RuntimeWarning: invalid value encountered in exp
  np_result = np_fn(a)
ok
test_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:1367: RuntimeWarning: overflow encountered in exp
  np_result = np_fn(a)
ok
test_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_exp_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_exp_slow_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_float_domains_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acos_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_acosh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_asin_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_atanh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_erfinv_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'unsupported dtype'
test_float_domains_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_erfinv_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log10_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log1p_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log2_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_log_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_logit_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_mvlgamma_mvlgamma_p_1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_3_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_3_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_5_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_mvlgamma_mvlgamma_p_5_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_float_domains_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_rsqrt_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_special_ndtri_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'unsupported dtype'
test_float_domains_special_ndtri_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'unsupported dtype'
test_float_domains_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_special_ndtri_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_float_domains_sqrt_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_frexp_assert_raises_cuda (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_out_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_out_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_frexp_out_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_hardshrink_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_hardshrink_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_hardshrink_edge_cases_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_hardshrink_edge_cases_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_hardsigmoid_backward_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardsigmoid_backward_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardsigmoid_backward_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardsigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardsigmoid_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardswish_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardswish_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_hardswish_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range2_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_range3_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_special_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_special_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_special_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_i0_special_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igamma_common_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_igamma_common_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_igamma_edge_cases_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igamma_edge_cases_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igammac_common_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igammac_common_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igammac_edge_cases_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_igammac_edge_cases_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_complex_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_complex_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_int_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_int_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_int_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_isinf_isnan_int_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isfinite_type_cuda (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_isinf_type_cuda (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_isposinf_isneginf_complex_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_complex_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_float_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_float_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_float_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_float_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_int_and_bool_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isposinf_isneginf_non_boolean_output_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_complex_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_complex_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_nan_inf_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_isreal_noncomplex_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_mish_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... test_unary_ufuncs.py:1026: RuntimeWarning: overflow encountered in exp
  expected_output_np = input_np * np.tanh(np.log1p(np.exp(input_np)))
ok
test_mish_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_mvlgamma_argcheck_cuda (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_neg_error_message_cuda (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_non_contig_expand_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_non_contig_expand_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expand_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_non_contig_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_non_contig_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_non_contig_index_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_non_contig_index_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_index_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_non_contig_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_empty_cuda (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_nonzero_noncontiguous_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_op_invert_cuda (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_out_arg_all_dtypes_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_ceil_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_floor_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_frac_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_arg_all_dtypes_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_arg_all_dtypes_frexp_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_out_arg_all_dtypes_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_imag_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_lgamma_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_nn_functional_logsigmoid_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped "Skipped! Op doesn't support out= kwarg."
test_out_arg_all_dtypes_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_real_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_out_arg_all_dtypes_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_round_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_bool (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_out_arg_all_dtypes_trunc_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_polygamma_neg_cuda (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_extremal_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_extremal_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle extremal values'
test_reference_numerics_extremal_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_extremal_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_extremal_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_hard_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_hard_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'This op does not handle large values'
test_reference_numerics_hard_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_hard_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_hard_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_abs_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_acosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_angle_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_angle_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_angle_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_angle_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_asinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_atanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_bitwise_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_bitwise_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_ceil_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_ceil_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_ceil_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_clamp_scalar_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_clamp_scalar_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_clamp_scalar_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_clamp_scalar_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_clamp_scalar_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_conj_physical_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cos_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_cosh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_deg2rad_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_deg2rad_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_deg2rad_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_deg2rad_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_deg2rad_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_digamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_digamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_digamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_digamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erf_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erf_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erf_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erf_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erf_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_erfinv_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_erfinv_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_erfinv_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_erfinv_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_exp2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_exp_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_exp_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_expm1_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_expm1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_expm1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_expm1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_expm1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_floor_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_floor_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_floor_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_frac_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_frac_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_frac_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_frexp_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_normal_frexp_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped "test doesn't currently work on the ROCm stack"
test_reference_numerics_normal_i0_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_i0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_i0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_i0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_i0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_imag_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_lgamma_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_lgamma_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_lgamma_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_lgamma_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log10_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log1p_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log1p_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log1p_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log1p_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log1p_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log2_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log2_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_log2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_log_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logical_not_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_logit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_5_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_5_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_5_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_mvlgamma_mvlgamma_p_5_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nan_to_num_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nan_to_num_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nan_to_num_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nan_to_num_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nan_to_num_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_neg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nn_functional_logsigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_nn_functional_logsigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_polygamma_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_polygamma_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_polygamma_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_polygamma_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_polygamma_polygamma_n_1_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_2_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_2_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_2_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_2_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_3_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_3_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_3_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_3_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_4_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_4_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_4_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_polygamma_polygamma_n_4_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_positive_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_positive_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_positive_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_positive_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_positive_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_positive_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rad2deg_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_rad2deg_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rad2deg_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rad2deg_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rad2deg_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_real_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_reciprocal_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_reciprocal_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_reciprocal_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_reciprocal_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_reciprocal_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_reciprocal_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_round_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_round_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_round_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_rsqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sgn_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sigmoid_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sign_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sign_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sign_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sign_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sign_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_signbit_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_signbit_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_signbit_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_signbit_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_signbit_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sin_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinc_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Skipped!'
test_reference_numerics_normal_sinc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinc_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinc_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sinh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_entr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_entr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_entr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_entr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_entr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_erfcx_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_erfcx_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_erfcx_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i0e_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i0e_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i0e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i0e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i0e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1e_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1e_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_i1e_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtr_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtr_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtr_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtr_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtr_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtri_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtri_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_ndtri_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_polygamma_special_polygamma_n_0_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_polygamma_special_polygamma_n_0_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_polygamma_special_polygamma_n_0_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_special_polygamma_special_polygamma_n_0_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_sqrt_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_square_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tan_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_tanh_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_trunc_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_trunc_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_reference_numerics_normal_trunc_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_silu_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_silu_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_i0_i1_vs_scipy_cuda_bfloat16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_i0_i1_vs_scipy_cuda_float16 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_i0_i1_vs_scipy_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_i0_i1_vs_scipy_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_ndtr_vs_scipy_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... ok
test_special_ndtr_vs_scipy_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok
test_threshold_cuda_complex128 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_complex64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_float32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_int16 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_int32 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_int64 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_int8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_threshold_cuda_uint8 (__main__.TestUnaryUfuncsCUDA) ... skipped 'Only runs on cpu'
test_unary_out_op_mem_overlap_cuda_float64 (__main__.TestUnaryUfuncsCUDA) ... ok

----------------------------------------------------------------------
Ran 4470 tests in 1070.065s

OK (skipped=196)
Running test_utils ... [2021-10-12 11:24:38.796803]
Executing ['/opt/conda/bin/python3.6', 'test_utils.py', '-v'] ... [2021-10-12 11:24:38.796892]
test_assert_scriptable (__main__.TestAssert) ... ok
test_assert_true (__main__.TestAssert) ... ok
test_bottleneck_cpu_only (__main__.TestBottleneck) ... skipped 'CPU-only test'
test_bottleneck_cuda (__main__.TestBottleneck) ... ok
test_checkpoint (__main__.TestCheckpoint) ... ok
test_checkpoint_module_list (__main__.TestCheckpoint) ... ok
test_checkpoint_no_tensors (__main__.TestCheckpoint) ... /opt/conda/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
ok
test_checkpoint_non_tensor (__main__.TestCheckpoint) ... ok
test_checkpoint_non_tensor_inputs_outputs (__main__.TestCheckpoint) ... ok
test_checkpoint_partial_grad (__main__.TestCheckpoint) ... ok
test_checkpoint_rng_cpu (__main__.TestCheckpoint) ... ok
test_checkpoint_rng_cuda (__main__.TestCheckpoint) ... ok
test_checkpoint_sequential_deprecated_multiple_args (__main__.TestCheckpoint) ... ok
test_checkpoint_sequential_deprecated_no_args (__main__.TestCheckpoint) ... ok
test_checkpoint_trigger (__main__.TestCheckpoint) ... ok
test_checkpoint_valid (__main__.TestCheckpoint) ... ok
test_smoke (__main__.TestCollectEnv) ... ok
test_python_exception_writing (__main__.TestCrashHandler) ... Wrote minidump to /tmp/tmpii6jjxvw/d602f359-8025-4dc6-655120b1-efbe2c11.dmp
ok
test_multi_drop (__main__.TestDataLoaderUtils) ... ok
test_multi_keep (__main__.TestDataLoaderUtils) ... skipped 'FIXME: Intermittent CUDA out-of-memory error on Windows and time-out under ASAN'
test_random_seed (__main__.TestDataLoaderUtils) ... ok
test_single_drop (__main__.TestDataLoaderUtils) ... ok
test_single_keep (__main__.TestDataLoaderUtils) ... ok
test_external_module_register (__main__.TestExtensionUtils) ... ok
test_deprecated (__main__.TestFFI) ... ok
test_import_hipify (__main__.TestHipify) ... ok
test_download_url_to_file (__main__.TestHub) ... ok
test_hub_dir (__main__.TestHub) ... ok
test_list_entrypoints (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/archive/master.zip" to /tmp/tmpa897mkyvhub_dir/master.zip
ok
test_load_commit_from_forked_repo (__main__.TestHub) ... ok
test_load_from_branch (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/archive/ci/test_slash.zip" to /tmp/tmpa897mkyvhub_dir/ci_test_slash.zip
Downloading: "https://github.com/ailzhang/torchhub_example/releases/download/0.1/mnist_init_ones" to /tmp/tmpa897mkyvhub_dir/checkpoints/mnist_init_ones
  0%|          | 0.00/1.65M [00:00<?, ?B/s]100%|| 1.65M/1.65M [00:00<00:00, 26.4MB/s]
ok
test_load_from_github (__main__.TestHub) ... ok
test_load_from_local_dir (__main__.TestHub) ... Using cache found in /tmp/tmpa897mkyvhub_dir/ailzhang_torchhub_example_master
ok
test_load_state_dict_from_url (__main__.TestHub) ... ok
test_load_state_dict_from_url_with_name (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/releases/download/0.1/mnist_init_ones" to /tmp/tmp2ao0nzrghub_dir/checkpoints/test_file
  0%|          | 0.00/1.65M [00:00<?, ?B/s]100%|| 1.65M/1.65M [00:00<00:00, 27.9MB/s]
ok
test_load_zip_1_6_checkpoint (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/archive/master.zip" to /tmp/tmp2ao0nzrghub_dir/master.zip
Downloading: "https://github.com/ailzhang/torchhub_example/releases/download/0.1/mnist_init_ones_1_6.zip" to /tmp/tmp2ao0nzrghub_dir/checkpoints/mnist_init_ones_1_6.zip
  0%|          | 0.00/1.65M [00:00<?, ?B/s]100%|| 1.65M/1.65M [00:00<00:00, 21.6MB/s]
ok
test_load_zip_checkpoint (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/releases/download/0.1/mnist_init_ones.zip" to /tmp/tmp2ao0nzrghub_dir/checkpoints/mnist_init_ones.zip
  0%|          | 0.00/2.63k [00:00<?, ?B/s]100%|| 2.63k/2.63k [00:00<00:00, 2.79MB/s]
/opt/conda/lib/python3.6/site-packages/torch/hub.py:497: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.
  warnings.warn('Falling back to the old format < 1.6. This support will be '
ok
test_set_dir (__main__.TestHub) ... Downloading: "https://github.com/ailzhang/torchhub_example/archive/master.zip" to /tmp/master.zip
Downloading: "https://github.com/ailzhang/torchhub_example/releases/download/0.1/mnist_init_ones" to /tmp/checkpoints/mnist_init_ones
  0%|          | 0.00/1.65M [00:00<?, ?B/s]100%|| 1.65M/1.65M [00:00<00:00, 24.5MB/s]
ok
test_check_onnx_broadcast (__main__.TestONNXUtils) ... ok
test_prepare_onnx_paddings (__main__.TestONNXUtils) ... ok
test_load_standalone (__main__.TestStandaloneCPPJIT) ... ok

----------------------------------------------------------------------
Ran 41 tests in 41.367s

OK (skipped=2)
Running test_view_ops ... [2021-10-12 11:25:23.248918]
Executing ['/opt/conda/bin/python3.6', 'test_view_ops.py', '-v'] ... [2021-10-12 11:25:23.249000]
test_T_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_complex64 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_float16 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_float64 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_int16 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_int32 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_int8 (__main__.TestOldViewOpsCUDA) ... ok
test_atleast_cuda_uint8 (__main__.TestOldViewOpsCUDA) ... ok
test_big_transpose_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_shapes_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_broadcast_tensors_cuda_float32 (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_broadcast_to_cuda_bool (__main__.TestOldViewOpsCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:1603: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /var/lib/jenkins/pytorch/torch/csrc/utils/tensor_numpy.cpp:187.)
  return torch.from_numpy(a)
ok
test_broadcast_to_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_complex64 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_float16 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_float64 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_int16 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_int32 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_int8 (__main__.TestOldViewOpsCUDA) ... ok
test_broadcast_to_cuda_uint8 (__main__.TestOldViewOpsCUDA) ... ok
test_chunk_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_conj_neg_view_numpy_error_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_contiguous_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_empty_reshape_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_expand_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_flatten_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_memory_format_resize__cuda (__main__.TestOldViewOpsCUDA) ... ok
test_memory_format_resize_as_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_narrow_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_narrow_tensor_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_python_types_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_ravel_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_bfloat16 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_bool (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_complex64 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_float16 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_float64 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_int16 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_int32 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_int8 (__main__.TestOldViewOpsCUDA) ... ok
test_reshape_view_semantics_cuda_uint8 (__main__.TestOldViewOpsCUDA) ... ok
test_resize_all_dtypes_and_devices_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_resize_as_all_dtypes_and_devices_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_resize_as_preserves_strides_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_split_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_t_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_tensor_split_errors_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_bool (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_complex64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_float16 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_float64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_int16 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_int32 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_int8 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_indices_cuda_uint8 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_bool (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_complex64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_float16 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_float64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_int16 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_int32 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_int8 (__main__.TestOldViewOpsCUDA) ... ok
test_tensor_split_sections_cuda_uint8 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_invalid_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_invalid_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_invalid_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_vs_numpy_cuda_complex128 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_vs_numpy_cuda_float32 (__main__.TestOldViewOpsCUDA) ... ok
test_transpose_vs_numpy_cuda_int64 (__main__.TestOldViewOpsCUDA) ... ok
test_unsqueeze_cuda (__main__.TestOldViewOpsCUDA) ... skipped 'Only runs on cpu'
test_view_all_dtypes_and_devices_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_view_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_view_empty_cuda (__main__.TestOldViewOpsCUDA) ... ok
test_T_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_advanced_indexing_assignment_cuda (__main__.TestViewOpsCUDA) ... ok
test_advanced_indexing_nonview_cuda (__main__.TestViewOpsCUDA) ... ok
test_as_strided_inplace_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_as_strided_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_basic_indexing_ellipses_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_basic_indexing_newaxis_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_basic_indexing_slice_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_chunk_view_cuda (__main__.TestViewOpsCUDA) ... skipped 'See https://github.com/pytorch/pytorch/pull/32720'
test_conj_imag_view_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_conj_imag_view_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_conj_self_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_contiguous_nonview_cuda (__main__.TestViewOpsCUDA) ... ok
test_contiguous_self_cuda (__main__.TestViewOpsCUDA) ... ok
test_diagonal_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_expand_as_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_expand_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_flatten_nonview_cuda (__main__.TestViewOpsCUDA) ... ok
test_flatten_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_movedim_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_narrow_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_permute_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_noncomplex_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_view_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_real_imag_view_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_reshape_as_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_reshape_nonview_cuda (__main__.TestViewOpsCUDA) ... ok
test_reshape_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_select_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_bool (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_complex128 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_complex64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_float16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_float32 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_float64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_int16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_int32 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_int64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_int8 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex128_uint8 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_bool (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_complex128 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_complex64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_float16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_float32 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_float64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_int16 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_int32 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_int64 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_int8 (__main__.TestViewOpsCUDA) ... ok
test_set_real_imag_cuda_complex64_uint8 (__main__.TestViewOpsCUDA) ... ok
test_split_view_cuda (__main__.TestViewOpsCUDA) ... skipped 'See https://github.com/pytorch/pytorch/pull/32720'
test_squeeze_inplace_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_squeeze_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_t_inplace_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_t_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_transpose_inplace_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_transpose_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_unbind_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_unfold_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_unsqueeze_inplace_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_unsqueeze_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_view_as_complex_cuda (__main__.TestViewOpsCUDA) ... ok
test_view_as_real_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_view_as_real_cuda_complex32 (__main__.TestViewOpsCUDA) ... ok
test_view_as_real_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_as_view_cuda (__main__.TestViewOpsCUDA) ... ok
test_view_dtype_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_dtype_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_view_dtype_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_view_dtype_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_bool (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_dsplit_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_bool (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_hsplit_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_bool (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_split_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_bfloat16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_bool (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_complex128 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_complex64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_float16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_float32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_float64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_int16 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_int32 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_int64 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_int8 (__main__.TestViewOpsCUDA) ... ok
test_view_tensor_vsplit_cuda_uint8 (__main__.TestViewOpsCUDA) ... ok
test_view_view_cuda (__main__.TestViewOpsCUDA) ... ok

----------------------------------------------------------------------
Ran 226 tests in 47.312s

OK (skipped=11)
Running test_vmap ... [2021-10-12 11:26:14.064582]
Executing ['/opt/conda/bin/python3.6', 'test_vmap.py', '-v'] ... [2021-10-12 11:26:14.064662]
test_accepts_nested_inputs (__main__.TestVmapAPI) ... test_vmap.py:374: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1])((x, y))
test_vmap.py:376: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1], in_dims=(0,))((x, y))
test_vmap.py:378: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1], in_dims=((0, 0),))((x, y))
test_vmap.py:381: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1])([x, y])
test_vmap.py:383: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1], in_dims=(0,))([x, y])
test_vmap.py:385: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z[0] + z[1], in_dims=([0, 0],))([x, y])
test_vmap.py:388: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z['x'] + z['y'])({'x': x, 'y': y})
test_vmap.py:390: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z['x'] + z['y'], in_dims=(0,))({'x': x, 'y': y})
test_vmap.py:392: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out = vmap(lambda z: z['x'] + z['y'], in_dims=({'x': 0, 'y': 0},))({'x': x, 'y': y})
test_vmap.py:396: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  out_fn = vmap(lambda z: z['x'][0] + z['x'][1][0] + z['y'][0] + z['y'][1])
ok
test_backward_unsupported_interaction (__main__.TestVmapAPI) ... test_vmap.py:746: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(backward_on_vmapped_tensor)(x)
test_vmap.py:752: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(backward_with_vmapped_grad)(x, grad)
test_vmap.py:758: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(completely_unrelated_backward)(y)
ok
test_batched_gradient_basic (__main__.TestVmapAPI) ... test_vmap.py:790: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  jacobian = vmap(vjp_mul)(batched_v)
ok
test_constant_function (__main__.TestVmapAPI) ... test_vmap.py:61: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: torch.tensor(3.14))(torch.ones(3))
ok
test_different_map_dim_size_raises (__main__.TestVmapAPI) ... test_vmap.py:39: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul)(x, y)
test_vmap.py:41: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=((0, 0),))((x, y))
test_vmap.py:43: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z['x'] + z['y'], in_dims=({'x': 0, 'y': 0},))({'x': x, 'y': y})
ok
test_fallback_atan2 (__main__.TestVmapAPI) ... test_vmap.py:552: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, (2, 0))(x, y)
test_vmap.py:558: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(op), (2, 0))(x, y)
test_vmap.py:564: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(vmap(op)))(x, y)
ok
test_fallback_does_not_warn_by_default (__main__.TestVmapAPI) ... ok
test_fallback_masked_fill (__main__.TestVmapAPI) ... test_vmap.py:580: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(torch.index_add, (0, None, None, 0))(x, dim, index, values)
test_vmap.py:580: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(torch.index_add, (0, None, None, 0))(x, dim, index, values)
ok
test_fallback_multiple_returns (__main__.TestVmapAPI) ... test_vmap.py:598: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(torch.var_mean)(tensor)
test_vmap.py:604: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(torch.var_mean))(tensor)
test_vmap.py:610: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(vmap(torch.var_mean)))(tensor)
ok
test_fallback_warns_when_warnings_are_enabled (__main__.TestVmapAPI) ... ok
test_fallback_with_undefined_grad (__main__.TestVmapAPI) ... ok
test_fallback_zero_dim (__main__.TestVmapAPI) ... test_vmap.py:523: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, (0, None))(x, y)
test_vmap.py:525: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, (None, 0))(y, x)
test_vmap.py:527: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op)(x, x)
test_vmap.py:532: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, (0, None))(x, y)
test_vmap.py:534: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, (None, 0))(y, x)
test_vmap.py:536: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op)(x, x)
ok
test_func_with_no_inputs (__main__.TestVmapAPI) ... test_vmap.py:55: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo)()
test_vmap.py:58: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(bar)()
ok
test_functools_partial (__main__.TestVmapAPI) ... test_vmap.py:796: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(functools.partial(torch.mul, x))(y)
ok
test_grad_unsupported_interaction (__main__.TestVmapAPI) ... test_vmap.py:771: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(output_to_grad_is_vmapped)(input_tensor)
test_vmap.py:779: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(input_to_grad_is_vmapped)(input_tensor)
ok
test_in_dim_not_in_tensor_err_msg (__main__.TestVmapAPI) ... test_vmap.py:461: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo)(torch.randn([]))
test_vmap.py:463: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo, in_dims=(0,))(torch.randn([]))
test_vmap.py:465: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo, in_dims=(-1,))(x)
test_vmap.py:467: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo, in_dims=(2,))(y)
test_vmap.py:469: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=([3, 0],))([x, y])
test_vmap.py:471: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo, in_dims=(0,))(torch.randn(2, 3))
test_vmap.py:472: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(foo, in_dims=(1,))(torch.randn(2, 3))
ok
test_in_dims_wrong_type_err_msg (__main__.TestVmapAPI) ... test_vmap.py:405: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, [0, 0])(x, y)
test_vmap.py:407: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, set({0, 0}))(x, y)
test_vmap.py:409: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, 'lol')(x, y)
test_vmap.py:411: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=[0, 0])([x, y])
test_vmap.py:413: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, (0, 0))(x, y)
ok
test_inplace_fallback_nary_different_levels (__main__.TestVmapAPI) ... test_vmap.py:705: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, in_dims=(0, None))(x, y)
test_vmap.py:711: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(vmap(op, in_dims=(0, None)))(x, y)
test_vmap.py:719: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, in_dims=(None, 0))(x, y)
test_vmap.py:724: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(vmap(op, in_dims=(0, None)), in_dims=(None, 0))(x, y)
test_vmap.py:729: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(vmap(op, in_dims=(0, None)), in_dims=(None, 1))(x, y)
test_vmap.py:734: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(vmap(op, in_dims=(None, 0)))(x, y)
ok
test_inplace_fallback_nary_same_levels (__main__.TestVmapAPI) ... test_vmap.py:670: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op, (2, 0))(x, y)
test_vmap.py:678: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(vmap(op), (2, 0))(x, y)
test_vmap.py:686: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(vmap(op)))(x, y)
ok
test_inplace_fallback_unary (__main__.TestVmapAPI) ... test_vmap.py:629: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op)(x)
test_vmap.py:636: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, out_dims=(1,))(x)
test_vmap.py:643: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(op))(x)
test_vmap.py:650: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(vmap(op)))(x)
ok
test_integer_in_dim_but_not_tensor_input_err_msg (__main__.TestVmapAPI) ... test_vmap.py:444: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.sum)(x, 0)
test_vmap.py:446: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.sum, (0, 0))(x, 0)
test_vmap.py:448: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=([0, 0],))([x, 1])
test_vmap.py:450: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.sum, (0, None))(x, 0)
ok
test_multiple_inputs (__main__.TestVmapAPI) ... test_vmap.py:76: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(torch.mul)(x, y)
ok
test_multiple_out_dims (__main__.TestVmapAPI) ... test_vmap.py:218: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(foo, out_dims=(0, 1))(x)
test_vmap.py:221: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(bar, out_dims=(-1, 0, 1, 2))(x, y)
ok
test_multiple_outputs (__main__.TestVmapAPI) ... test_vmap.py:84: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  outputs = vmap(foo)(x)
ok
test_multiple_outputs_error_cases (__main__.TestVmapAPI) ... test_vmap.py:104: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(returns_tuple_of_tensors)(x)
test_vmap.py:109: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(returns_list_of_two_tensors)(x)
test_vmap.py:111: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(returns_list_of_one_tensor)(x)
ok
test_nested_non_default_in_dims (__main__.TestVmapAPI) ... test_vmap.py:335: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(vmap(torch.mul), (1, 0)), (1, 2))(x, y)
ok
test_nested_out_dims (__main__.TestVmapAPI) ... test_vmap.py:234: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda y: vmap(lambda x: x, out_dims=1)(y))(y)
test_vmap.py:239: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda y: vmap(lambda x: x, out_dims=1)(y), out_dims=1)(y)
test_vmap.py:244: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda y: vmap(lambda x: x, out_dims=-1)(y), out_dims=-1)(y)
test_vmap.py:251: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda y: vmap(lambda x: x * y, out_dims=1)(x), out_dims=-1)(y)
ok
test_nested_with_different_map_dim (__main__.TestVmapAPI) ... test_vmap.py:125: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: vmap(lambda y: x * y)(y))(x)
test_vmap.py:130: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: vmap(lambda y: vmap(lambda z: x * y * z)(z))(y))(x)
ok
test_nested_with_same_map_dim (__main__.TestVmapAPI) ... test_vmap.py:116: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(vmap(torch.mul))(x, y)
test_vmap.py:119: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(vmap(vmap(torch.mul)))(x, y)
ok
test_nn_module (__main__.TestVmapAPI) ... test_vmap.py:802: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(model)(tensor)
ok
test_non_default_in_dims_out_dims (__main__.TestVmapAPI) ... test_vmap.py:342: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, in_dims=1, out_dims=1)(x)
test_vmap.py:347: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, in_dims=2, out_dims=1)(x)
test_vmap.py:356: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(foo, in_dims=1, out_dims=1)(x)
test_vmap.py:360: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(foo, in_dims=2, out_dims=1)(x)
test_vmap.py:365: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vmap(foo, 1, 1), 1, 1)(x)
ok
test_non_tensor_output_raises (__main__.TestVmapAPI) ... test_vmap.py:26: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: 3.14)(torch.ones(3))
test_vmap.py:32: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(multiple_outputs)(torch.ones(3))
ok
test_non_zero_in_dims (__main__.TestVmapAPI) ... test_vmap.py:308: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: x, (1,))(tensor)
test_vmap.py:314: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(torch.mul, (0, 1))(x, y)
test_vmap.py:316: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(torch.mul, (1, 0))(x, y)
ok
test_none_in_dims (__main__.TestVmapAPI) ... test_vmap.py:324: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(torch.mul, (0, None))(x, y)
test_vmap.py:329: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(torch.mul, (0, None))(x, 2)
ok
test_nonzero_out_dims (__main__.TestVmapAPI) ... test_vmap.py:169: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, out_dims=1)(tensor)
test_vmap.py:175: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, out_dims=2)(tensor)
test_vmap.py:181: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, out_dims=-1)(tensor)
test_vmap.py:188: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x, y: (x, y), out_dims=2)(tensor, other)
test_vmap.py:196: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(lambda x: x, out_dims=2)(tensor)
test_vmap.py:204: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(foo, out_dims=1)(x, y)
ok
test_noop_in_inner_vmap (__main__.TestVmapAPI) ... test_vmap.py:137: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(lambda x: vmap(lambda y: x)(y))(x)
ok
test_not_enough_in_dims_err_msg (__main__.TestVmapAPI) ... test_vmap.py:421: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, (0,))(x, y)
test_vmap.py:423: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, (0, 0, 0))(x, y)
test_vmap.py:425: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=([0],))([x, y])
test_vmap.py:427: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda z: z[0] + z[1], in_dims=((0, 0),))([x, y])
test_vmap.py:429: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.mul, (0, 0))(x, y)
ok
test_out_dim_out_of_bounds_err_msg (__main__.TestVmapAPI) ... test_vmap.py:300: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=3)(x)
test_vmap.py:302: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=-4)(x)
ok
test_out_dims_and_num_outputs_mismatch_err_msg (__main__.TestVmapAPI) ... test_vmap.py:283: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=(0, 0))(x)
test_vmap.py:285: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: (x, x, x), out_dims=(0, 0, 0, 0))(x)
test_vmap.py:289: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: (x, x), out_dims=(0,))(x)
test_vmap.py:291: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: (x, x, x), out_dims=(0, 0))(x)
ok
test_out_dims_edge_case (__main__.TestVmapAPI) ... test_vmap.py:261: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  expected = vmap(foo, out_dims=1)(tensor)
test_vmap.py:262: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(foo, out_dims=(1,))(tensor)
ok
test_out_dims_must_be_int_or_tuple_of_int_err_msg (__main__.TestVmapAPI) ... test_vmap.py:269: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims='lol')(tensor)
test_vmap.py:271: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=('lol',))(tensor)
test_vmap.py:273: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=None)(tensor)
test_vmap.py:275: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda x: x, out_dims=(None,))(tensor)
ok
test_single_input (__main__.TestVmapAPI) ... test_vmap.py:70: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  output = vmap(square)(x)
ok
test_unsupported_op_err_msg (__main__.TestVmapAPI) ... test_vmap.py:148: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.ravel)(tensor)
test_vmap.py:154: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(out_op)(tensor, tensor)
test_vmap.py:159: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(lambda t: torch.atleast_1d([t]))(tensor)
test_vmap.py:164: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(torch.Tensor.item)(tensor)
ok
test_add_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_binary_cross_entropy_cuda (__main__.TestVmapBatchedGradientCUDA) ... /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
test_vmap.py:880: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, in_dims, out_dims)(*inputs)
ok
test_diagonal_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_div_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_expand_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_index_cuda (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:880: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, in_dims, out_dims)(*inputs)
ok
test_inplace_manyview_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_inplace_on_view_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_lgamma_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_log1p_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_log_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_logsumexp_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_max_cuda (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:880: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, in_dims, out_dims)(*inputs)
ok
test_median_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_min_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_mul_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_permute_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_reshape_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_select_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_sigmoid_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_slice_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_stack_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_sub_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_symeig_cuda (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:2417: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /var/lib/jenkins/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)
  return torch.symeig(x, eigenvectors=True)[0]
test_vmap.py:880: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(op, in_dims, out_dims)(*inputs)
ok
test_threshold_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_trace_cuda (__main__.TestVmapBatchedGradientCUDA) ... ok
test_unrelated_output_cuda (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:2477: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vjp)(gy)
ok
test_unrelated_output_multiple_grad_cuda (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:2492: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  result = vmap(vjp)(gy)
ok
test_vmap_fallback_check (__main__.TestVmapBatchedGradientCUDA) ... ok
test_vmap_fallback_check_ok (__main__.TestVmapBatchedGradientCUDA) ... test_vmap.py:962: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op_using_fallback)(torch.rand(3))
ok
test_T_numpy (__main__.TestVmapOperators) ... ok
test_as_strided (__main__.TestVmapOperators) ... ok
test_binary_pointwise_ops (__main__.TestVmapOperators) ... ok
test_bmm (__main__.TestVmapOperators) ... ok
test_cat (__main__.TestVmapOperators) ... ok
test_chunk (__main__.TestVmapOperators) ... ok
test_clamp (__main__.TestVmapOperators) ... ok
test_clone (__main__.TestVmapOperators) ... ok
test_comparison_ops (__main__.TestVmapOperators) ... ok
test_conj (__main__.TestVmapOperators) ... ok
test_contiguous (__main__.TestVmapOperators) ... ok
test_diagonal (__main__.TestVmapOperators) ... ok
test_dot (__main__.TestVmapOperators) ... ok
test_expand_as (__main__.TestVmapOperators) ... ok
test_fill_and_zero_inplace (__main__.TestVmapOperators) ... ok
test_imag (__main__.TestVmapOperators) ... ok
test_is_complex (__main__.TestVmapOperators) ... ok
test_is_contiguous (__main__.TestVmapOperators) ... ok
test_is_floating_point (__main__.TestVmapOperators) ... ok
test_mm (__main__.TestVmapOperators) ... ok
test_movedim (__main__.TestVmapOperators) ... ok
test_mv (__main__.TestVmapOperators) ... ok
test_narrow (__main__.TestVmapOperators) ... ok
test_new_empty (__main__.TestVmapOperators) ... ok
test_new_empty_strided (__main__.TestVmapOperators) ... ok
test_new_zeros (__main__.TestVmapOperators) ... ok
test_no_random_op_support (__main__.TestVmapOperators) ... ok
test_real (__main__.TestVmapOperators) ... ok
test_reshape (__main__.TestVmapOperators) ... ok
test_reshape_as (__main__.TestVmapOperators) ... ok
test_result_type (__main__.TestVmapOperators) ... ok
test_select (__main__.TestVmapOperators) ... ok
test_slice (__main__.TestVmapOperators) ... ok
test_split (__main__.TestVmapOperators) ... ok
test_squeeze (__main__.TestVmapOperators) ... ok
test_stack (__main__.TestVmapOperators) ... ok
test_stride (__main__.TestVmapOperators) ... ok
test_sum_dim (__main__.TestVmapOperators) ... ok
test_t (__main__.TestVmapOperators) ... ok
test_tensor_split (__main__.TestVmapOperators) ... ok
test_to (__main__.TestVmapOperators) ... ok
test_trace (__main__.TestVmapOperators) ... ok
test_transpose (__main__.TestVmapOperators) ... ok
test_unary_pointwise_ops (__main__.TestVmapOperators) ... ok
test_unbind (__main__.TestVmapOperators) ... ok
test_unfold (__main__.TestVmapOperators) ... ok
test_view (__main__.TestVmapOperators) ... ok
test_view_as (__main__.TestVmapOperators) ... ok
test_view_as_complex (__main__.TestVmapOperators) ... ok
test_view_as_real (__main__.TestVmapOperators) ... ok
test_vmap_fallback_check (__main__.TestVmapOperators) ... ok
test_vmap_fallback_check_ok (__main__.TestVmapOperators) ... test_vmap.py:962: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.
  vmap(op_using_fallback)(torch.rand(3))
ok

----------------------------------------------------------------------
Ran 125 tests in 18.331s

OK
Running test_vulkan ... [2021-10-12 11:26:37.009144]
Executing ['/opt/conda/bin/python3.6', 'test_vulkan.py', '-v'] ... [2021-10-12 11:26:37.009223]
test_conv (__main__.TestVulkanRewritePass) ... skipped 'Vulkan backend must be available for these tests.'

----------------------------------------------------------------------
Ran 1 test in 0.001s

OK (skipped=1)
Running test_xnnpack_integration ... [2021-10-12 11:26:39.254127]
Executing ['/opt/conda/bin/python3.6', 'test_xnnpack_integration.py', '-v'] ... [2021-10-12 11:26:39.254207]
test_conv1d_basic (__main__.TestXNNPACKConv1dTransformPass) ... /opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py:139: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /var/lib/jenkins/pytorch/c10/core/TensorImpl.h:1378.)
  return callable(*args, **kwargs)
ok
test_conv1d_with_relu_fc (__main__.TestXNNPACKConv1dTransformPass) ... skipped 'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test'
test_conv2d (__main__.TestXNNPACKOps) ... ok
test_conv2d_transpose (__main__.TestXNNPACKOps) ... ok
test_linear (__main__.TestXNNPACKOps) ... ok
test_linear_1d_input (__main__.TestXNNPACKOps) ... ok
test_decomposed_linear (__main__.TestXNNPACKRewritePass) ... ok
test_linear (__main__.TestXNNPACKRewritePass) ... ok
test_combined_model (__main__.TestXNNPACKSerDes) ... ok
test_conv2d (__main__.TestXNNPACKSerDes) ... ok
test_conv2d_transpose (__main__.TestXNNPACKSerDes) ... ok
test_linear (__main__.TestXNNPACKSerDes) ... ok

----------------------------------------------------------------------
Ran 12 tests in 112.860s

OK (skipped=1)
distributed/optim/test_zero_redundancy_optimizer failed!
distributions/test_distributions failed!
test_cpp_api_parity failed! Received signal: SIGIOT
test_cuda failed! Received signal: SIGIOT
test_indexing failed! Received signal: SIGIOT
test_jit failed! Received signal: SIGIOT
test_linalg failed! Received signal: SIGIOT
test_namedtensor failed! Received signal: SIGIOT
test_nn failed! Received signal: SIGIOT
test_ops failed! Received signal: SIGIOT
test_reductions failed! Received signal: SIGIOT
test_sort_and_select failed! Received signal: SIGIOT
test_torch failed! Received signal: SIGIOT
